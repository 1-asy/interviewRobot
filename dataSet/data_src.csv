实体1,实体2,关系
AutoML问题构成?,特征选择,question2answer
AutoML问题构成?,模型选择,question2answer
AutoML问题构成?,算法选择,question2answer
特征工程选择思路？,有监督的特征选择,question2answer
特征工程选择思路？,基于模型，lr的系数，树模型的importance等等,question2answer
特征工程选择思路？,基于选择，前项后项选择,question2answer
特征工程选择思路？,无监督的特征选择,question2answer
特征工程选择思路？,基于统计信息的，熵、相关性、KL系数,question2answer
特征工程选择思路？,基于方差，因子分解，PCA主成分分享，方差系数,question2answer
模型相关的选择思路?,模型选择,question2answer
模型相关的选择思路?,各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,question2answer
模型相关的选择思路?,参数选择,question2answer
模型相关的选择思路?,grid_search,question2answer
模型相关的选择思路?,random_search,question2answer
模型相关的选择思路?,...,question2answer
常见优化算法思路？,SGD,question2answer
常见优化算法思路？,GD,question2answer
常见优化算法思路？,LBFGS,question2answer
常见优化算法思路？,FTRL,question2answer
AutoML参数选择所使用的方法？,暴力搜索,question2answer
AutoML参数选择所使用的方法？,grid_search,question2answer
AutoML参数选择所使用的方法？,random_search,question2answer
AutoML参数选择所使用的方法？,拟合搜索,question2answer
AutoML参数选择所使用的方法？,贝叶斯优化,question2answer
AutoML参数选择所使用的方法？,其他方法,question2answer
AutoML参数选择所使用的方法？,Meta学习,question2answer
AutoML参数选择所使用的方法？,转移学习,question2answer
讲讲贝叶斯优化如何在automl上应用？,目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,question2answer
讲讲贝叶斯优化如何在automl上应用？,步骤：,question2answer
讲讲贝叶斯优化如何在automl上应用？,随机选取几个超参数进行f拟合，得到先验数据集合D,question2answer
讲讲贝叶斯优化如何在automl上应用？,根据先验数据D得到模型M,question2answer
讲讲贝叶斯优化如何在automl上应用？,根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,question2answer
讲讲贝叶斯优化如何在automl上应用？,循环23两步直至达到条件,question2answer
讲讲贝叶斯优化如何在automl上应用？,问题：,question2answer
讲讲贝叶斯优化如何在automl上应用？,稳定性：同一组超参数的预测结果在不同轮次不一致,question2answer
讲讲贝叶斯优化如何在automl上应用？,f函数需要多次计算，资源耗费时间损失,question2answer
讲讲贝叶斯优化如何在automl上应用？,难以确定比较通用的拟合模型f,question2answer
讲讲贝叶斯优化如何在automl上应用？,手记：,question2answer
讲讲贝叶斯优化如何在automl上应用？,![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9n45r6f3nj30q40tkjz8.jpg),question2answer
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,随机生成若干超参点，更新gp模型,question2answer
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,根据gp模型选取最优推荐值,question2answer
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,推荐值附近随机生成点，根据cquisitionfunction选取附近点极值点，acquisitionfunction通常：,question2answer
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,基于GPUCB的最大置信上界,question2answer
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,基于均值和方差的平衡结果,question2answer
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,ThompsonSampling,question2answer
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,EI(期望提升),question2answer
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,重复以上步骤,question2answer
写出全概率公式&贝叶斯公式,全概率公式：设事件![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wed60nzaj305i01cmx2.jpg)构成一个完备事件组，即它们两两不相容，和为全集且![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wedhjqtej304w01cjra.jpg)，则对任一事件A有：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8weetxaqxj30dk01e74b.jpg),question2answer
写出全概率公式&贝叶斯公式,贝叶斯公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wefkh3r5j30l203iq3c.jpg),question2answer
说说你怎么理解为什么有全概率公式&贝叶斯公式,全概率公式为全概率就是表示达到某个目的，有多种方式，算到达目的的概率。**key：算概率**,question2answer
说说你怎么理解为什么有全概率公式&贝叶斯公式,贝叶斯公式为当给定条件发生变化后，会导致事件发生的可能性发生何种变化。**key：概率变化**,question2answer
什么是先验概率,先验概率（priorprobability）：指根据以往经验和分析。在实验或采样前就可以得到的概率。key:简单的暴力统计,question2answer
什么是后验概率,后验概率（posteriorprobability）：指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。key：条件概率,question2answer
经典概率题,有一个木桶，里面有M个白球，小明每分钟从桶中随机取出一个球涂成红色（无论白或红都涂红）再放回，问小明将桶中球全部涂红的期望时间是多少？,question2answer
经典概率题,P[i]代表M个球中已经有i个球是红色后，还需要的时间期望，去将所有球都变成红色。,question2answer
经典概率题,P[i]=(i/M)*P[i]+(1i/M)*P[i+1]+1,question2answer
经典概率题,解释一下，每一次抽取，(i/M)概率不变，(1i/M)进入下一轮，额外加一次本次操作,question2answer
解释方差：,期望值与真实值之间的波动程度，衡量的是**稳定性**,question2answer
解释偏差：,期望值与真实值之间的一致差距，衡量的是**准确性**,question2answer
模型训练为什么要引入偏差和方差？请理论论证。,优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,question2answer
模型训练为什么要引入偏差和方差？请理论论证。,**Err=bias+var+irreducibleerror**,question2answer
模型训练为什么要引入偏差和方差？请理论论证。,"以回归任务为例,其实更准确的公式为：**Err=bias^2+var+irreducibleerror^2**",question2answer
模型训练为什么要引入偏差和方差？请理论论证。,符号的定义：一个真实的任务可以理解为Y=f(x)+e，其中f(x)为规律部分，e为噪声部分,question2answer
模型训练为什么要引入偏差和方差？请理论论证。,训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,question2answer
模型训练为什么要引入偏差和方差？请理论论证。,方差：模型的稳定性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx3a2qc3j306400ot8j.jpg),question2answer
模型训练为什么要引入偏差和方差？请理论论证。,偏差：模型的准确性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx6rqg34j305g00odfn.jpg),question2answer
模型训练为什么要引入偏差和方差？请理论论证。,"Err(x)=Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))+Err(f,Y)",question2answer
模型训练为什么要引入偏差和方差？请理论论证。,"Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))为可解释规则误差",question2answer
模型训练为什么要引入偏差和方差？请理论论证。,"Err(f,Y)为噪声e部分，即为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxc64l5yj300g00b0pn.jpg)",question2answer
模型训练为什么要引入偏差和方差？请理论论证。,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxyj2g67j301k00mmwx.jpg)可推导如下：,question2answer
模型训练为什么要引入偏差和方差？请理论论证。,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxkvx39gj30gh01b3yj.jpg),question2answer
模型训练为什么要引入偏差和方差？请理论论证。,f为真实值，固定；![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型不同数据预测结果的期望，固定；所以f![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)固定,question2answer
模型训练为什么要引入偏差和方差？请理论论证。,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxlzjp9hj305s00mt8j.jpg)中![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxm8gzmrj301z00mq2p.jpg)为常数。所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxlzjp9hj305s00mt8j.jpg)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxo2v9ozj30bw00m0sn.jpg)=0,question2answer
模型训练为什么要引入偏差和方差？请理论论证。,Err(x)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxvudtfgj30cm00mjra.jpg),question2answer
什么情况下引发高方差？,过高复杂度的模型，对训练集进行过拟合,question2answer
什么情况下引发高方差？,带来的后果就是在训练集合上效果非常好，但是在校验集合上效果极差,question2answer
什么情况下引发高方差？,更加形象的理解就是用一条高次方程去拟合线性数据,question2answer
如何解决高方差问题？,在模型复杂程度不变的情况下，增加更多数据,question2answer
如何解决高方差问题？,在数据量不变的情况下，减少特征维度,question2answer
如何解决高方差问题？,在数据和模型都不变的情况下，加入正则化,question2answer
以上方法是否一定有效？,增加数据如果和原数据分布一致，无论增加多少必定解决不了高方差,question2answer
以上方法是否一定有效？,smote对样本进行扩充是否必定可以避免高方差？,question2answer
以上方法是否一定有效？,过采样是否解决高方差问题？,question2answer
以上方法是否一定有效？,减少的特征维度如果是共线性的维度，对原模型没有任何影响,question2answer
以上方法是否一定有效？,罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？,question2answer
以上方法是否一定有效？,正则化通常都是有效的,question2answer
如何解决高偏差问题？,尝试获得更多的特征,question2answer
如何解决高偏差问题？,从数据入手，进行特征交叉，或者特征的embedding化,question2answer
如何解决高偏差问题？,尝试增加多项式特征,question2answer
如何解决高偏差问题？,从模型入手，增加更多线性及非线性变化，提高模型的复杂度,question2answer
如何解决高偏差问题？,尝试减少正则化程度λ,question2answer
以上方法是否一定有效？,特征越稀疏，高方差的风险越高,question2answer
以上方法是否一定有效？,多个线性变换=一个线性变换，多个非线性变换不一定=一个多线性变换,question2answer
以上方法是否一定有效？,正则化通常都是有效的,question2answer
遇到过的机器学习中的偏差与方差问题？,从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,question2answer
遇到过的机器学习中的偏差与方差问题？,从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,基础：,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,Bagging,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyk4wjxkj302v00qa9u.jpg),question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyp35ynkj308l00qglh.jpg),question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,其中,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lypqw1tjj300d00c0q9.jpg)可以直接提取出来,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyuaep07j308v00ya9x.jpg),question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,所以，化简以上的式子可得：Var(F)=m*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyw59y9oj300h00g0rq.jpg)*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lywg4870j300g00k0r2.jpg)+![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyzygc4lj304300lgle.jpg),question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz7qrpwsj304f015t8i.jpg),question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,E(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz97enuuj3059011743.jpg),question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,结论：,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,Boosting同理,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,boosting的前提是弱模型之间高度相关，我们不妨设相关度为1,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzwvy93dj302k00kmwx.jpg),question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzj06hv1j304300qwea.jpg),question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,结论：,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,整体模型的方差等于基模型的数量平方成正比，越多模型不稳定性越高，越容易过拟合。,question2answer
就理论角度论证Bagging、Boosting的方差偏差问题,GradientBoostingDecisionTree为典型例子,question2answer
遇到过的深度学习中的偏差与方差问题？,神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,question2answer
遇到过的深度学习中的偏差与方差问题？,但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,question2answer
遇到过的深度学习中的偏差与方差问题？,因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。,question2answer
遇到过的深度学习中的偏差与方差问题？,dropout,question2answer
遇到过的深度学习中的偏差与方差问题？,dense中的normalization,question2answer
遇到过的深度学习中的偏差与方差问题？,数据的shuffle,question2answer
方差、偏差与模型的复杂度之间的关系？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8ly6pdyouj30dn07eq38.jpg),question2answer
什么叫生成模型？,"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",question2answer
什么叫生成模型？,说白了就是玩的一手x在不同类别下出现的概率+不同类别的概率+x出现的概率进行复合计算，复合的时候考虑独立性的问题。,question2answer
什么叫判别模型？,求一个通过自变量能够表示出因变量的公式y=F(x)或者p(y/x)，核心是对于x找到一个最合适的公式得到y,question2answer
什么时候会选择生成/判别模型？,明确一点：绝大多数情况下，判别模型都要比生成模型效果好，而且需要的数据量和前提假设都要小于生成模型，上面的概念中可得原因。,question2answer
什么时候会选择生成/判别模型？,所以这个问题更想问的是什么时候要去用生成模型：,question2answer
什么时候会选择生成/判别模型？,但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,question2answer
什么时候会选择生成/判别模型？,如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,question2answer
什么时候会选择生成/判别模型？,一般会追问，如何构造？,question2answer
什么时候会选择生成/判别模型？,FM/FFM,question2answer
什么时候会选择生成/判别模型？,NeuralNetwork,question2answer
什么时候会选择生成/判别模型？,线性Dense,question2answer
什么时候会选择生成/判别模型？,非线性激活,question2answer
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,question2answer
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,生成式模型：朴素贝叶斯，混合高斯模型，马尔科夫随机场，EM,question2answer
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,仔细看过这些模型细节的朋友都应该知道，他们最后都是判断x属于拟合一个正负样本分布，然后对比属于正负样本的概率,question2answer
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,判别式模型：最大熵模型，CRF,question2answer
我的理解：,无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,question2answer
我的理解：,算出属于正负样本的概率在相互对比的就是生成模型，直接得到结果概率的就是判别模型,question2answer
我的理解：,生成模型得分布，判别模型得最优划分,question2answer
我的理解：,生成模型可以得到判别模型，反之不成立,question2answer
我的理解：,生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,question2answer
极大似然估计 - MLE,原理：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。,question2answer
极大似然估计 - MLE,似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,question2answer
最大后验估计 - MAP,"MAP的基础使贝叶斯公式：P(θ/X)=P(θ,X)/P(X)。目的是通过观测值使得后验概率P(θ,X)最大即可",question2answer
极大似然估计与最大后验概率的区别？,最大似然估计中的采样满足所有采样都是独立同分布的假设,question2answer
极大似然估计与最大后验概率的区别？,最大后验概率在考虑了p(X/θ)的同时，还考虑了p(θ),question2answer
到底什么是似然什么是概率估计？,似然：给定了x求θ真实的可能性,question2answer
到底什么是似然什么是概率估计？,概率估计：给定了θ，X=x的可能性,question2answer
DNN与DeepFM之间的区别?,DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,question2answer
Wide&Deep与DeepFM之间的区别?,DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,question2answer
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,question2answer
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,question2answer
DeepFM怎么优化的？,embedding向量可以通过FM初始化,question2answer
DeepFM怎么优化的？,Deep层可以做优化,question2answer
DeepFM怎么优化的？,NFM:把deep层的做NFM类型的处理，其实就是deep层在输入之前也做一个二阶特征的交叉处理和fm层一致,question2answer
DeepFM怎么优化的？,FM层可以变得交叉更多阶,question2answer
DeepFM怎么优化的？,XDeepFM,question2answer
不定长文本数据如何输入deepFM？,截断补齐,question2answer
不定长文本数据如何输入deepFM？,结合文本id+文本长度，在做文本处理之前，先做不等长的sum_pooled的操作,question2answer
deepfm的embedding初始化有什么值得注意的地方吗？,"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",question2answer
deepfm的embedding初始化有什么值得注意的地方吗？,"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",question2answer
deepfm的embedding初始化有什么值得注意的地方吗？,文本项目上也可以用预训练好的特征,question2answer
主要使用了什么机制?,Attention机制，针对不同的广告，用户历史行为与该广告的权重是不同的。,question2answer
activation unit的作用,基于Attention机制，结合当前需对比的不同目标，将用户历史行为进行不同的权重分配。,question2answer
activation unit的作用,![](https://tva1.sinaimg.cn/large/006tNbRwgy1ga15ugjlkmj308901imx1.jpg),question2answer
activation unit的作用,通常用户兴趣可以由历史行为(点击/浏览/收藏)等合并得到，及![](https://tva1.sinaimg.cn/large/006tNbRwgy1ga15vxtcu8j302w01imwy.jpg),question2answer
activation unit的作用,activationunit在这种思路上，认为面对不同的对象Va兴趣的权重Wi应该也是变换而不是固定的，所以用了g(ViVa)来动态刻画不同目标下的历史行为的不同重要性,question2answer
DICE怎么设计的,先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,question2answer
DICE怎么设计的,"x_p=tf.sigmoid(tf.layers.batch_normalization(x,center=False,scale=False,training=True))",question2answer
DICE怎么设计的,aplha*(1x_p)*x+x_p*x,question2answer
DICE使用的过程中，有什么需要注意的地方,在用batch_normalization的时候，需要设置traning=True，否则在做test的时候，获取不到training过程中的各batch的期望,question2answer
DICE使用的过程中，有什么需要注意的地方,test的时候，方差计算利用的是期望的无偏估计计算方法:E(u^2)`*`m/(m1),question2answer
选用的原因？,类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,question2answer
选用的原因？,向量级别的特征交互而不是元素级交互,question2answer
选用的原因？,经验上，vectorwise的方式构建的特征交叉关系比bitwise的方式更容易学习,question2answer
选用的原因？,我也不知道具体好在哪，如果有大佬会可以指导一下，感恩,question2answer
选用的原因？,"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",question2answer
选用的原因？,"思路一：改变Memorization为attention网络，强化feature直接的关系，对B进行""电脑""与""蔡徐坤""之间的绑定而不是""篮球""和""蔡徐坤""之间的绑定",question2answer
选用的原因？,思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,question2answer
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,显示是可以写出feature交互的公式，隐式相反,question2answer
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,元素级是以feature值交互，向量级是feature向量级点乘处理,question2answer
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,question2answer
简单介绍一下XDeepFm的思想？,借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,question2answer
简单介绍一下XDeepFm的思想？,高阶特征交互：DNN,question2answer
简单介绍一下XDeepFm的思想？,低价特征交互：DCN结构(![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)),question2answer
简单介绍一下XDeepFm的思想？,这样的网络结构保证来来自X0的1，2，3...N阶的特征组合,question2answer
简单介绍一下XDeepFm的思想？,借鉴来DCN的交叉网络的特殊结构**自动构造有限高阶交叉特征**：![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg),question2answer
简单介绍一下XDeepFm的思想？,流程概述：,question2answer
简单介绍一下XDeepFm的思想？,featureembedding,question2answer
简单介绍一下XDeepFm的思想？,"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",question2answer
简单介绍一下XDeepFm的思想？,CIN中：,question2answer
简单介绍一下XDeepFm的思想？,"先记录\[batch,field,embedding_size]作为X0，并切分为embedding`*`\[batch,field,1]份",question2answer
简单介绍一下XDeepFm的思想？,设置三层隐层，单层结点数为200，单层操作如下（以X1为例）：,question2answer
简单介绍一下XDeepFm的思想？,"获取上一次的layerout：X0，并进行切分：embedding`*`\[batch,field,1]",question2answer
简单介绍一下XDeepFm的思想？,"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",question2answer
简单介绍一下XDeepFm的思想？,"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",question2answer
简单介绍一下XDeepFm的思想？,加偏置项，并进行激活函数处理，完成一轮处理,question2answer
简单介绍一下XDeepFm的思想？,"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",question2answer
简单介绍一下XDeepFm的思想？,实际过程中，可以对每层对结果进行采样泛化；可以通过最后层输出的残差连接保证梯度消失等等,question2answer
和DCN比，有哪些核心的变化？,DCN是bitwise的，而CIN是vectorwise的,question2answer
和DCN比，有哪些核心的变化？,DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,question2answer
和DCN比，有哪些核心的变化？,![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)和![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg)差异的![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9in06u4m6j300j00f0sh.jpg)导致的,question2answer
时间复杂度多少？,假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,question2answer
时间复杂度多少？,CIN:O(m`*`L`*`H`*`H),question2answer
时间复杂度多少？,DNN:O(m`*`D`*`H+L`*`H`*`H),question2answer
变长数据如何处理的？,input数据中只拿了近20次的点击，部分用户是没有20次的历史行为的，所以我们记录了每一个用户实际点击的次数，在做embedding的时候，我们除以的是真实的historylength,question2answer
变长数据如何处理的？,20次点击过去一周内的行为，曾经尝试扩大历史点击次数到40，60没有很明显的效果提升,question2answer
变长数据如何处理的？,点击行为是处理过的，停留时间过短的click不要,question2answer
变长数据如何处理的？,点击行为是处理过的，连续多次的重复点击会去重,question2answer
变长数据如何处理的？,点击行为是处理过的，session内的点击次数需要在约定范围内,question2answer
input是怎么构造的,最近历史20次点击商品id/文章id，如果不足不需要补充,question2answer
input是怎么构造的,最近历史20次点击商品id对应的品牌/文章id对应的类目，如果不足不需要补充,question2answer
input是怎么构造的,最近历史20次点击商品id对应的类别/文章id对应的栏目，如果不足不需要补充,question2answer
input是怎么构造的,最后一次点击商品id/文章id,question2answer
input是怎么构造的,历史上最高频的商品id/文章id,question2answer
input是怎么构造的,exampleage,question2answer
input是怎么构造的,user_info:age/gender/地理位置/注册时长,question2answer
input是怎么构造的,cross_info:最后一次点击距click时间，最后一次点击商品浏览次数,question2answer
input是怎么构造的,phone_info:设备信息，登录状态,question2answer
最后一次点击实际如何处理的？,我们会以日进行切分，每日首次点击的lastclick会以\[unknow]进行替代，隔日的点击不会进行计算,question2answer
output的是时候train和predict如何处理的,train的时候是进行负采样的,question2answer
output的是时候train和predict如何处理的,predict的时候是进行的all_embeddingdot,question2answer
如何进行负采样的？,该次点击时间之前所以的item或者article作为候选集,question2answer
如何进行负采样的？,负采样我们会进行剔除，把该次click下的同时show的样本进行剔除后采样,question2answer
如何进行负采样的？,均衡采样，不会根据其他样本showtime进行加权,question2answer
如何进行负采样的？,为了尽可能多的修正全量样本，尽快达到收敛,question2answer
如何进行负采样的？,为了避免其他推荐产生的交叉影响,question2answer
item向量在softmax的时候你们怎么选择的？,是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,question2answer
Example Age的理解？,官方：upload_timeclick_time,question2answer
Example Age的理解？,希望更倾向于新上视频,question2answer
Example Age的理解？,民间：click_timenow,question2answer
Example Age的理解？,希望平衡样本构造时间对当前的影响,question2answer
什么叫做不对称的共同浏览（asymmetric co-watch）问题？,item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,question2answer
为什么不采取类似RNN的Sequence model？,在实际的推荐数据获取中，历史点击流收到若干种在线的推荐算法影响，并不全是像自然语言问题一样真实具有序列性,question2answer
YouTube如何避免百万量级的softmax问题的？,负采样,question2answer
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,工程妥协，避免几百万的item每次都要计算一边，而采取的ANN的邻近搜索加快速度,question2answer
Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,exampleage,question2answer
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,不对称的共同浏览问题，避免引入futureinformation，产生与事实不符的数据穿越,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,embedding,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,引入了doc2vec做init,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,权重共享，没有在softmax处重新构造,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,负采样,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,限定采样集合在click时间发生之前已经有的item,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,剔除该次点击click时同时展现的其他item,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,均衡采样,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,加快收敛,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,避免热门商品item的过度影响,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,click数据的预处理,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,historyclick,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,停留时间过短的click不要,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,连续多次的重复点击会去重,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,session内的点击次数需要在约定范围内,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,lastclick,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,每日初次点击的lastclick以\[unknown]替代，不做隔日的数据连接,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,在history引入multiheadattention,question2answer
整个过程中有什么亮点？有哪些决定性的提升？,ctr提高了，但是有效点击没有变,question2answer
辗转相除法,```python,question2answer
辗转相除法,"defsolve(a,b):",question2answer
辗转相除法,"returnaifb==0elsesolve(b,a%b)",question2answer
辗转相除法,```,question2answer
其他方法,穷举法,question2answer
其他方法,辗转相减法,question2answer
四则运算,(u+v)'=u'+v',question2answer
四则运算,(uv)'=u'v',question2answer
四则运算,(uv)'=u'v+uv',question2answer
四则运算,(u/v)'=(u'vuv')/v^2,question2answer
常见导数,"y=c(常数),y'=0",question2answer
常见导数,"y=pow(x,a),y'=a·pow(x,a1)",question2answer
常见导数,"y=pow(a,x),y'=pow(a,x)·ln(a)",question2answer
常见导数,"y=log(a,x),y'=1/(xlna);特别的ln(x)=1/x",question2answer
常见导数,"y=sin(x),y'=cos(x)",question2answer
常见导数,"y=cos(x),y'=sin(x)",question2answer
常见导数,"y=tan(x),y'=1/(cos(x)^2)",question2answer
复合函数的运算法则,"若y=f(g(x)),y'=f'(g(x))·g'(x),前提是g在x处可导，f在g(x)处可导",question2answer
莱布尼兹公式,"若u(x),v(x)均n阶可导，则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xl2xwg4ij307000udfv.jpg)",question2answer
切线方程,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xko8hysoj306c0123yi.jpg),question2answer
法线方程,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xkomfj6lj309c01amx9.jpg),question2answer
费马定理,f在x0的邻域内有定义，且恒满足：f(x)<=f(x0)｜f(x)>=f(x0),question2answer
费马定理,f在x0处可导，则满足f'(x0)=0,question2answer
拉格朗日中值定理,设函数f(x)满足条件：,question2answer
拉格朗日中值定理,"\[a,b]上连续",question2answer
拉格朗日中值定理,"\(a,b)内可导，则\(a,b)存在ζ，使得f(b)f(a)=f'(ζ)(ba)",question2answer
柯西中值定理,"设函数f(x),g(x)满足条件：",question2answer
柯西中值定理,"\[a,b]上连续",question2answer
柯西中值定理,"\(a,b)内可导，且f'(x)和g'(x)存在，且g'(x)!=0",question2answer
柯西中值定理,"则\(a,b)存在ζ，使得(f(b)f(a))g'(ζ)=f'(ζ)(g(b)g(a))",question2answer
期望,离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,question2answer
期望,若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,question2answer
期望,E(ax+by+c)=aE(x)+bE(y)+c,question2answer
期望,如果x和y独立，E(xy)=E(x)E(y),question2answer
方差,方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,question2answer
方差,方差刻画了随机变量的取值对于其数学期望的离散程度。,question2answer
方差,方差深入：,question2answer
方差,很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,question2answer
方差,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zkdv6ukj308700kweb.jpg),question2answer
方差,如果x和y独立，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zl5np3yj308600kt8j.jpg),question2answer
标准差,标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,question2answer
协方差,协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,question2answer
协方差,在概率论和统计学中，协方差用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。,question2answer
协方差,"Cov(x,y)=E((xE(x))(yE(y)))",question2answer
协方差,"Cov(c+ax,d+by)=abCov(x,y)",question2answer
相关系数,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zq1gbe7j306601ct8k.jpg),question2answer
相关系数,"\[1,1]之间，值越接近1，说明两个变量正相关性（线性）越强。越接近1，说明负相关性越强，当为0时，表示两个变量没有相关性",question2answer
均匀分布,"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",question2answer
均匀分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91ysz3sxsj30aw023glh.jpg),question2answer
伯努利分布,"伯努利分布：参数为p∈[0,1]，设随机变量X∈{0,1}，则概率分布函数为：",question2answer
伯努利分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91yyoulv0j306m00lmwz.jpg),question2answer
伯努利分布,期望为p，方差为p(1p),question2answer
二项分布,独立重复地进行n次试验中，成功x次的概率:,question2answer
二项分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z2sy9m5j308800ja9w.jpg),question2answer
二项分布,期望为np，方差为np(1p),question2answer
高斯分布,我们在做模型训练的之后，随机变量取值范围是实数，大多数情况下都假设变量服从高斯分布，原因：,question2answer
高斯分布,随机变量大多数情况下有若干个因素组合而成，中心极限定理表明，多个独立随机变量的和近似正态分布,question2answer
高斯分布,在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,question2answer
高斯分布,典型的一维正态分布的概率密度函数为:,question2answer
高斯分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z5lcnmcj30dk02fa9z.jpg),question2answer
拉普拉斯分布,概率密度函数：,question2answer
拉普拉斯分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z66211aj309c01hjr9.jpg),question2answer
拉普拉斯分布,期望为u，方差为2γ^2,question2answer
拉普拉斯分布,拉普拉斯分布比高斯分布更加尖锐和狭窄，在正则化中通常会利用该性质,question2answer
泊松分布,假设已知事件在单位时间（或者单位面积）内发生的平均次数为λ，则泊松分布描述了：事件在单位时间（或者单位面积）内发生的具体次数为k的概率。,question2answer
泊松分布,概率密度函数：,question2answer
泊松分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z8oplp1j306f01g0sk.jpg),question2answer
泊松分布,期望：λ，方差为：λ,question2answer
条件概率,P(A/B)=P(AB)/P(B),question2answer
独立,P(AB)=P(A)P(B),question2answer
概率基础公式,加法：P(A+B)=P(A)+P(B)P(AB),question2answer
概率基础公式,减法：P(AB)=P(A)P(AB),question2answer
概率基础公式,乘法：P(AB)=P(A)P(B/A),question2answer
全概率：,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920h3l62yj305u00qweb.jpg),question2answer
贝叶斯,P(B/A)=P(B)*P(A/B)/P(A),question2answer
切比雪夫不等式,"p(|xu|>k∂)<=1/(k^2),满足k>0,u为期望,∂为标准差",question2answer
切比雪夫不等式,绝大多数数据都应该在均值附近,question2answer
抽球,有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),question2answer
抽球,无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),question2answer
纸牌问题,问题：54张牌，分成6份，每份9张牌，大小王在一起的概率？,question2answer
纸牌问题,54张牌分成6等份，共有M=(C54取9)*(C45取9)*...种分法。,question2answer
纸牌问题,其中大小王在同一份的分法有N=(C6取1)*(C52取7)*(C45取9)*...种。,question2answer
纸牌问题,因此所求概率为P=N/M,question2answer
棍子/绳子问题,问题：一根棍子折三段能组成三角形的概率？,question2answer
棍子/绳子问题,假设：棍子长度为1，第一段长度为x，第二段长度为y，第三段长度1xy,question2answer
棍子/绳子问题,分母：总样本空间为：1*1=1,question2answer
棍子/绳子问题,分子：两边之和大于第三边，得1/8,question2answer
贝叶斯,问题：某城市发生一起汽车撞人逃跑事件，该城市只有两种颜色的车，蓝20%绿80%，事发时现场只有一个目击者，他指正是蓝车，但根据专家分析，当时那种条件下能看正确的可能性是80%，那么肇事的车是蓝车的概率是多少？,question2answer
贝叶斯,假设事件A为目击者指正蓝车，事件B为肇事车为蓝车，事件C为肇事车为绿车，那么有：,question2answer
贝叶斯,0.2`*`0.8/(0.2`*`0.8+0.8`*`0.2)=0.5,question2answer
选择时间问题,"问题：一个活动,n个女生手里拿着长短不一的玫瑰花,无序的排成一排,一个男生从头走到尾,试图拿更长的玫瑰花,一旦拿了一朵就不能再拿其他的,错过了就不能回头,问最好的策略及其概率?",question2answer
选择时间问题,1/e,question2answer
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,均匀分布：,question2answer
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,E(x)=(a+b)/2,question2answer
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,标准差：D(x)=(ba)^2/12,question2answer
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,所以只需要对x做变换：sqrt(12(x1/2))即可,question2answer
抽红蓝球球,问题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数,question2answer
抽红蓝球球,假设设抽到蓝球的概率为p，设抽到红球的概率为q，那么抽取到的次数为：1·p+2p·q+...+np·q^(n1),question2answer
抽红蓝球球,"可得E=p\[1+2q+...+nq^(n1)],令1+2q+...+nq^(n1)=s，再由s为等比公式和ssq得，E=1/p",question2answer
泰勒公式,定义：f(x)在x0处的邻域内有n+1阶的导数，在x0的邻域内的任x，x和x0之间至少存在一个ζ，使得,question2answer
泰勒公式,f(x)=f(x0)+f'(x0)(xx0)+1/2!f''(x0)(xx0)^2+...+Rn(x),question2answer
泰勒公式,其中，Rn(x)=f<n+1>(ζ)/(n+1)!(xx0)^(n+1)，为泰勒余项,question2answer
常见泰勒公式,![](https://i.bmp.ovh/imgs/2019/11/0a10d9591cc0c4ac.png),question2answer
迭代公式推导,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfple4x4j303x00i742.jpg),question2answer
迭代公式推导,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfr2t8flj306l00mmwz.jpg),question2answer
迭代公式推导,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfruh1klj3043015wea.jpg),question2answer
迭代公式推导,这边是2次，所以直接以y=x^2化简了,question2answer
实现它,```,question2answer
实现它,"defget_ans(nums,count=10000):",question2answer
实现它,ans=nums,question2answer
实现它,ifnotans:,question2answer
实现它,returnans,question2answer
实现它,Times=0,question2answer
实现它,whileTimes<count:,question2answer
实现它,ans=0.5*(ans+nums/ans),question2answer
实现它,Times+=1,question2answer
实现它,returnans,question2answer
实现它,```,question2answer
范数,1范数：各列绝对值和的最大值,question2answer
范数,"2范数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zwforqmj302i00n0si.jpg),矩阵$A^TA$的最大特征值开平方根",question2answer
特征值分解，特征向量,特征值分解可以得到特征值与特征向量,question2answer
特征值分解，特征向量,特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,question2answer
特征值分解，特征向量,"矩阵A的特征值与其特征向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zxnzwesj300h00h0rz.jpg),特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zy0ybcpj300a00c0pd.jpg)满足：",question2answer
特征值分解，特征向量,也可写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9201wcuefj303600qa9u.jpg),question2answer
特征值分解，特征向量,其中，Q为特征向量组成的矩阵，∑为特征值由大到小组成的矩阵,question2answer
正定性,如何判断矩阵的正定性？,question2answer
正定性,矩阵的特征值大于等于0，半正定,question2answer
正定性,矩阵的特征值大于0，正定,question2answer
正定性,正定性的用途？,question2answer
正定性,Hessian矩阵正定性在梯度下降的应用,question2answer
正定性,"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",question2answer
正定性,在svm中核函数构造的基本假设,question2answer
统计方法,3∂原则,question2answer
统计方法,数据需要服从正态分布,question2answer
统计方法,只能解决一维问题：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m954lh5tj301h00gdfl.jpg),question2answer
统计方法,基于正态分布的离群点检测方法,question2answer
统计方法,一元高斯分布校验：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m91basuwj306201h0sl.jpg)，如果概率值大小离群则代表为异常点,question2answer
统计方法,多元高斯分布检测：,question2answer
统计方法,假设n维的数据集合![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算n维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg),question2answer
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fdr3xfj30140080sl.jpg)的协方差矩阵：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fly7rpj306g00ja9z.jpg),question2answer
统计方法,得到![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9gzmd1cj30bz01874b.jpg),question2answer
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m92kshc2j30d6073dft.jpg),question2answer
统计方法,马氏距离,question2answer
统计方法,假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。,question2answer
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kguzdgj307e00lmx3.jpg),question2answer
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9rdqiqjj300g00h0sl.jpg)统计检验,question2answer
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ryphxxj304u00kmx2.jpg),question2answer
统计方法,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第i维的均值，n是维度",question2answer
统计方法,箱型图,question2answer
统计方法,"IQR，\[Q13/2(Q3Q1),Q3+3/2(Q3Q1)]",question2answer
矩阵分解方法,PCA,question2answer
矩阵分解方法,去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,question2answer
矩阵分解方法,核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,question2answer
矩阵分解方法,问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,question2answer
矩阵分解方法,SVD,question2answer
矩阵分解方法,假设dataMat是一个p维的数据集合，有N个样本，它的协方差矩阵是X。那么协方差矩阵就通过奇异值分解写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8man1o3trj302h00hmx1.jpg),question2answer
矩阵分解方法,"其中P是一个(p,p)维的正交矩阵，它的每一列都是X的特征向量。D是一个(p,p)维的对角矩阵，包含了特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8manhjc7qj301n00ga9x.jpg)。",question2answer
矩阵分解方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mar6eb43j304300h746.jpg)可以认为是dataMat在主成分topj上的映射,question2answer
矩阵分解方法,最后还需要拉回原空间：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mb1hmrmnj306w00i747.jpg),question2answer
矩阵分解方法,异常值分数（outlierscore）：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8maynk5wkj30a300mjrc.jpg)+![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mayu64x7j305500mt8m.jpg),question2answer
特征值和特征向量的本质是什么？,一个特征向量可以看成2维平面上面的一条线，或者高维空间里面的一个超平面,question2answer
特征值和特征向量的本质是什么？,特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度,question2answer
矩阵乘法的实际意义？,两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,question2answer
矩阵乘法的实际意义？,矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,question2answer
密度的离群点检测,定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。,question2answer
密度的离群点检测,我们可以通过随机选择联通点，人为设置联通点附近最小半径a，半径内最小容忍点个数b，再考虑密度可达，形成蓝色方框内的正常数据区域，剩下的黄色区域内的点即为异常点。,question2answer
密度的离群点检测,LocalOutlierFactor算法,question2answer
密度的离群点检测,孤立森林:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mcoswixrj30iy0em0tb.jpg),question2answer
密度的离群点检测,经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),question2answer
密度的离群点检测,经验2：树的个数在256棵以下,question2answer
密度的离群点检测,缺点：,question2answer
密度的离群点检测,计算量大：o(n^2),question2answer
密度的离群点检测,需要人为选择阈值,question2answer
聚类的离群点检测,一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。,question2answer
聚类的离群点检测,缺点也就是聚类的缺点，包括初始点对结果的影响，数据是否保持凸型对结果对影响，簇的个数的选择,question2answer
如何处理异常点？,删除含有异常值的记录：直接将含有异常值的记录删除；,question2answer
如何处理异常点？,视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；,question2answer
如何处理异常点？,平均值修正：可用前后两个观测值的平均值修正该异常值；,question2answer
如何处理异常点？,生成列新特征：category异常,question2answer
如何处理异常点？,不处理：直接在具有异常值的数据集上进行数据挖掘；,question2answer
为什么要对数据进行采样平衡,"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",question2answer
为什么要对数据进行采样平衡,上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,question2answer
为什么要对数据进行采样平衡,比如99%为正样本，1%为负样本，如果分类器把所以样本预测为正样本则准确率高达99%，显然不符合实际情况,question2answer
是否一定需要对原始数据进行采样平衡,否。,question2answer
是否一定需要对原始数据进行采样平衡,采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降,question2answer
是否一定需要对原始数据进行采样平衡,采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,question2answer
有哪些常见的采样方法？,随机采样,question2answer
有哪些常见的采样方法？,无放回的简单抽样：每条样本被采到的概率相等且都为1/N,question2answer
有哪些常见的采样方法？,有放回的简单抽样：每条样本可能多次被选中,question2answer
有哪些常见的采样方法？,上采样：即合理地增加少数类的样本,question2answer
有哪些常见的采样方法？,下采样：欠抽样技术是将数据从原始数据集中移除,question2answer
有哪些常见的采样方法？,平衡采样：考虑正负样本比,question2answer
有哪些常见的采样方法？,分层采样：通过某一些feature对数据进行切分，按照切分后的占比分别进行采样,question2answer
有哪些常见的采样方法？,"整体采样：先将数据集T中的数据分组成G个互斥的簇,然后再从G个簇中简单随机采样s个簇作为样本集",question2answer
有哪些常见的采样方法？,合成采样,question2answer
有哪些常见的采样方法？,"相对于采样随机的方法进行上采样,还有两种比较流行的上采样的改进方式：",question2answer
有哪些常见的采样方法？,SMOTE,question2answer
有哪些常见的采样方法？,"x_new=x+rand(0,1)*(x′−x)",question2answer
有哪些常见的采样方法？,**带来新样本的同时有可能造成不同类别样本之间的重合**,question2answer
有哪些常见的采样方法？,BorderlineSMOTE为了解决上面的问题，在x_new生成之前，会先判断x这个点是否周围都是同类别的点,question2answer
有哪些常见的采样方法？,ADASYN,question2answer
有哪些常见的采样方法？,同上，也是在构造样本点的过程中考虑了正负样本比,question2answer
有哪些常见的采样方法？,平衡欠采样,question2answer
有哪些常见的采样方法？,EasyEnsemble，利用模型融合的方法（Ensemble）,question2answer
有哪些常见的采样方法？,少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合,question2answer
有哪些常见的采样方法？,BalanceCascade，利用模型融合的方法（Boost）,question2answer
有哪些常见的采样方法？,每次剔除预测正确的多数样本，加入新的未预测的多数样本,question2answer
有哪些常见的采样方法？,NearMiss,question2answer
有哪些常见的采样方法？,选择离各种情况下的少数样本位置最远的多数样本进行训练,question2answer
能否避免采样？,可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,question2answer
你平时怎么用采样方法？,尽量避免使用合成采样的方式去做数据填充，总结如下：,question2answer
你平时怎么用采样方法？,由于项目中时间的充裕问题，填充的结果往往是正负样本交叠且无感知的，会干扰分类器,question2answer
你平时怎么用采样方法？,通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀,question2answer
你平时怎么用采样方法？,合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合,question2answer
为什么需要对数据进行变换？,避免异常点：比如对连续变量进行份桶离散化,question2answer
为什么需要对数据进行变换？,可解释性或者需要连续输出：比如评分卡模型中的iv+woe,question2answer
为什么需要对数据进行变换？,使得原始数据的信息量更大：比如log/sqrt变换,question2answer
归一化和标准化之间的关系？,归一化(maxmin),question2answer
归一化和标准化之间的关系？,缩放仅仅跟最大、最小值的差别有关，只是一个去量纲的过程,question2answer
归一化和标准化之间的关系？,标准化(zscore),question2answer
归一化和标准化之间的关系？,缩放和所有点都相关，数据相对分布不会改变，集中的数据标准化后依旧集中,question2answer
归一化和标准化之间的关系？,作用,question2answer
归一化和标准化之间的关系？,解决部分模型由于数据值域不同对模型产生的影响，尤其是距离模型,question2answer
归一化和标准化之间的关系？,更快的收敛,question2answer
归一化和标准化之间的关系？,去量纲化,question2answer
归一化和标准化之间的关系？,避免数值计算溢出,question2answer
归一化和标准化之间的关系？,总结,question2answer
归一化和标准化之间的关系？,异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降,question2answer
归一化和标准化之间的关系？,分布角度：使用标准化之前，要求数据需要近似满足高斯分布，不然会改变数据的分布，尤其是对数据分布有强假设的情况下,question2answer
归一化和标准化之间的关系？,上线变动角度：归一化在上线的时候需要考虑上下约束届是否需要变动，标准化则不需要考虑变动,question2answer
归一化和标准化之间的关系？,值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定,question2answer
归一化和标准化之间的关系？,模型角度：一般涉及距离计算，协方差计算，数据满足高斯分布的情况下用标准化，其他归一化或其他变换,question2answer
归一化和标准化之间的关系？,常用模型,question2answer
归一化和标准化之间的关系？,knn：计算距离，不去量冈则结果受值域范围影响大,question2answer
归一化和标准化之间的关系？,neuralnetwork：梯度异常问题+激活函数问题,question2answer
连续特征常用方法,截断,question2answer
连续特征常用方法,"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",question2answer
连续特征常用方法,参考异常点里面的outlier识别，以最大值填充或者以None,question2answer
连续特征常用方法,二值化,question2answer
连续特征常用方法,数据分布过于不平衡,question2answer
连续特征常用方法,空值/异常值过多,question2answer
连续特征常用方法,分桶,question2answer
连续特征常用方法,小范围连续数据内不存在逻辑关系，比如31岁和32岁之间不存在明显的差异，可以归为一类,question2answer
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2hcomdwj30k60djdh4.jpg),question2answer
连续特征常用方法,离散化,question2answer
连续特征常用方法,数值无意义，比如学历、祖籍等等,question2answer
连续特征常用方法,缩放,question2answer
连续特征常用方法,zscore标准化,question2answer
连续特征常用方法,minmax归一化,question2answer
连续特征常用方法,范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg),question2answer
连续特征常用方法,L1范数,question2answer
连续特征常用方法,L2范数,question2answer
连续特征常用方法,平方根缩放,question2answer
连续特征常用方法,对数缩放,question2answer
连续特征常用方法,对数缩放适用于处理长尾分且取值为正数的数值变量,question2answer
连续特征常用方法,它将大端长尾压缩为短尾，并将小端进行延伸,question2answer
连续特征常用方法,可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)log(y),question2answer
连续特征常用方法,可以把有偏分布修正为近似正太分布,question2answer
连续特征常用方法,BoxCox转换,question2answer
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8mfjjwir3j309m038gln.jpg),question2answer
连续特征常用方法,通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,question2answer
连续特征常用方法,特征交叉,question2answer
连续特征常用方法,人为分段交叉,question2answer
连续特征常用方法,提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释,question2answer
连续特征常用方法,离散变量的交并补,question2answer
连续特征常用方法,连续变量的点积，attention类似,question2answer
连续特征常用方法,交叉中需要并行特征筛选的步骤,question2answer
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2rf5aawj30ka0j6q48.jpg),question2answer
连续特征常用方法,自动组合,question2answer
连续特征常用方法,FM/FFM中的矩阵点积,question2answer
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2un2ckzj30eb07jaan.jpg),question2answer
连续特征常用方法,NeuralNetwork里面的dense,question2answer
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2vb1nnij30co07lt94.jpg),question2answer
连续特征常用方法,条件选择,question2answer
连续特征常用方法,通过树或者类似的特征组合模型去做最低熵的特征选择,question2answer
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2t44a2mj30im0bt0tz.jpg),question2answer
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2v20wjbj30hs0dx0tq.jpg),question2answer
连续特征常用方法,非线性编码,question2answer
连续特征常用方法,核向量进行升维,question2answer
连续特征常用方法,树模型的叶子结点的stack,question2answer
连续特征常用方法,谱聚类/pca/svd等信息抽取编码,question2answer
连续特征常用方法,lda/EM等分布拟合表示,question2answer
离散特征常用方法,onehotencoder,question2answer
离散特征常用方法,分层编码,question2answer
离散特征常用方法,有一定规律的类别数据，邮政编码，手机号等等,question2answer
离散特征常用方法,计数编码,question2answer
离散特征常用方法,"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",question2answer
离散特征常用方法,"对异常值比较敏感,特征取值有可能冲突",question2answer
离散特征常用方法,计数排名编码,question2answer
离散特征常用方法,解决上述问题，以排名代替值,question2answer
离散特征常用方法,Embedding,question2answer
离散特征常用方法,"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",question2answer
离散特征常用方法,类别特征之间交叉组合,question2answer
离散特征常用方法,笛卡尔交叉,question2answer
离散特征常用方法,类别特征和数值特征之间交叉组合,question2answer
离散特征常用方法,均值、中位数、标准差、最大值和最小值,question2answer
离散特征常用方法,分位数、方差、vif值、分段冲量,question2answer
文本特征,预处理手段有哪些？,question2answer
文本特征,将字符转化为小写,question2answer
文本特征,分词,question2answer
文本特征,去除无用字符,question2answer
文本特征,繁体转中文,question2answer
文本特征,去除停用词,question2answer
文本特征,去除稀有词,question2answer
文本特征,半角全角切换,question2answer
文本特征,错词纠正,question2answer
文本特征,关键词标记,question2answer
文本特征,TFIDF,question2answer
文本特征,LDA,question2answer
文本特征,LSA,question2answer
文本特征,提取词根,question2answer
文本特征,词干提取,question2answer
文本特征,标点符号编码,question2answer
文本特征,文档特征,question2answer
文本特征,实体插入和提取,question2answer
文本特征,文本向量化,question2answer
文本特征,word2vec,question2answer
文本特征,glove,question2answer
文本特征,bert,question2answer
文本特征,文本相似性,question2answer
文本特征,如何做样本构造？,question2answer
文本特征,按标点切分,question2answer
文本特征,按句切分,question2answer
文本特征,对话session切分,question2answer
文本特征,按文章切分,question2answer
文本特征,按场景切分,question2answer
文本特征,分词过程中会考虑哪些方面？,question2answer
文本特征,词性标注,question2answer
文本特征,词形还原和词干提取,question2answer
文本特征,词形还原为了通用性特征的提取,question2answer
文本特征,词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,question2answer
文本特征,文本中的统计信息一般有哪些？,question2answer
文本特征,直接统计值：,question2answer
文本特征,文本的长度,question2answer
文本特征,单词个数,question2answer
文本特征,数字个数,question2answer
文本特征,字母个数,question2answer
文本特征,大小写单词个数,question2answer
文本特征,大小写字母个数,question2answer
文本特征,标点符号个数,question2answer
文本特征,特殊字符个数,question2answer
文本特征,数字占比,question2answer
文本特征,字母占比,question2answer
文本特征,特殊字符占比,question2answer
文本特征,不同词性个数,question2answer
文本特征,直接统计值的统计信息：,question2answer
文本特征,最小最大均值方差标准差,question2answer
文本特征,分位数，最早/最晚出现位置,question2answer
文本特征,直接对文本特征进行整理手段有哪些？,question2answer
文本特征,NGram模型,question2answer
文本特征,将文本转换为连续序列，扩充样本特征,question2answer
文本特征,连续语意的提取,question2answer
文本特征,TFIDF,question2answer
文本特征,"权重评分，去除掉一些低重要性的词，比如每篇文章都出现的""的""，""了""",question2answer
文本特征,LDA,question2answer
文本特征,主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,question2answer
文本特征,相似度,question2answer
文本特征,余弦相似度,question2answer
文本特征,Jaccard相似度,question2answer
文本特征,共现性,question2answer
文本特征,Levenshtein(编辑距离),question2answer
文本特征,文本近似程度,question2answer
文本特征,海林格距离,question2answer
文本特征,用来衡量概率分布之间的相似性,question2answer
文本特征,JSD,question2answer
文本特征,衡量prob1和prob2两个分布的相似程度,question2answer
文本特征,向量化,question2answer
文本特征,word2vec,question2answer
文本特征,glove,question2answer
文本特征,bert,question2answer
文本特征,文本处理有大量可以讲，可以谈的，以上只是做了一个最简单的汇总，详细的会在自然语言处理的专题一条一条分析，面试官也一定会就每个知识点进行展开,question2answer
画一个最简单的最快速能实现的框架,建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,question2answer
画一个最简单的最快速能实现的框架,建议从简单的开始，然后面试官说还有其他方法么？再做延展：,question2answer
画一个最简单的最快速能实现的框架,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mzet073zj31jw0u046i.jpg),question2answer
为什么要做特征选择？,耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,question2answer
为什么要做特征选择？,过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,question2answer
为什么要做特征选择？,共线性：单因子对目标的作用被稀释，解释力下降,question2answer
从哪些方面可以做特征选择？,方差，是的feature内的方向更大，对目标区分度提高更高贡献,question2answer
从哪些方面可以做特征选择？,相关性，与区分目标有高相关的特征才有意义,question2answer
既然说了两个方向，分别介绍一些吧,方差,question2answer
既然说了两个方向，分别介绍一些吧,移除低方差特征,question2answer
既然说了两个方向，分别介绍一些吧,移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,question2answer
既然说了两个方向，分别介绍一些吧,考虑有值数据中的占比，异常数据的占比，正常范围数据过少的数据也可以移除,question2answer
既然说了两个方向，分别介绍一些吧,相关性,question2answer
既然说了两个方向，分别介绍一些吧,单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,question2answer
既然说了两个方向，分别介绍一些吧,皮尔森相关系数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n46pedyoj303e019t8i.jpg),question2answer
既然说了两个方向，分别介绍一些吧,Fisher得分:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n4jajze8j31hc0u0dt2.jpg),question2answer
既然说了两个方向，分别介绍一些吧,假设检验,question2answer
既然说了两个方向，分别介绍一些吧,卡方检验,question2answer
既然说了两个方向，分别介绍一些吧,ANOVA,question2answer
既然说了两个方向，分别介绍一些吧,熵检验,question2answer
既然说了两个方向，分别介绍一些吧,互信息熵,question2answer
既然说了两个方向，分别介绍一些吧,"度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立",question2answer
既然说了两个方向，分别介绍一些吧,KL散度,question2answer
既然说了两个方向，分别介绍一些吧,相对熵,question2answer
是不是一定需要对缺失值处理？,当缺失值占比在可接受的范围以内的时候才需要进行填充，如果缺失值大于50%以上的时候，可以选择进行二分化，如果缺失值大于80%可以完整删除该列而不是强行去填充,question2answer
直接填充方法有哪些？,均值,question2answer
直接填充方法有哪些？,中位数,question2answer
直接填充方法有哪些？,众数,question2answer
直接填充方法有哪些？,分位数,question2answer
模型插值方法有哪些？及方法的问题,有效性存疑，取决于特征列数,question2answer
模型插值方法有哪些？及方法的问题,生成的插值来源于其他列的特征，是不是意味着插值的结果已经是和其他列的组合高相关,question2answer
如何直接离散化？,离散特征新增缺失的category,question2answer
hold位填充方法有哪些？,把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果,question2answer
hold位填充方法有哪些？,可以参考YouTube中的新商品向量生成逻辑,question2answer
hold位填充方法有哪些？,bert中的\[UNK]向量，\[unused]向量,question2answer
怎么理解分布补全？,如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,question2answer
random方法,在缺失量特别少(通常认为小于1%)的时候，可以随机生成,question2answer
总结,实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多,question2answer
总结,快速,question2answer
总结,对原始数据的前提假设最少，也不会影响到非缺失列,question2answer
总结,在深度学习中，hold位填充方法用的最多,question2answer
总结,在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,question2answer
总结,而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性,question2answer
常见决策树,|模型|ID3|C4.5|CART|,question2answer
常见决策树,|:|:|:|::|,question2answer
常见决策树,|结构|多叉树|多叉树|二叉树|,question2answer
常见决策树,|特征选择|信息增益|信息增益率|Gini系数/均方差|,question2answer
常见决策树,|连续值处理|不支持|支持|支持|,question2answer
常见决策树,|缺失值处理|不支持|支持|支持|,question2answer
常见决策树,|枝剪|不支持|支持|支持|,question2answer
简述决策树构建过程,1.构建根节点，将所有训练数据都放在根节点,question2answer
简述决策树构建过程,2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,question2answer
简述决策树构建过程,3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,question2answer
详述信息熵计算方法及存在问题,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925k1ns0yj305y018glg.jpg),question2answer
详述信息熵计算方法及存在问题,其中，D为数据全集，C为不同因变量类别k上的子集(传统意义上的y的种类),question2answer
详述信息增益计算方法,条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,question2answer
详述信息增益计算方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925puuv1cj308p01kdfr.jpg),question2answer
详述信息增益计算方法,信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：,question2answer
详述信息增益计算方法,"I(D,A)=H(D)H(D/A)",question2answer
详述信息增益计算方法,简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,question2answer
详述信息增益率计算方法,"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",question2answer
详述信息增益率计算方法,信息增益率：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9260xx5wej304b017q2r.jpg),question2answer
解释Gini系数,Gini系数二分情况下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92705elt9j308000q744.jpg),question2answer
解释Gini系数,对于决策树样本D来说，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9273px3t2j30a4018wee.jpg),question2answer
解释Gini系数,对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg),question2answer
ID3存在的问题,缺点：,question2answer
ID3存在的问题,存在偏向于选择取值较多的特征问题,question2answer
ID3存在的问题,连续值不支持,question2answer
ID3存在的问题,缺失值不支持,question2answer
ID3存在的问题,无法枝剪,question2answer
C4.5相对于ID3的改进点,主动进行的连续的特征离散化,question2answer
C4.5相对于ID3的改进点,比如m个样本的连续特征A有m个，先set在order，再两两组合求中间值，以该值点作为划分的待选点,question2answer
C4.5相对于ID3的改进点,**连续特征可以再后序特征划分中仍可继续参与计算**,question2answer
C4.5相对于ID3的改进点,缺失问题优化,question2answer
C4.5相对于ID3的改进点,训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,question2answer
C4.5相对于ID3的改进点,预测：直接跳过该节点，并将此样本划入所有子节点，划分后乘以系数计算，系数为不缺失部分的样本分布,question2answer
C4.5相对于ID3的改进点,采用预枝剪,question2answer
CART的连续特征改进点,分类情况下的变量特征选择,question2answer
CART的连续特征改进点,离散变量：二分划分,question2answer
CART的连续特征改进点,连续变量：和C4.5一致，如果当前节点为连续属性，则该属性后面依旧可以参与子节点的产生选择过程,question2answer
CART的连续特征改进点,回归情况下，连续变量不再采取中间值划分，采用最小方差法,question2answer
CART分类树建立算法的具体流程,我们的算法从根节点开始，用训练集递归的建立CART树。,question2answer
CART分类树建立算法的具体流程,对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,question2answer
CART分类树建立算法的具体流程,计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归,question2answer
CART分类树建立算法的具体流程,计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数,question2answer
CART分类树建立算法的具体流程,在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,question2answer
CART分类树建立算法的具体流程,递归1～4,question2answer
CART回归树建立算法的具体流程,其他部分都一样，在构建过程中遇到连续值的话，并不是利用C4.5中的中间值基尼系数的方式，而是采取了最小方差方法：,question2answer
CART回归树建立算法的具体流程,对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),question2answer
CART输出结果的逻辑？,回归树：利用最终叶子的均值或者中位数来作为输出结果,question2answer
CART输出结果的逻辑？,分类树：利用最终叶子的大概率的分类类别来作为输出结果,question2answer
CART树算法的剪枝过程是怎么样的？,目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,question2answer
CART树算法的剪枝过程是怎么样的？,当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,question2answer
CART树算法的剪枝过程是怎么样的？,当然，这是两种极端情况。一般来说，𝛼越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的𝛼，一定存在使损失函数𝐶𝛼(𝑇)最小的唯一子树。,question2answer
CART树算法的剪枝过程是怎么样的？,"由枝剪到根结点及不枝剪两种情况可得：𝛼=(𝐶(𝑇)−𝐶(𝑇𝑡))/(|𝑇𝑡|−1),C(T)为根结点误差",question2answer
CART树算法的剪枝过程是怎么样的？,计算出每个子树是否剪枝的阈值𝛼,question2answer
CART树算法的剪枝过程是怎么样的？,选择阈值𝛼集合中的最小值,question2answer
CART树算法的剪枝过程是怎么样的？,分别针对不同的最小值𝛼所对应的剪枝后的最优子树做交叉验证,question2answer
树形结构为何不需要归一化？,无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,question2answer
决策树的优缺点,优点：,question2answer
决策树的优缺点,缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散,question2answer
决策树的优缺点,可解释性强,question2answer
决策树的优缺点,算法对数据没有强假设,question2answer
决策树的优缺点,可以解决线性及非线性问题,question2answer
决策树的优缺点,有特征选择等辅助功能,question2answer
决策树的优缺点,缺点：,question2answer
决策树的优缺点,处理关联性数据比较薄弱,question2answer
决策树的优缺点,正负量级有偏样本的样本效果较差,question2answer
决策树的优缺点,单棵树的拟合效果欠佳，容易过拟合,question2answer
简单介绍SVM?,从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2·||W||·||W||)subjectto：y(wx+b)>=1，其中||·||为2范数,question2answer
简单介绍SVM?,然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,question2answer
简单介绍SVM?,最后再利用SMO（序列最小优化）来解决这个对偶问题,question2answer
什么叫最优超平面？,两类样本分别分割在该超平面的两侧,question2answer
什么叫最优超平面？,超平面两侧的点离超平面尽可能的远,question2answer
什么是支持向量？,在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。换句话说，就是超平面附近决定超平面位置的那些参与计算锁定平面位置的点,question2answer
SVM 和全部数据有关还是和局部数据有关?,局部,question2answer
加大训练数据量一定能提高SVM准确率吗？,支持向量的添加才会提高，否则无效,question2answer
如何解决多分类问题？,对训练器进行组合。其中比较典型的有一对一，和一对多,question2answer
可以做回归吗，怎么做？,可以，把loss函数变为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93jrdscfrj309l011dfo.jpg),question2answer
SVM 能解决哪些问题？,线性问题,question2answer
SVM 能解决哪些问题？,对于n为数据，找到n1维的超平面将数据分成2份。通过增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的,question2answer
SVM 能解决哪些问题？,非线性问题,question2answer
SVM 能解决哪些问题？,SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,question2answer
介绍一下你知道的不同的SVM分类器？,硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器,question2answer
介绍一下你知道的不同的SVM分类器？,软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器,question2answer
介绍一下你知道的不同的SVM分类器？,kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,question2answer
什么叫软间隔？,软间隔允许部分样本点不满足约束条件：1<y(wx+b),question2answer
SVM 软间隔与硬间隔表达式,硬间隔：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93ha4y4glj3094011a9w.jpg),question2answer
SVM 软间隔与硬间隔表达式,软间隔：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93hae5o8lj30d701f0sn.jpg),question2answer
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,svm原问题是：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),question2answer
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg),question2answer
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,"拉格朗日乘子法：求f的最小值时，有h=0的限制条件，那么就构造∑λh+f=Loss,作为新loss",question2answer
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件,question2answer
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,question2answer
为什么要把原问题转换为对偶问题？,因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,question2answer
为什么要把原问题转换为对偶问题？,引入了核函数,question2answer
为什么求解对偶问题更加高效？,原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题,question2answer
为什么求解对偶问题更加高效？,因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0,question2answer
alpha系数有多少个？,样本点的个数,question2answer
KKT限制条件，KKT条件有哪些，完整描述,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,question2answer
KKT限制条件，KKT条件有哪些，完整描述,KKT乘子λ>=0,question2answer
引入拉格朗日的优化方法后的损失函数解释,原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),question2answer
引入拉格朗日的优化方法后的损失函数解释,优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),question2answer
引入拉格朗日的优化方法后的损失函数解释,要求KKT乘子λ>=0,question2answer
核函数的作用是啥,核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,question2answer
核函数的种类和应用场景,线性核函数：主要用于线性可分的情形。参数少，速度快。,question2answer
核函数的种类和应用场景,多项式核函数：,question2answer
核函数的种类和应用场景,高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,question2answer
核函数的种类和应用场景,sigmoid核函数：,question2answer
核函数的种类和应用场景,拉普拉斯核函数：,question2answer
如何选择核函数,我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,question2answer
常用核函数的定义？,在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,question2answer
常用核函数的定义？,"1)线性：K(v1,v2)=<v1,v2>",question2answer
常用核函数的定义？,"2)多项式：K(v1,v2)=(r<v1,v2>+c)^n",question2answer
常用核函数的定义？,"3)Radialbasisfunction：K(v1,v2)=exp(r||v1v2||^2)",question2answer
常用核函数的定义？,"4)Sigmoid：tanh(r<v1,v2>+c)",question2answer
核函数需要满足什么条件？,Mercer定理：核函数矩阵是对称半正定的,question2answer
为什么在数据量大的情况下常常用lr代替核SVM？,计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),question2answer
为什么在数据量大的情况下常常用lr代替核SVM？,在使用核函数的时候参数假设全靠试，时间成本过高,question2answer
高斯核可以升到多少维？为什么,无穷维,question2answer
高斯核可以升到多少维？为什么,e的n次方的泰勒展开得到了一个无穷维度的映射,question2answer
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",线性问题：,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",线性：,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",逻辑回归，线性svm,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",非线性：,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",贝叶斯，决策树，核svm，DNN,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",数据问题：,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",数据量大特征多：,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",逻辑回归,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",决策树算法,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",数据量少特征少：,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",核svm,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",缺失值多：,question2answer
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",树模型,question2answer
Linear SVM 和 LR 有什么异同？,LR是参数模型，SVM为非参数模型。,question2answer
Linear SVM 和 LR 有什么异同？,LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,question2answer
Linear SVM 和 LR 有什么异同？,在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,question2answer
Linear SVM 和 LR 有什么异同？,LR的模型相对简单，在进行大规模线性分类时比较方便。,question2answer
损失函数是啥,"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",question2answer
最小二乘/梯度下降手推,最小二乘,question2answer
最小二乘/梯度下降手推,损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93458dklvj306l011t8j.jpg),question2answer
最小二乘/梯度下降手推,求导可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93489pnxxj3052014jr7.jpg),question2answer
最小二乘/梯度下降手推,使右侧为0可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934a6q83tj304300kdfm.jpg),question2answer
最小二乘/梯度下降手推,如果X点乘X的转置可逆则有唯一解，否则无法如此求解,question2answer
最小二乘/梯度下降手推,梯度下降,question2answer
最小二乘/梯度下降手推,损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg),question2answer
最小二乘/梯度下降手推,求导可得梯度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934mwtnodj307401fdfp.jpg),question2answer
介绍一下岭回归,加上l2的线性回归：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934px37g4j305b017glf.jpg),question2answer
介绍一下岭回归,在用最小二乘推导的过程和上面一样，最后在结果上进行了平滑，保证有解：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934t64beij304w00kmwy.jpg),question2answer
什么时候使用岭回归？,样本数少，或者样本重复程度高,question2answer
什么时候用Lasso回归？,特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,question2answer
请问从EM角度理解kmeans?,kmeans是两个步骤交替进行，可以分别看成E步和M步,question2answer
请问从EM角度理解kmeans?,M步中将每类的中心更新为分给该类各点的均值，可以认为是在「各类分布均为单位方差的高斯分布」的假设下，最大化似然值；,question2answer
请问从EM角度理解kmeans?,E步中将每个点分给中心距它最近的类（硬分配），可以看成是EM算法中E步（软分配）的近似,question2answer
为什么kmeans一定会收敛?,M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,question2answer
kmeans初始点除了随机选取之外的方法？,先层次聚类，再在不同层次上选取初始点进行kmeans聚类,question2answer
解释一下朴素贝叶斯中考虑到的条件独立假设,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92b15s6daj308x00rq2s.jpg),question2answer
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,贝叶斯公式是完整的数学公式P(A/B)=P(A)P(B/A)/P(B),question2answer
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,"朴素贝叶斯=贝叶斯公式+条件独立假设，在实际使用过程中，朴素贝叶斯完全只需要关注P(A,B)=P(A)P(B/A)即可",question2answer
朴素贝叶斯中出现的常见模型有哪些,多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,question2answer
朴素贝叶斯中出现的常见模型有哪些,因为统计次数，所以会出现0次可能，所以实际中进行了平滑操作,question2answer
朴素贝叶斯中出现的常见模型有哪些,先验平滑：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92bmactdlj303o0133yb.jpg),question2answer
朴素贝叶斯中出现的常见模型有哪些,后验平滑：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92borlh0nj3043018q2r.jpg),question2answer
朴素贝叶斯中出现的常见模型有哪些,两者形式非常像，区别就在先验平滑分母考虑的是平滑类别y个数，后验平滑分母考虑的是平滑特征对应特征x可选的个数,question2answer
朴素贝叶斯中出现的常见模型有哪些,高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),question2answer
朴素贝叶斯中出现的常见模型有哪些,高斯模型假设在对应类别下的每一维特征都服从高斯分布（正态分布）,question2answer
朴素贝叶斯中出现的常见模型有哪些,伯努利：伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次,question2answer
出现估计概率值为 0 怎么处理,拉普拉斯平滑,question2answer
朴素贝叶斯的优缺点？,优点：对小规模数据表现很好，适合多分类任务，适合增量式训练,question2answer
朴素贝叶斯的优缺点？,缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）,question2answer
朴素贝叶斯与 LR 区别？,生成模型和判别模型,question2answer
朴素贝叶斯与 LR 区别？,条件独立要求,question2answer
朴素贝叶斯与 LR 区别？,小数据集和大数据集,question2answer
logistic分布函数和密度函数，手绘大概的图像,分布函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93b9whhwuj306z01amwz.jpg),question2answer
logistic分布函数和密度函数，手绘大概的图像,密度函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93bdnikzbj306e01pjr8.jpg),question2answer
logistic分布函数和密度函数，手绘大概的图像,其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,question2answer
LR推导，基础5连问,基础公式,question2answer
LR推导，基础5连问,f(x)=wx+b,question2answer
LR推导，基础5连问,y=sigmoid(f(x)),question2answer
LR推导，基础5连问,可以看作是一次线性拟合+一次sigmoid的非线性变化,question2answer
LR推导，基础5连问,伯努利过程,question2answer
LR推导，基础5连问,对于lr来说事情只有发生和不发生两种可能，对于已知样本来说，满足伯努利的概率假设：,question2answer
LR推导，基础5连问,"p(y=1/x,θ)=h(θ,x)",question2answer
LR推导，基础5连问,"p(y=0/x,θ)=1h(θ,x)",question2answer
LR推导，基础5连问,"p(y/x,θ)=h(θ,x)^y·(1h(θ,x))^(1y)",question2answer
LR推导，基础5连问,第i个样本正确预测的概率如上可得,question2answer
LR推导，基础5连问,几率odds,question2answer
LR推导，基础5连问,数据特征下属于正例及反例的比值,question2answer
LR推导，基础5连问,ln(y/(1y)),question2answer
LR推导，基础5连问,极大似然,question2answer
LR推导，基础5连问,第i个样本正确预测的概率如上可得每条样本的情况下,question2answer
LR推导，基础5连问,综合全部样本发生的概率都要最大的话，采取极大似然连乘可得：,question2answer
LR推导，基础5连问,"∏(h(θ,x)^y·(1h(θ,x))^(1y))",question2answer
LR推导，基础5连问,损失函数,question2answer
LR推导，基础5连问,通常会对极大似然取对数，得到损失函数，方便计算,question2answer
LR推导，基础5连问,"∑ylogh(θ,x)+(1y)log(1h(θ,x))最大",question2answer
LR推导，基础5连问,"及1/m·∑ylogh(θ,x)+(1y)log(1h(θ,x))最小",question2answer
LR推导，基础5连问,梯度下降,question2answer
LR推导，基础5连问,损失函数求偏导，更新θ,question2answer
LR推导，基础5连问,θj+1=θj∆·∂Loss/∂θ=θj∆·1/m·∑x·(hy),question2answer
LR推导，基础5连问,∆为学习率,question2answer
梯度下降如何并行化？,首先需要理解梯度下降的更新公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cn8ok1fj307a01ft8k.jpg),question2answer
梯度下降如何并行化？,∑处的并行，不同样本在不同机器上进行计算，计算完再进行合并,question2answer
梯度下降如何并行化？,同一条样本不同特征维度进行拆分，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)处并行，把![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93csl2l0pj301400ia9t.jpg)内的xi和Wi拆分成块分别计算后合并，再把外层![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)同样拆分成若干块进行计算,question2answer
LR明明是分类模型为什么叫回归？,观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,question2answer
为什么LR可以用来做CTR预估？,1.点击行为为正向，未点击行为为负向，ctr需要得到点击行为的概率，lr可以产出正向行为的概率，完美match,question2answer
为什么LR可以用来做CTR预估？,2.实现简单，方便并行，计算迭代速度很快,question2answer
为什么LR可以用来做CTR预估？,3.可解释性强，可结合正则化等优化方法,question2answer
满足什么样条件的数据用LR最好？,特征之间尽可能独立,question2answer
满足什么样条件的数据用LR最好？,不独立所以我们把不独立的特征交叉了,question2answer
满足什么样条件的数据用LR最好？,还记得FM的思路？,question2answer
满足什么样条件的数据用LR最好？,离散特征,question2answer
满足什么样条件的数据用LR最好？,连续特征通常没有特别含义，31岁和32岁差在哪？,question2answer
满足什么样条件的数据用LR最好？,离散特征方便交叉考虑,question2answer
满足什么样条件的数据用LR最好？,在异常值处理上也更加方便,question2answer
满足什么样条件的数据用LR最好？,使的lr满足分布假设,question2answer
满足什么样条件的数据用LR最好？,什么分布假设？,question2answer
满足什么样条件的数据用LR最好？,在某种确定分类上的特征分布满足高斯分布,question2answer
满足什么样条件的数据用LR最好？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wh7dd6bkj310w034gmb.jpg),question2answer
满足什么样条件的数据用LR最好？,C1和C2为正负类，观测样本中该特征在正负类中出现概率的比值满足线性条件的前提就是P服从正太分布,question2answer
满足什么样条件的数据用LR最好？,实际中不满足的很多，不满足我们通常就离散化，oneHotEncode,question2answer
满足什么样条件的数据用LR最好？,此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中,question2answer
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,question2answer
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,思路二：Exponentialmodel的形式是这样的：假设第i个特征对第k类的贡献是![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmq11x6zj300s00f3y9.jpg)，则数据点![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmqpotqfj302700imwx.jpg)属于第k类的概率正比于![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmrp4qv4j305b00i0sj.jpg)。,question2answer
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,二分类上：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmsc1vfkj30jm036wet.jpg),question2answer
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,化简即为sigmoid,question2answer
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,以上思路源自：PRML（PatternRecognitionandMachineLearning）,question2answer
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,question2answer
利用几率odds的意义在哪？,直接对分类模型进行建模，前提假设为非常弱的指定类别上自变量的条件分布满足高斯,question2answer
利用几率odds的意义在哪？,由预测0/1的类别扩展到了预测01的概率值,question2answer
利用几率odds的意义在哪？,任意阶可导的优秀性质,question2answer
Sigmoid函数到底起了什么作用？,"数据规约：\[0,1]",question2answer
Sigmoid函数到底起了什么作用？,线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,question2answer
Sigmoid函数到底起了什么作用？,sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,question2answer
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,更新速度只与真实的x和y相关，与激活函数无关，更新平稳,question2answer
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,question2answer
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,mse下的lr损失函数非凸，难以得到解析解,question2answer
LR中若标签为+1和-1，损失函数如何推导？,way1:把01的sigmoid的lr结果Y映射为2y1，推导不变,question2answer
LR中若标签为+1和-1，损失函数如何推导？,"way2:把激活函数换成tanh，因为tanh的值域范围为\[1,1],满足结果，推导不变",question2answer
LR中若标签为+1和-1，损失函数如何推导？,way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,question2answer
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,question2answer
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了,question2answer
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,question2answer
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,question2answer
LR可以用核么？可以怎么用？,结论：可以，加l2正则项后可用,question2answer
LR可以用核么？可以怎么用？,原因：,question2answer
LR可以用核么？可以怎么用？,核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,question2answer
LR可以用核么？可以怎么用？,w拆解的z的线性组合中的系数α来源,question2answer
LR可以用核么？可以怎么用？,representertheorem的证明,question2answer
LR可以用核么？可以怎么用？,凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,question2answer
LR可以用核么？可以怎么用？,如何将将W*表示成β的形式带到我们最佳化的问题,question2answer
LR中的L1/L2正则项是啥？,"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",question2answer
LR中的L1/L2正则项是啥？,"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",question2answer
lr加l1还是l2好？,这个问题还可以换一个说法，l1和l2的各自作用。,question2answer
lr加l1还是l2好？,刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,question2answer
正则化是依据什么理论实现模型优化？,结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,question2answer
LR可以用来处理非线性问题么？,特征交叉，类似fm,question2answer
LR可以用来处理非线性问题么？,核逻辑回归，类似svm,question2answer
LR可以用来处理非线性问题么？,线性变换+非线性激活，类似neuralnetwork,question2answer
为什么LR需要归一化或者取对数?,**模型中对数据对处理一般都有一个标答是提升数据表达能力，也就是使数据含有的可分信息量更大**,question2answer
为什么LR需要归一化或者取对数?,工程角度：,question2answer
为什么LR需要归一化或者取对数?,加速收敛,question2answer
为什么LR需要归一化或者取对数?,提高计算效率,question2answer
为什么LR需要归一化或者取对数?,理论角度:,question2answer
为什么LR需要归一化或者取对数?,梯度下降过程稳定,question2answer
为什么LR需要归一化或者取对数?,使得数据在某类上更服从高斯分布，满足前提假设，这个是必须要答出来的,question2answer
为什么LR需要归一化或者取对数?,[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),question2answer
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,question2answer
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,离散后结合正则化可以进行特征筛选，更好防止过拟合,question2answer
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,数据的鲁棒性更好，不会因为无意义的连续值变动导致异常因素的影响，（31岁和32岁的差异在哪呢？）,question2answer
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,离散变量的计算相对于连续变量更快,question2answer
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,lr的output是彼此之间相对谁的可能性更高，而不是概率，概率是事情发生的可能，lr的output不代表可能,question2answer
LR对比万物？,lr和线性回归,question2answer
LR对比万物？,lr解用的极大似然，线性回归用的最小二乘,question2answer
LR对比万物？,lr用于分类，线性回归用于回归,question2answer
LR对比万物？,但两者都是广义线性回归GLM问题,question2answer
LR对比万物？,两者对非线性问题的处理能力都是欠佳的,question2answer
LR对比万物？,lr和最大熵,question2answer
LR对比万物？,在解决二分类问题是等同的,question2answer
LR对比万物？,lr和svm,question2answer
LR对比万物？,都可分类，都是判别式模型思路,question2answer
LR对比万物？,通常都是用正则化进行规约,question2answer
LR对比万物？,模型上,question2answer
LR对比万物？,lr是交叉熵，svm是HingeLoss,question2answer
LR对比万物？,lr是全量数据拟合，svm是支持向量拟合,question2answer
LR对比万物？,lr是参数估计有参数的前提假设，svm没有,question2answer
LR对比万物？,lr依赖的是极大似然，svm依赖的是距离,question2answer
LR对比万物？,lr和朴素贝叶斯,question2answer
LR对比万物？,如果朴素贝叶斯也有在某一类上的数据x满足高斯分布的假设前提，lr和朴素贝叶斯一致,question2answer
LR对比万物？,lr是判别模型，朴素贝叶斯是生成模型,question2answer
LR对比万物？,lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,question2answer
LR对比万物？,lr和最大熵模型,question2answer
LR对比万物？,本质没有区别,question2answer
LR对比万物？,最大熵模型在解决二分类问题就是逻辑回归,question2answer
LR对比万物？,最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,question2answer
LR梯度下降方法？,随机梯度下降,question2answer
LR梯度下降方法？,局部最优解，可跳出鞍点,question2answer
LR梯度下降方法？,计算快,question2answer
LR梯度下降方法？,批梯度下降,question2answer
LR梯度下降方法？,全局最优解,question2answer
LR梯度下降方法？,计算量大,question2answer
LR梯度下降方法？,mini批梯度下降,question2answer
LR梯度下降方法？,综合以上两种方法,question2answer
LR梯度下降方法？,除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,question2answer
LR的优缺点？,优点,question2answer
LR的优缺点？,简单，易部署，训练速度快,question2answer
LR的优缺点？,模型下限较高,question2answer
LR的优缺点？,可解释性强,question2answer
LR的优缺点？,缺点,question2answer
LR的优缺点？,只能线性可分,question2answer
LR的优缺点？,数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),question2answer
LR的优缺点？,模型上限较低,question2answer
除了做分类，你还会用LR做什么？,特征筛选，特征的系数决定该特征的重要性,question2answer
你有用过sklearn中的lr么？你用的是哪个包？,sklearn.linear_model.LogisticRegression,question2answer
看过源码么？为什么去看？,看部分参数的解释,question2answer
看过源码么？为什么去看？,比如dual、weight_class中的1:0还是0:1比,question2answer
看过源码么？为什么去看？,比如输出值的形式，输出的格式,question2answer
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,penalty是正则化，solver是函数优化方法,question2answer
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,question2answer
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,question2answer
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,l1和l2选择参考上面讲的正则化部分,question2answer
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,随机梯度下降在数据较少的时候最好别用，但是速度比较快。默认的是坐标轴下降法,question2answer
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,首先，决定是否为多分类的参数是multi_class,question2answer
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,在二分类的时候，multi和ovr和auto都是一样的,question2answer
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,question2answer
我的总结,逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,question2answer
我的总结,逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,question2answer
我的总结,逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,question2answer
我的总结,逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,question2answer
解释下随机森林?,随机森林=bagging+决策树,question2answer
解释下随机森林?,随机：特征选择随机+数据采样随机,question2answer
解释下随机森林?,特征随机是在决策树**每个结点上选择的时候随机**，并不是在每棵树创建的时候随机,question2answer
解释下随机森林?,每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,question2answer
解释下随机森林?,数据采样，是有放回的采样,question2answer
解释下随机森林?,1个样本**未被选到**的概率为p=(11/N)^N=1/e，即为OOB,question2answer
解释下随机森林?,森林：多决策树组合,question2answer
解释下随机森林?,可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,question2answer
随机森林用的是什么树？,CART树,question2answer
随机森林的生成过程？,生成单棵决策树,question2answer
随机森林的生成过程？,随机选取样本,question2answer
随机森林的生成过程？,从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,question2answer
随机森林的生成过程？,不需要剪枝，直到该节点的所有训练样例都属于同一类,question2answer
随机森林的生成过程？,生成若干个决策树,question2answer
解释下随机森林节点的分裂策略？,Gini系数,question2answer
解释下随机森林节点的分裂策略？,在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),question2answer
随机森林的损失函数是什么？,"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",question2answer
随机森林的损失函数是什么？,回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,question2answer
随机森林的损失函数是什么？,参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),question2answer
为了防止随机森林过拟合可以怎么做?,增加树的数量,question2answer
为了防止随机森林过拟合可以怎么做?,增加叶子结点的数据数量,question2answer
为了防止随机森林过拟合可以怎么做?,bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),question2answer
为了防止随机森林过拟合可以怎么做?,随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,question2answer
随机森林特征选择的过程？,特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,question2answer
随机森林特征选择的过程？,通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）,question2answer
随机森林特征选择的过程？,是使用uniform或者gaussian抽取随机值替换原特征,question2answer
是否用过随机森林，有什么技巧?,除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,question2answer
RF的参数有哪些，如何调参？,要调整的参数主要是n_estimators和max_features,question2answer
RF的参数有哪些，如何调参？,n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,question2answer
RF的参数有哪些，如何调参？,max_features是分割节点时考虑的特征的随机子集的大小。这个值越低，方差减小得越多，但是偏差的增大也越多,question2answer
RF的参数有哪些，如何调参？,回归：max_features=n_features,question2answer
RF的参数有哪些，如何调参？,分类：max_features=sqrt(n_features),question2answer
RF的参数有哪些，如何调参？,其他参数中,question2answer
RF的参数有哪些，如何调参？,class_weight也可以调整正负样本的权重,question2answer
RF的参数有哪些，如何调参？,max_depth=None和min_samples_split=2结合，为不限制生成一个不修剪的完全树,question2answer
RF的优缺点 ？,优点:,question2answer
RF的优缺点 ？,不同决策树可以由不同主机并行训练生成，效率很高,question2answer
RF的优缺点 ？,随机森林算法继承了CART的优点,question2answer
RF的优缺点 ？,将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题,question2answer
RF的优缺点 ？,缺点：,question2answer
RF的优缺点 ？,没有严格数学理论支持,question2answer
介绍一下Boosting的思想？,初始化训练一个弱学习器，初始化下的各条样本的权重一致,question2answer
介绍一下Boosting的思想？,根据上一个弱学习器的结果，调整权重，使得错分的样本的权重变得更高,question2answer
介绍一下Boosting的思想？,基于调整后的样本及样本权重训练下一个弱学习器,question2answer
介绍一下Boosting的思想？,预测时直接串联综合各学习器的加权结果,question2answer
最小二乘回归树的切分过程是怎么样的？,回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值,question2answer
最小二乘回归树的切分过程是怎么样的？,分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性,question2answer
最小二乘回归树的切分过程是怎么样的？,属性上有多个值，则需要遍历所有可能的属性值，挑选使平方误差最小的划分属性值作为本属性的划分值,question2answer
最小二乘回归树的切分过程是怎么样的？,递归重复以上步骤，直到满足叶子结点上值的要求,question2answer
有哪些直接利用了Boosting思想的树模型？,adaboost，gbdt等等,question2answer
gbdt和boostingtree的boosting分别体现在哪里？,boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,question2answer
gbdt和boostingtree的boosting分别体现在哪里？,gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,question2answer
gbdt的中的tree是什么tree？有什么特征？,Carttree，但是都是回归树,question2answer
常用回归问题的损失函数？,mse:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bmopzfjj303s011t8i.jpg),question2answer
常用回归问题的损失函数？,负梯度：yh(x),question2answer
常用回归问题的损失函数？,初始模型F0由目标变量的平均值给出,question2answer
常用回归问题的损失函数？,绝对损失:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bne7cipj303400q742.jpg),question2answer
常用回归问题的损失函数？,负梯度：sign(yh(x)),question2answer
常用回归问题的损失函数？,初始模型F0由目标变量的中值给出,question2answer
常用回归问题的损失函数？,Huber损失：mse和绝对损失的结合,question2answer
常用回归问题的损失函数？,负梯度：yh(x)和sign(yh(x))分段函数,question2answer
常用回归问题的损失函数？,它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,question2answer
常用分类问题的损失函数？,对数似然损失函数,question2answer
常用分类问题的损失函数？,"二元且标签y属于{1,+1}：𝐿(𝑦,𝑓(𝑥))=𝑙𝑜𝑔(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥)))",question2answer
常用分类问题的损失函数？,负梯度：y/(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥))),question2answer
常用分类问题的损失函数？,多元：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94cj1yce6j306x01iglg.jpg),question2answer
常用分类问题的损失函数？,"指数损失函数:𝐿(𝑦,𝑓(𝑥))=𝑒𝑥𝑝(−𝑦𝑓(𝑥))",question2answer
常用分类问题的损失函数？,负梯度：y·𝑒𝑥𝑝(−𝑦𝑓(𝑥)),question2answer
常用分类问题的损失函数？,除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,question2answer
什么是gbdt中的损失函数的负梯度？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg),question2answer
什么是gbdt中的损失函数的负梯度？,当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,question2answer
如何用损失函数的负梯度实现gbdt？,"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",question2answer
如何用损失函数的负梯度实现gbdt？,构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),question2answer
如何用损失函数的负梯度实现gbdt？,"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值𝑐𝑡𝑗,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",question2answer
如何用损失函数的负梯度实现gbdt？,首先，根据feature切分后的损失均方差大小，选取最优的特征切分,question2answer
如何用损失函数的负梯度实现gbdt？,其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,question2answer
如何用损失函数的负梯度实现gbdt？,这样就完整的构造出一棵树：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bfr5cn5j303d01kjr6.jpg),question2answer
如何用损失函数的负梯度实现gbdt？,本轮最终得到的强学习器的表达式如下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94binx5prj307o01k0sl.jpg),question2answer
拟合损失函数的负梯度为什么是可行的？,泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg),question2answer
拟合损失函数的负梯度为什么是可行的？,m轮树模型可以写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h8zovulj305v00iglf.jpg),question2answer
拟合损失函数的负梯度为什么是可行的？,"对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)",question2answer
拟合损失函数的负梯度为什么是可行的？,"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",question2answer
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（𝛽）,question2answer
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,question2answer
Shrinkage收缩的作用？,每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,question2answer
Shrinkage收缩的作用？,原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94i9bokjzj306g00iglf.jpg),question2answer
Shrinkage收缩的作用？,Shrinkage：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94iawlfq3j307600it8j.jpg),question2answer
feature属性会被重复多次使用么？,会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,question2answer
gbdt如何进行正则化的？,子采样,question2answer
gbdt如何进行正则化的？,每一棵树基于原始原本的一个子集进行训练,question2answer
gbdt如何进行正则化的？,rf是有放回采样，gbdt是无放回采样,question2answer
gbdt如何进行正则化的？,特征子采样可以来控制模型整体的方差,question2answer
gbdt如何进行正则化的？,利用Shrinkage收缩，控制每一棵子树的贡献度,question2answer
gbdt如何进行正则化的？,每棵Cart树的枝剪,question2answer
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受,question2answer
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,bagging，关注于提升分类器的泛化能力,question2answer
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,boosting，关注于提升分类器的精度,question2answer
gbdt的优缺点？,优点：,question2answer
gbdt的优缺点？,数据要求比较低，不需要前提假设，能处理缺失值，连续值，离散值,question2answer
gbdt的优缺点？,使用一些健壮的损失函数，对异常值的鲁棒性非常强,question2answer
gbdt的优缺点？,调参相对较简单,question2answer
gbdt和randomforest区别？,相同：,question2answer
gbdt和randomforest区别？,都是多棵树的组合,question2answer
gbdt和randomforest区别？,不同：,question2answer
gbdt和randomforest区别？,RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,question2answer
gbdt和randomforest区别？,gbdt对异常值比rf更加敏感,question2answer
gbdt和randomforest区别？,gbdt是串行，rf是并行,question2answer
gbdt和randomforest区别？,gbdt是cart回归树，rf是cart分类回归树都可以,question2answer
gbdt和randomforest区别？,gbdt是提高降低偏差提高性能，rf是通过降低方差提高性能,question2answer
gbdt和randomforest区别？,gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,question2answer
GBDT和LR的差异？,从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,question2answer
GBDT和LR的差异？,当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,question2answer
XGboost缺点,每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,question2answer
XGboost缺点,预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,question2answer
XGboost缺点,levelwise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数,question2answer
LightGBM对Xgboost的优化,将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,question2answer
LightGBM对Xgboost的优化,优点：时间开销由O(features)降低到O(bins),question2answer
LightGBM对Xgboost的优化,缺点：很多数据精度被丢失，相当于用了正则,question2answer
LightGBM对Xgboost的优化,利用leafwise代替levelwise,question2answer
LightGBM对Xgboost的优化,每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环,question2answer
LightGBM对Xgboost的优化,直方图做差加速,question2answer
LightGBM亮点,单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,question2answer
LightGBM亮点,"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,显示的把树模型复杂度作为正则项加到优化目标中,question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,优化目标计算中用到二阶泰勒展开代替一阶，更加准确,question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,实现了分裂点寻找近似算法,question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,暴力枚举,question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,近似算法（分桶）,question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,更加高效和快速,question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,数据事先排序并且以block形式存储，有利于并行计算,question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,基于分布式通信框架rabit，可以运行在MPI和yarn上,question2answer
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,实现做了面向体系结构的优化，针对cache和内存做了性能优化,question2answer
xgboost和gbdt的区别？,模型优化上：,question2answer
xgboost和gbdt的区别？,基模型的优化：,question2answer
xgboost和gbdt的区别？,gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),question2answer
xgboost和gbdt的区别？,损失函数上的优化：,question2answer
xgboost和gbdt的区别？,gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,question2answer
xgboost和gbdt的区别？,gbdt没有在loss中带入结点个数和预测值的正则项,question2answer
xgboost和gbdt的区别？,特征选择上的优化：,question2answer
xgboost和gbdt的区别？,实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,question2answer
xgboost和gbdt的区别？,节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权,question2answer
xgboost和gbdt的区别？,正则化的优化：,question2answer
xgboost和gbdt的区别？,特征采样,question2answer
xgboost和gbdt的区别？,样本采样,question2answer
xgboost和gbdt的区别？,工程优化上：,question2answer
xgboost和gbdt的区别？,xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,question2answer
xgboost和gbdt的区别？,"cacheaware,outofcorecomputation",question2answer
xgboost和gbdt的区别？,支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit,question2answer
xgboost优化目标/损失函数改变成什么样？,原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mjezeisj307401fmx0.jpg),question2answer
xgboost优化目标/损失函数改变成什么样？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒一阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mrtqxv0j30480173yc.jpg),question2answer
xgboost优化目标/损失函数改变成什么样？,改变：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mldvhz5j30ay01k3yf.jpg),question2answer
xgboost优化目标/损失函数改变成什么样？,J为叶子结点的个数，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mo56g1vj300o00e0s1.jpg)为第j个叶子结点中的最优值,question2answer
xgboost优化目标/损失函数改变成什么样？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒二阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mtjds7qj309r0193yg.jpg),question2answer
xgboost如何使用MAE或MAPE作为目标函数？,MAE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mxhhvg8j303l011q2q.jpg),question2answer
xgboost如何使用MAE或MAPE作为目标函数？,MAPE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mx7uyuej303f0170sj.jpg),question2answer
xgboost如何使用MAE或MAPE作为目标函数？,利用可导的函数逼近MAE或MAPE,question2answer
xgboost如何使用MAE或MAPE作为目标函数？,mse,question2answer
xgboost如何使用MAE或MAPE作为目标函数？,Huberloss,question2answer
xgboost如何使用MAE或MAPE作为目标函数？,PseudoHuberloss,question2answer
xgboost如何寻找分裂节点的候选集？,暴力枚举,question2answer
xgboost如何寻找分裂节点的候选集？,法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大,question2answer
xgboost如何寻找分裂节点的候选集？,近似算法（approx）,question2answer
xgboost如何寻找分裂节点的候选集？,近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,question2answer
xgboost如何寻找分裂节点的候选集？,离散值直接分桶,question2answer
xgboost如何寻找分裂节点的候选集？,连续值分位数分桶,question2answer
xgboost如何处理缺失值？,训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个,question2answer
xgboost如何处理缺失值？,预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,question2answer
xgboost在计算速度上有了哪些点上提升？,特征预排序,question2answer
xgboost在计算速度上有了哪些点上提升？,按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,question2answer
xgboost在计算速度上有了哪些点上提升？,block可以仅存放样本的索引，而不是样本本身，这样节省了大量的存储空间,question2answer
xgboost特征重要性是如何得到的？,’weight‘：代表着某个特征被选作分裂结点的次数；,question2answer
xgboost特征重要性是如何得到的？,’gain‘：使用该特征作为分类结点的信息增益；,question2answer
xgboost特征重要性是如何得到的？,’cover‘：某特征作为划分结点，覆盖样本总数的平均值；,question2answer
XGBoost中如何对树进行剪枝？,在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,question2answer
XGBoost中如何对树进行剪枝？,在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,question2answer
XGBoost中如何对树进行剪枝？,当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂,question2answer
XGBoost中如何对树进行剪枝？,XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,question2answer
XGBoost模型如果过拟合了怎么解决？,直接修改模型：,question2answer
XGBoost模型如果过拟合了怎么解决？,降低树的深度,question2answer
XGBoost模型如果过拟合了怎么解决？,增大叶子结点的权重,question2answer
XGBoost模型如果过拟合了怎么解决？,增大惩罚系数,question2answer
XGBoost模型如果过拟合了怎么解决？,subsample的力度变大，降低异常点的影响,question2answer
XGBoost模型如果过拟合了怎么解决？,减小learningrate，提高estimator,question2answer
xgboost如何调参数？,先确定learningrate和estimator,question2answer
xgboost如何调参数？,再确定每棵树的基本信息，max_depth和min_child_weight,question2answer
xgboost如何调参数？,再确定全局信息：比如最小分裂增益，子采样参数，正则参数,question2answer
xgboost如何调参数？,重新降低learningrate，得到最优解,question2answer
Attention对比RNN和CNN，分别有哪点你觉得的优势？,对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,question2answer
Attention对比RNN和CNN，分别有哪点你觉得的优势？,对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序,question2answer
写出Attention的公式？,![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986pbyaj0j308t019t8l.jpg),question2answer
解释你怎么理解Attention的公式的？,"Q:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986rjgs7qj301400g741.jpg),K:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986rz9d1ej301a00g741.jpg),V:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986shjz4tj301900g741.jpg)",question2answer
解释你怎么理解Attention的公式的？,首先，我们可以理解为Attention把input重新进行了一轮编码，获得一个新的序列,question2answer
解释你怎么理解Attention的公式的？,除以![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98734eqn7j300y00na9t.jpg)的目的是为了平衡qk的值，避免softmax之后过小,question2answer
解释你怎么理解Attention的公式的？,qk除了点击还可以直接拼接再内接一个参数变量等等,question2answer
解释你怎么理解Attention的公式的？,MultiAttention只是重复了h次的Attention，最后把结果进行拼接,question2answer
Attention模型怎么避免词袋模型的顺序问题的困境的？,增加了positionEmbedding,question2answer
Attention模型怎么避免词袋模型的顺序问题的困境的？,可以直接随机初始化,question2answer
Attention模型怎么避免词袋模型的顺序问题的困境的？,也可以参考Google的sin/cos位置初始化方法,question2answer
Attention模型怎么避免词袋模型的顺序问题的困境的？,如此选取的原因之一是sin(a+b)=sin(a)cos(b)+cos(a)sin(b)。这很好的保证了位置p+k可以表示成p的线性变换，相对位置可解释,question2answer
"Attention机制，里面的q,k,v分别代表什么？",Q：指的是query，相当于decoder的内容,question2answer
"Attention机制，里面的q,k,v分别代表什么？",K：指的是key，相当于encoder的内容,question2answer
"Attention机制，里面的q,k,v分别代表什么？",V：指的是value，相当于encoder的内容,question2answer
"Attention机制，里面的q,k,v分别代表什么？",q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,question2answer
为什么self-attention可以替代seq2seq？,seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,question2answer
为什么self-attention可以替代seq2seq？,selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,question2answer
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,question2answer
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,"针对Q和K中的每一维i都有qi和ki相互独立且均值0方差1，不妨记![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envjoy8oj301h00gdfl.jpg),![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envuw3p0j301g00ga9t.jpg)",question2answer
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,E(XY)=E(X)E(Y)=0,question2answer
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9enzh4vzvj30gh017t8t.jpg),question2answer
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,question2answer
你觉得bn过程是什么样的？,按batch进行期望和标准差计算,question2answer
你觉得bn过程是什么样的？,对整体数据进行标准化,question2answer
你觉得bn过程是什么样的？,对标准化的数据进行线性变换,question2answer
你觉得bn过程是什么样的？,变换系数需要学习,question2answer
手写一下bn过程？,"mu=1.0*np.sum(X,axis=0)/X.shape\[0]",question2answer
手写一下bn过程？,Xmu=Xmu,question2answer
手写一下bn过程？,sq=Xmu**2,question2answer
手写一下bn过程？,"var=1.0*np.sum(sq,axis=0)/X.shape\[0]",question2answer
手写一下bn过程？,out=alhpa*(XXmu)/np.sqrt(var+eps)+beta,question2answer
知道LN么？讲讲原理,和bn过程近似，只是作用的方向是在维度上，而不是batch上,question2answer
知道LN么？讲讲原理,这样做的好处就是不会受到batch大小不一致的影响,question2answer
介绍残差网络,常见结构，CV里面用的比较多,question2answer
介绍残差网络,y=F(x)+x,question2answer
介绍残差网络,y=F(x)+indentity`*`x,question2answer
残差网络为什么能解决梯度消失的问题,![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is4x344xj304j01jglf.jpg),question2answer
残差网络为什么能解决梯度消失的问题,![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8cigikj305s0180sl.jpg),question2answer
残差网络为什么能解决梯度消失的问题,虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,question2answer
残差网络残差作用,防止梯度消失,question2answer
残差网络残差作用,恒等映射使得网络突破层数限制，避免网络退化,question2answer
残差网络残差作用,对输出的变化更敏感,question2answer
残差网络残差作用,X=5;F(X)=5.1;F(X)=H(X)+X=>H(X)=0.1,question2answer
残差网络残差作用,X=5;F(X)=5.2;F(X)=H(X)+X=>H(X)=0.2,question2answer
残差网络残差作用,H(X)变换了100%，去掉相同的主体部分，从而突出微小的变化,question2answer
你平时有用过么？或者你在哪些地方遇到了,我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络,question2answer
你平时有用过么？或者你在哪些地方遇到了,Bert和Transform中attention部分残差网络用的比较频繁,question2answer
Bert的双向体现在什么地方？,mask+attention，mask的word结合全部其他encoderword的信息,question2answer
Bert的是怎样实现mask构造的？,MLM：将完整句子中的部分字mask，预测该mask词,question2answer
Bert的是怎样实现mask构造的？,NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,question2answer
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,question2answer
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,随机替换也帮助训练修正了\[unused]和\[UNK],question2answer
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,强迫文本记忆上下文信息,question2answer
为什么BERT有3个嵌入层，它们都是如何实现的？,input_id是语义表达，和传统的w2v一样，方法也一样的lookup,question2answer
为什么BERT有3个嵌入层，它们都是如何实现的？,"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",question2answer
为什么BERT有3个嵌入层，它们都是如何实现的？,"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",question2answer
bert的损失函数？,"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",question2answer
bert的损失函数？,"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",question2answer
bert的损失函数？,MLM+NSP即为最后的损失,question2answer
手写一个multi-head attention？,"tf.multal(tf.nn.softmax(tf.multiply(tf.multal(q,k,transpose_b=True),1/math.sqrt(float(size_per_head)))),v)",question2answer
长文本预测如何构造Tokens？,headonly：保存前510个token（留两个位置给\[CLS]和\[SEP]）,question2answer
长文本预测如何构造Tokens？,tailonly：保存最后510个token,question2answer
长文本预测如何构造Tokens？,head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,question2answer
你用过什么模块？bert流程是怎么样的？,modeling.py,question2answer
你用过什么模块？bert流程是怎么样的？,"首先定义处理好输入的tokens的对应的id作为input_id,因为不是训练所以input_mask和segment_id都是采取默认的1即可",question2answer
你用过什么模块？bert流程是怎么样的？,在通过embedding_lookup把input_id向量化，如果存在句子之间的位置差异则需要对segment_id进行处理，否则无操作；再进行position_embedding操作,question2answer
你用过什么模块？bert流程是怎么样的？,进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,question2answer
你用过什么模块？bert流程是怎么样的？,输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],question2answer
知道分词模块：FullTokenizer做了哪些事情么？,BasicTokenizer：根据空格等进行普通的分词,question2answer
知道分词模块：FullTokenizer做了哪些事情么？,包括了一些预处理的方法：去除无意义词，跳过'\t'这些词，unicode变换，中文字符筛选等等,question2answer
知道分词模块：FullTokenizer做了哪些事情么？,WordpieceTokenizer：前者的结果再细粒度的切分为WordPiece,question2answer
知道分词模块：FullTokenizer做了哪些事情么？,中文不处理，因为有词缀一说：解决OOV,question2answer
Bert中如何获得词意和句意？,get_pooled_out代表了涵盖了整条语句的信息,question2answer
Bert中如何获得词意和句意？,get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,question2answer
源码中Attention后实际的流程是如何的？,Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,question2answer
源码中Attention后实际的流程是如何的？,所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output,question2answer
为什么要在Attention后使用残差结构？,残差结构能够很好的消除层数加深所带来的信息损失问题,question2answer
平时用官方Bert包么？耗时怎么样？,第三方：bert_serving,question2answer
平时用官方Bert包么？耗时怎么样？,官方：bert_base,question2answer
平时用官方Bert包么？耗时怎么样？,耗时：64GTesla，64max_seq_length，8090doc/s,question2answer
平时用官方Bert包么？耗时怎么样？,在线预测只能一条一条的入参，实际上在可承受的计算量内batch越大整体的计算性能性价比越高,question2answer
你觉得BERT比普通LM的新颖点？,mask机制,question2answer
你觉得BERT比普通LM的新颖点？,next_sentence_predict机制,question2answer
elmo、GPT、bert三者之间有什么区别？,特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,question2answer
elmo、GPT、bert三者之间有什么区别？,单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,question2answer
elmo、GPT、bert三者之间有什么区别？,GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,question2answer
阐述CRF原理？,"首先X,Y是随机变量，P(Y/X)是给定X条件下Y的条件概率分布",question2answer
阐述CRF原理？,如果Y满足马尔可夫满足马尔科夫性，及不相邻则条件独立,question2answer
阐述CRF原理？,则条件概率分布P(Y|X)为条件随机场CRF,question2answer
线性链条件随机场的公式是？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9ah66y1nxj30cr015747.jpg),question2answer
CRF与HMM区别?,"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",question2answer
CRF与HMM区别?,CRF是无向图，HMM是有向图,question2answer
CRF与HMM区别?,CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,question2answer
Bert+crf中的各部分作用详解？,Bert把中文文本进行了embedding，得到每个字的表征向量,question2answer
Bert+crf中的各部分作用详解？,dense操作得到了每个文本文本对应的未归一化的tag概率,question2answer
Bert+crf中的各部分作用详解？,CRF在选择每个词的tag的过程其实就是一个最优Tag路径的选择过程,question2answer
Bert+crf中的各部分作用详解？,CRF层能从训练数据中获得约束性的规则,question2answer
Bert+crf中的各部分作用详解？,比如开始都是以xxxB，中间都是以xxxI，结尾都是以xxxE,question2answer
Bert+crf中的各部分作用详解？,比如在只有label1I，label2I..的情况下，不会出现label1B,question2answer
GolVe的损失函数？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9achsr4agj3094016a9x.jpg),question2answer
解释GolVe的损失函数？,"其实，一句话解释就是想构造一个向量表征方式，使得向量的点击和共现矩阵中的对应关系一致。因为共现矩阵中的对应关系证明了，存在i，k，j三个不同的文本，如果i和k相关，j和k相关，那么p(i,j)=p(j,k)近似于1，其他情况都过大和过小。",question2answer
为什么GolVe会用的相对比W2V少？,GloVe算法本身使用了全局信息，自然内存费的也就多一些,question2answer
为什么GolVe会用的相对比W2V少？,公现矩阵，NXN的，N为词袋量,question2answer
为什么GolVe会用的相对比W2V少？,W2V的工程实现结果相对来说支持的更多，比如most_similarty等功能,question2answer
如何处理未出现词？,"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",question2answer
详述LDA原理？,从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg),question2answer
详述LDA原理？,多项式分布的共轭分布是狄利克雷分布,question2answer
详述LDA原理？,二项式分布的共轭分布是Beta分布,question2answer
详述LDA原理？,从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),question2answer
详述LDA原理？,从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg),question2answer
详述LDA原理？,从词语的多项式分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)中采样最终生成词语![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8uydisuj300n00e0s0.jpg),question2answer
详述LDA原理？,文档里某个单词出现的概率可以用公式表示：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b9y6avdtj306e01jdfo.jpg),question2answer
详述LDA原理？,采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,吉布斯采样,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	先随机给每个词附上主题,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	有了联合概率分布，去除词wi后，就可以得到其他词主题条件概率分布,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	根据条件概率分布使用坐标轮换的吉布斯采样方法，得到词对应的平稳矩阵及词对应的主题,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,通常会引申出如下几个问题：,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	MCMC中什么叫做蒙特卡洛方法？,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		通常用于求概率密度的积分,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		用已知分布去评估未知分布,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		rejectacpect过程,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	马尔科夫链收敛性质？,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		非周期性，不能出现死循环,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		连通性，不能有断点,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	MCMC中什么叫做马尔科夫链采样过程？,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		再根据平稳矩阵后的条件概率p(x/xt)得到平稳分布的样本集(xn+1,xn+2...)",question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	给定平稳矩阵如何得到概率分布样本集？,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		MC采样,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			给定任意的转移矩阵Q，已知π(i)p(i,j)=π(j)p(j,i)，近似拟合π(i)Q(i,j)a(i,j)=π(j)Q(j,i)a(j,i)",question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,			根据Q的条件概率Q(x/xt)得到xt+1,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,			u~uniform,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			u<π(xt+1)Q(xt+1,xt)则accept，就和蒙特模拟一样否则xt+1=xt",question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			(xt,xt+1...)代表着我们的分布样本集",question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		MH采样,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度",question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		Gibbs采样,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,			同上，差别在固定n−1个特征在某一个特征采样及坐标轮换采样,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	什么叫做坐标转换采样？,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		平面上任意两点满足细致平稳条件π(A)P(A>B)=π(B)P(B>A),question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		从条件概率分布P(x2|x(t)1)中采样得到样本x(t+1)2,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		从条件概率分布P(x1|x(t+1)2)中采样得到样本x(t+1)1,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		其为一对样本，有点像Lasso回归中的固定n1维特征求一维特征求极值的思路,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,变分推断EM算法,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	变分推断在于隐藏变量没法直接求，**用三个独立分布的变分分步去拟合三个隐藏变量的条件分布**,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,	EM过程,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		E：最小化相对熵，偏导为0得到变分参数,question2answer
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值,question2answer
LDA的共轭分布解释下?,以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,question2answer
PLSA和LDA的区别?,LDA是加了狄利克雷先验的PLSA,question2answer
PLSA和LDA的区别?,PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,question2answer
PLSA和LDA的区别?,LDA是贝叶斯思想，PLSA是MLE,question2answer
怎么确定LDA的topic个数,对文档d属于哪个topic有多不确定，这个不确定程度就是Perplexity,question2answer
怎么确定LDA的topic个数,多次尝试，调优perplexitytopicnumber曲线,question2answer
怎么确定LDA的topic个数,困惑度越小，越容易过拟合,question2answer
怎么确定LDA的topic个数,某个词属于某个主题的困惑度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b7zjns8uj305i012jr7.jpg)，某个文章的困惑度即为词的连乘：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b83z3d22j304q01dweb.jpg),question2answer
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA比较是doc，word2vec是词,question2answer
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,question2answer
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,question2answer
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,question2answer
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,通常alpha为1/k，k为类别数，beta一般为0.01,question2answer
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,question2answer
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,question2answer
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,chucksize大一些更新的过程比较平稳，收敛更加平稳,question2answer
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,迭代次数一般不超过2000次，200万doc大约在2300次收敛,question2answer
从隐藏层到输出的Softmax层的计算有哪些方法？,层次softmax,question2answer
从隐藏层到输出的Softmax层的计算有哪些方法？,负采样,question2answer
层次softmax流程？,构造HuffmanTree,question2answer
层次softmax流程？,最大化对数似然函数,question2answer
层次softmax流程？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9adl5ex45j305g00qmwz.jpg),question2answer
层次softmax流程？,输入层：是上下文的词语的词向量,question2answer
层次softmax流程？,投影层：对其求和，所谓求和，就是简单的向量加法,question2answer
层次softmax流程？,输出层：输出最可能的word,question2answer
层次softmax流程？,沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,question2answer
层次softmax流程？,对每层每个变量求偏导，参考sgd,question2answer
负采样流程？,统计每个词出现对概率，丢弃词频过低对词,question2answer
负采样流程？,每次选择softmax的负样本的时候，从丢弃之后的词库里选择（选择是需要参考出现概率的）,question2answer
负采样流程？,负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,question2answer
word2vec两种方法各自的优势?,**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,question2answer
word2vec两种方法各自的优势?,"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",question2answer
怎么衡量学到的embedding的好坏?,从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,question2answer
怎么衡量学到的embedding的好坏?,对item2vec得到的词向量进行聚类或者可视化,question2answer
word2vec和glove区别？,word2vec是基于邻近词共现，glove是基于全文共现,question2answer
word2vec和glove区别？,word2vec利用了负采样或者层次softmax加速，相对更快,question2answer
word2vec和glove区别？,glove用了全局共现矩阵，更占内存资源,question2answer
word2vec和glove区别？,word2vec是“predictive”的模型，而GloVe是“countbased”的模型,question2answer
你觉得word2vec有哪些问题？,没考虑词序,question2answer
你觉得word2vec有哪些问题？,对于中文依赖分词结果的好坏,question2answer
你觉得word2vec有哪些问题？,新生词无法友好处理,question2answer
你觉得word2vec有哪些问题？,无正则化处理,question2answer
AutoML问题构成?,AutoML,question2feature
特征工程选择思路？,特征,question2feature
常见优化算法思路？,优化,question2feature
AutoML参数选择所使用的方法？,AutoML,question2feature
AutoML参数选择所使用的方法？,参数,question2feature
讲讲贝叶斯优化如何在automl上应用？,优化,question2feature
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,搜索,question2feature
写出全概率公式&贝叶斯公式,贝叶斯公式,question2feature
说说你怎么理解为什么有全概率公式&贝叶斯公式,贝叶斯公式,question2feature
模型训练为什么要引入偏差和方差？请理论论证。,训练,question2feature
遇到过的机器学习中的偏差与方差问题？,机器学习,question2feature
就理论角度论证Bagging、Boosting的方差偏差问题,Bagging,question2feature
就理论角度论证Bagging、Boosting的方差偏差问题,Boosting,question2feature
遇到过的深度学习中的偏差与方差问题？,深度,question2feature
方差、偏差与模型的复杂度之间的关系？,复杂度,question2feature
什么叫判别模型？,判别,question2feature
什么时候会选择生成/判别模型？,判别,question2feature
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,CRF,question2feature
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,EM,question2feature
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,熵,question2feature
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,混合,question2feature
极大似然估计 - MLE,极大似然,question2feature
极大似然估计 - MLE,估计,question2feature
极大似然估计 - MLE,MLE,question2feature
最大后验估计 - MAP,估计,question2feature
最大后验估计 - MAP,MAP,question2feature
极大似然估计与最大后验概率的区别？,极大似然,question2feature
极大似然估计与最大后验概率的区别？,估计,question2feature
到底什么是似然什么是概率估计？,估计,question2feature
DNN与DeepFM之间的区别?,DNN,question2feature
DNN与DeepFM之间的区别?,DeepFM,question2feature
Wide&Deep与DeepFM之间的区别?,DeepFM,question2feature
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,deepFM,question2feature
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,欠拟合,question2feature
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,过拟合,question2feature
DeepFM怎么优化的？,DeepFM,question2feature
DeepFM怎么优化的？,优化,question2feature
不定长文本数据如何输入deepFM？,长文本,question2feature
不定长文本数据如何输入deepFM？,deepFM,question2feature
deepfm的embedding初始化有什么值得注意的地方吗？,deepfm,question2feature
deepfm的embedding初始化有什么值得注意的地方吗？,embedding,question2feature
deepfm的embedding初始化有什么值得注意的地方吗？,初始化,question2feature
activation unit的作用,activation,question2feature
activation unit的作用,unit,question2feature
DICE怎么设计的,DICE,question2feature
DICE使用的过程中，有什么需要注意的地方,DICE,question2feature
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,特征,question2feature
简单介绍一下XDeepFm的思想？,XDeepFm,question2feature
和DCN比，有哪些核心的变化？,DCN,question2feature
时间复杂度多少？,复杂度,question2feature
如何进行负采样的？,负采样,question2feature
item向量在softmax的时候你们怎么选择的？,向量,question2feature
item向量在softmax的时候你们怎么选择的？,softmax,question2feature
为什么不采取类似RNN的Sequence model？,RNN,question2feature
YouTube如何避免百万量级的softmax问题的？,YouTube,question2feature
YouTube如何避免百万量级的softmax问题的？,softmax,question2feature
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,YouTube,question2feature
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,训练,question2feature
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,预测,question2feature
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,搜索,question2feature
Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,feature,question2feature
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,YouTube,question2feature
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,留一法,question2feature
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,random,question2feature
常见导数,导数,question2feature
复合函数的运算法则,函数,question2feature
切线方程,切线,question2feature
法线方程,法线,question2feature
拉格朗日中值定理,拉格朗日中值,question2feature
期望,期望,question2feature
均匀分布,均匀分布,question2feature
拉普拉斯分布,拉普拉斯,question2feature
泊松分布,泊松,question2feature
切比雪夫不等式,切比雪夫不等式,question2feature
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,均匀分布,question2feature
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,均值,question2feature
泰勒公式,泰勒公式,question2feature
常见泰勒公式,泰勒公式,question2feature
迭代公式推导,迭代,question2feature
范数,范数,question2feature
特征值分解，特征向量,特征值,question2feature
特征值分解，特征向量,分解,question2feature
统计方法,统计,question2feature
矩阵分解方法,矩阵分解,question2feature
特征值和特征向量的本质是什么？,特征值,question2feature
聚类的离群点检测,聚类,question2feature
为什么要对数据进行采样平衡,平衡,question2feature
是否一定需要对原始数据进行采样平衡,原始数据,question2feature
是否一定需要对原始数据进行采样平衡,平衡,question2feature
归一化和标准化之间的关系？,标准化,question2feature
连续特征常用方法,特征,question2feature
离散特征常用方法,离散,question2feature
离散特征常用方法,特征,question2feature
文本特征,特征,question2feature
画一个最简单的最快速能实现的框架,快速,question2feature
为什么要做特征选择？,特征选择,question2feature
从哪些方面可以做特征选择？,特征选择,question2feature
如何直接离散化？,离散,question2feature
random方法,random,question2feature
详述信息增益计算方法,增益,question2feature
详述信息增益率计算方法,增益,question2feature
ID3存在的问题,ID3,question2feature
C4.5相对于ID3的改进点,ID3,question2feature
CART的连续特征改进点,CART,question2feature
CART的连续特征改进点,特征,question2feature
CART分类树建立算法的具体流程,CART,question2feature
CART分类树建立算法的具体流程,分类,question2feature
CART分类树建立算法的具体流程,树,question2feature
CART回归树建立算法的具体流程,CART,question2feature
CART回归树建立算法的具体流程,回归,question2feature
CART回归树建立算法的具体流程,树,question2feature
CART输出结果的逻辑？,CART,question2feature
CART输出结果的逻辑？,输出,question2feature
CART输出结果的逻辑？,逻辑,question2feature
CART树算法的剪枝过程是怎么样的？,CART,question2feature
CART树算法的剪枝过程是怎么样的？,树,question2feature
CART树算法的剪枝过程是怎么样的？,剪枝,question2feature
简单介绍SVM?,SVM,question2feature
什么是支持向量？,向量,question2feature
SVM 和全部数据有关还是和局部数据有关?,SVM,question2feature
加大训练数据量一定能提高SVM准确率吗？,训练,question2feature
加大训练数据量一定能提高SVM准确率吗？,SVM,question2feature
如何解决多分类问题？,分类,question2feature
可以做回归吗，怎么做？,回归,question2feature
SVM 能解决哪些问题？,SVM,question2feature
介绍一下你知道的不同的SVM分类器？,SVM,question2feature
SVM 软间隔与硬间隔表达式,SVM,question2feature
SVM 软间隔与硬间隔表达式,表达式,question2feature
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,SVM,question2feature
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,对偶,question2feature
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,对偶,question2feature
为什么要把原问题转换为对偶问题？,对偶,question2feature
为什么求解对偶问题更加高效？,对偶,question2feature
KKT限制条件，KKT条件有哪些，完整描述,KKT,question2feature
KKT限制条件，KKT条件有哪些，完整描述,KKT,question2feature
引入拉格朗日的优化方法后的损失函数解释,拉格朗日,question2feature
引入拉格朗日的优化方法后的损失函数解释,优化方法,question2feature
引入拉格朗日的优化方法后的损失函数解释,损失,question2feature
引入拉格朗日的优化方法后的损失函数解释,函数,question2feature
核函数的作用是啥,核,question2feature
核函数的作用是啥,函数,question2feature
核函数的种类和应用场景,核,question2feature
核函数的种类和应用场景,函数,question2feature
如何选择核函数,核,question2feature
如何选择核函数,函数,question2feature
常用核函数的定义？,核,question2feature
常用核函数的定义？,函数,question2feature
核函数需要满足什么条件？,核,question2feature
核函数需要满足什么条件？,函数,question2feature
为什么在数据量大的情况下常常用lr代替核SVM？,lr,question2feature
为什么在数据量大的情况下常常用lr代替核SVM？,核,question2feature
为什么在数据量大的情况下常常用lr代替核SVM？,SVM,question2feature
高斯核可以升到多少维？为什么,核,question2feature
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,SVM,question2feature
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,逻辑,question2feature
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,回归,question2feature
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,训练,question2feature
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,决策,question2feature
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",机器学习,question2feature
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",svm,question2feature
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",逻辑,question2feature
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",回归,question2feature
Linear SVM 和 LR 有什么异同？,Linear,question2feature
Linear SVM 和 LR 有什么异同？,SVM,question2feature
Linear SVM 和 LR 有什么异同？,LR,question2feature
损失函数是啥,损失,question2feature
损失函数是啥,函数,question2feature
最小二乘/梯度下降手推,二乘,question2feature
最小二乘/梯度下降手推,梯度,question2feature
介绍一下岭回归,回归,question2feature
什么时候使用岭回归？,回归,question2feature
什么时候用Lasso回归？,回归,question2feature
请问从EM角度理解kmeans?,EM,question2feature
请问从EM角度理解kmeans?,kmeans,question2feature
为什么kmeans一定会收敛?,kmeans,question2feature
为什么kmeans一定会收敛?,收敛,question2feature
kmeans初始点除了随机选取之外的方法？,kmeans,question2feature
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,贝叶斯公式,question2feature
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,分类,question2feature
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,差别,question2feature
出现估计概率值为 0 怎么处理,估计,question2feature
朴素贝叶斯与 LR 区别？,LR,question2feature
logistic分布函数和密度函数，手绘大概的图像,logistic,question2feature
logistic分布函数和密度函数，手绘大概的图像,函数,question2feature
logistic分布函数和密度函数，手绘大概的图像,函数,question2feature
LR推导，基础5连问,LR,question2feature
梯度下降如何并行化？,梯度,question2feature
LR明明是分类模型为什么叫回归？,LR,question2feature
LR明明是分类模型为什么叫回归？,分类,question2feature
LR明明是分类模型为什么叫回归？,回归,question2feature
为什么LR可以用来做CTR预估？,LR,question2feature
为什么LR可以用来做CTR预估？,CTR,question2feature
满足什么样条件的数据用LR最好？,LR,question2feature
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,LR,question2feature
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,sigmoid,question2feature
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,函数,question2feature
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,激活函数,question2feature
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,函数,question2feature
利用几率odds的意义在哪？,odds,question2feature
Sigmoid函数到底起了什么作用？,Sigmoid,question2feature
Sigmoid函数到底起了什么作用？,函数,question2feature
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,LR,question2feature
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,极大似然,question2feature
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,函数,question2feature
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,熵,question2feature
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,损失,question2feature
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,函数,question2feature
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,损失,question2feature
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,函数,question2feature
LR中若标签为+1和-1，损失函数如何推导？,LR,question2feature
LR中若标签为+1和-1，损失函数如何推导？,损失,question2feature
LR中若标签为+1和-1，损失函数如何推导？,函数,question2feature
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,特征,question2feature
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,特征,question2feature
LR可以用核么？可以怎么用？,LR,question2feature
LR可以用核么？可以怎么用？,核,question2feature
LR中的L1/L2正则项是啥？,LR,question2feature
LR中的L1/L2正则项是啥？,L1,question2feature
LR中的L1/L2正则项是啥？,L2,question2feature
lr加l1还是l2好？,lr,question2feature
lr加l1还是l2好？,l1,question2feature
lr加l1还是l2好？,l2,question2feature
正则化是依据什么理论实现模型优化？,优化,question2feature
LR可以用来处理非线性问题么？,LR,question2feature
LR可以用来处理非线性问题么？,非线性,question2feature
为什么LR需要归一化或者取对数?,LR,question2feature
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,LR,question2feature
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,特征,question2feature
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,离散,question2feature
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,离散,question2feature
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,逻辑,question2feature
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,参数,question2feature
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,目标,question2feature
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,函数,question2feature
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,逻辑,question2feature
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,回归,question2feature
LR对比万物？,LR,question2feature
LR梯度下降方法？,LR,question2feature
LR梯度下降方法？,梯度,question2feature
LR的优缺点？,LR,question2feature
除了做分类，你还会用LR做什么？,分类,question2feature
除了做分类，你还会用LR做什么？,LR,question2feature
你有用过sklearn中的lr么？你用的是哪个包？,sklearn,question2feature
你有用过sklearn中的lr么？你用的是哪个包？,lr,question2feature
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,sklearn,question2feature
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,LogisticRegression,question2feature
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,sklearn,question2feature
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,LogisticRegression,question2feature
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,分类,question2feature
解释下随机森林?,随机森林,question2feature
随机森林用的是什么树？,随机森林,question2feature
随机森林用的是什么树？,树,question2feature
随机森林的生成过程？,随机森林,question2feature
解释下随机森林节点的分裂策略？,随机森林,question2feature
随机森林的损失函数是什么？,随机森林,question2feature
随机森林的损失函数是什么？,损失,question2feature
随机森林的损失函数是什么？,函数,question2feature
为了防止随机森林过拟合可以怎么做?,随机森林,question2feature
为了防止随机森林过拟合可以怎么做?,过拟合,question2feature
随机森林特征选择的过程？,随机森林,question2feature
随机森林特征选择的过程？,特征选择,question2feature
是否用过随机森林，有什么技巧?,随机森林,question2feature
RF的参数有哪些，如何调参？,RF,question2feature
RF的参数有哪些，如何调参？,参数,question2feature
RF的优缺点 ？,RF,question2feature
介绍一下Boosting的思想？,Boosting,question2feature
最小二乘回归树的切分过程是怎么样的？,二乘,question2feature
最小二乘回归树的切分过程是怎么样的？,回归,question2feature
最小二乘回归树的切分过程是怎么样的？,树,question2feature
有哪些直接利用了Boosting思想的树模型？,Boosting,question2feature
有哪些直接利用了Boosting思想的树模型？,树,question2feature
gbdt和boostingtree的boosting分别体现在哪里？,gbdt,question2feature
gbdt和boostingtree的boosting分别体现在哪里？,boostingtree,question2feature
gbdt和boostingtree的boosting分别体现在哪里？,boosting,question2feature
gbdt的中的tree是什么tree？有什么特征？,gbdt,question2feature
gbdt的中的tree是什么tree？有什么特征？,特征,question2feature
常用回归问题的损失函数？,回归,question2feature
常用回归问题的损失函数？,损失,question2feature
常用回归问题的损失函数？,函数,question2feature
常用分类问题的损失函数？,分类,question2feature
常用分类问题的损失函数？,损失,question2feature
常用分类问题的损失函数？,函数,question2feature
什么是gbdt中的损失函数的负梯度？,gbdt,question2feature
什么是gbdt中的损失函数的负梯度？,损失,question2feature
什么是gbdt中的损失函数的负梯度？,函数,question2feature
什么是gbdt中的损失函数的负梯度？,梯度,question2feature
如何用损失函数的负梯度实现gbdt？,损失,question2feature
如何用损失函数的负梯度实现gbdt？,函数,question2feature
如何用损失函数的负梯度实现gbdt？,梯度,question2feature
如何用损失函数的负梯度实现gbdt？,gbdt,question2feature
拟合损失函数的负梯度为什么是可行的？,拟合,question2feature
拟合损失函数的负梯度为什么是可行的？,损失,question2feature
拟合损失函数的负梯度为什么是可行的？,函数,question2feature
拟合损失函数的负梯度为什么是可行的？,梯度,question2feature
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,拟合,question2feature
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,损失,question2feature
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,函数,question2feature
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,梯度,question2feature
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,拟合,question2feature
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,拟合,question2feature
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,梯度,question2feature
feature属性会被重复多次使用么？,feature,question2feature
gbdt如何进行正则化的？,gbdt,question2feature
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,树,question2feature
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,集成学习,question2feature
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,树,question2feature
gbdt的优缺点？,gbdt,question2feature
gbdt和randomforest区别？,gbdt,question2feature
gbdt和randomforest区别？,randomforest,question2feature
GBDT和LR的差异？,GBDT,question2feature
GBDT和LR的差异？,LR,question2feature
XGboost缺点,XGboost,question2feature
LightGBM对Xgboost的优化,LightGBM,question2feature
LightGBM对Xgboost的优化,Xgboost,question2feature
LightGBM对Xgboost的优化,优化,question2feature
LightGBM亮点,LightGBM,question2feature
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,xgboost,question2feature
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,gbdt,question2feature
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,boosting,question2feature
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,优化,question2feature
xgboost和gbdt的区别？,xgboost,question2feature
xgboost和gbdt的区别？,gbdt,question2feature
xgboost优化目标/损失函数改变成什么样？,xgboost,question2feature
xgboost优化目标/损失函数改变成什么样？,优化,question2feature
xgboost优化目标/损失函数改变成什么样？,目标,question2feature
xgboost优化目标/损失函数改变成什么样？,损失,question2feature
xgboost优化目标/损失函数改变成什么样？,函数,question2feature
xgboost如何使用MAE或MAPE作为目标函数？,xgboost,question2feature
xgboost如何使用MAE或MAPE作为目标函数？,MAE,question2feature
xgboost如何使用MAE或MAPE作为目标函数？,MAPE,question2feature
xgboost如何使用MAE或MAPE作为目标函数？,目标,question2feature
xgboost如何使用MAE或MAPE作为目标函数？,函数,question2feature
xgboost如何寻找分裂节点的候选集？,xgboost,question2feature
xgboost如何处理缺失值？,xgboost,question2feature
xgboost在计算速度上有了哪些点上提升？,xgboost,question2feature
xgboost特征重要性是如何得到的？,xgboost,question2feature
xgboost特征重要性是如何得到的？,特征,question2feature
xgboost特征重要性是如何得到的？,重要性,question2feature
XGBoost中如何对树进行剪枝？,XGBoost,question2feature
XGBoost中如何对树进行剪枝？,剪枝,question2feature
XGBoost模型如果过拟合了怎么解决？,XGBoost,question2feature
XGBoost模型如果过拟合了怎么解决？,过拟合,question2feature
xgboost如何调参数？,xgboost,question2feature
xgboost如何调参数？,参数,question2feature
Attention对比RNN和CNN，分别有哪点你觉得的优势？,Attention,question2feature
Attention对比RNN和CNN，分别有哪点你觉得的优势？,RNN,question2feature
Attention对比RNN和CNN，分别有哪点你觉得的优势？,CNN,question2feature
写出Attention的公式？,Attention,question2feature
解释你怎么理解Attention的公式的？,Attention,question2feature
Attention模型怎么避免词袋模型的顺序问题的困境的？,Attention,question2feature
Attention模型怎么避免词袋模型的顺序问题的困境的？,词袋模型,question2feature
"Attention机制，里面的q,k,v分别代表什么？",Attention,question2feature
为什么self-attention可以替代seq2seq？,seq2seq,question2feature
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,维度,question2feature
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,点积,question2feature
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,维度,question2feature
你觉得bn过程是什么样的？,bn,question2feature
手写一下bn过程？,bn,question2feature
知道LN么？讲讲原理,LN,question2feature
残差网络为什么能解决梯度消失的问题,梯度,question2feature
Bert的双向体现在什么地方？,Bert,question2feature
Bert的是怎样实现mask构造的？,Bert,question2feature
Bert的是怎样实现mask构造的？,mask,question2feature
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,mask,question2feature
为什么BERT有3个嵌入层，它们都是如何实现的？,BERT,question2feature
bert的损失函数？,bert,question2feature
bert的损失函数？,损失,question2feature
bert的损失函数？,函数,question2feature
手写一个multi-head attention？,attention,question2feature
长文本预测如何构造Tokens？,长文本,question2feature
长文本预测如何构造Tokens？,预测,question2feature
你用过什么模块？bert流程是怎么样的？,bert,question2feature
知道分词模块：FullTokenizer做了哪些事情么？,分词,question2feature
知道分词模块：FullTokenizer做了哪些事情么？,FullTokenizer,question2feature
Bert中如何获得词意和句意？,Bert,question2feature
源码中Attention后实际的流程是如何的？,Attention,question2feature
为什么要在Attention后使用残差结构？,Attention,question2feature
平时用官方Bert包么？耗时怎么样？,Bert,question2feature
你觉得BERT比普通LM的新颖点？,BERT,question2feature
你觉得BERT比普通LM的新颖点？,LM,question2feature
elmo、GPT、bert三者之间有什么区别？,elmo,question2feature
elmo、GPT、bert三者之间有什么区别？,GPT,question2feature
elmo、GPT、bert三者之间有什么区别？,bert,question2feature
阐述CRF原理？,CRF,question2feature
CRF与HMM区别?,CRF,question2feature
CRF与HMM区别?,HMM,question2feature
Bert+crf中的各部分作用详解？,Bert,question2feature
Bert+crf中的各部分作用详解？,crf,question2feature
GolVe的损失函数？,GolVe,question2feature
GolVe的损失函数？,损失,question2feature
GolVe的损失函数？,函数,question2feature
解释GolVe的损失函数？,GolVe,question2feature
解释GolVe的损失函数？,损失,question2feature
解释GolVe的损失函数？,函数,question2feature
为什么GolVe会用的相对比W2V少？,GolVe,question2feature
为什么GolVe会用的相对比W2V少？,W2V,question2feature
详述LDA原理？,LDA,question2feature
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,LDA,question2feature
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,主题,question2feature
LDA的共轭分布解释下?,LDA,question2feature
PLSA和LDA的区别?,LDA,question2feature
怎么确定LDA的topic个数,LDA,question2feature
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA,question2feature
LDA和Word2Vec区别？LDA和Doc2Vec区别？,Word2Vec,question2feature
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA,question2feature
LDA和Word2Vec区别？LDA和Doc2Vec区别？,Doc2Vec,question2feature
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,LDA,question2feature
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,Dirichlet,question2feature
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,参数,question2feature
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,beta,question2feature
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,trick,question2feature
从隐藏层到输出的Softmax层的计算有哪些方法？,隐藏层,question2feature
从隐藏层到输出的Softmax层的计算有哪些方法？,输出,question2feature
从隐藏层到输出的Softmax层的计算有哪些方法？,Softmax,question2feature
层次softmax流程？,softmax,question2feature
负采样流程？,负采样,question2feature
word2vec两种方法各自的优势?,word2vec,question2feature
怎么衡量学到的embedding的好坏?,embedding,question2feature
word2vec和glove区别？,word2vec,question2feature
word2vec和glove区别？,glove,question2feature
你觉得word2vec有哪些问题？,word2vec,question2feature
特征选择,特征选择,answer2feature
有监督的特征选择,特征选择,answer2feature
基于模型，lr的系数，树模型的importance等等,lr,answer2feature
基于模型，lr的系数，树模型的importance等等,树,answer2feature
无监督的特征选择,特征选择,answer2feature
基于统计信息的，熵、相关性、KL系数,统计,answer2feature
基于统计信息的，熵、相关性、KL系数,熵,answer2feature
基于统计信息的，熵、相关性、KL系数,相关性,answer2feature
基于统计信息的，熵、相关性、KL系数,KL,answer2feature
基于方差，因子分解，PCA主成分分享，方差系数,分解,answer2feature
各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,非线性,answer2feature
各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,特征,answer2feature
各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,特征,answer2feature
参数选择,参数,answer2feature
SGD,SGD,answer2feature
GD,GD,answer2feature
FTRL,FTRL,answer2feature
暴力搜索,搜索,answer2feature
拟合搜索,拟合,answer2feature
拟合搜索,搜索,answer2feature
贝叶斯优化,优化,answer2feature
Meta学习,Meta,answer2feature
目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,拟合,answer2feature
目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,参数,answer2feature
目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,参数,answer2feature
目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,参数,answer2feature
随机选取几个超参数进行f拟合，得到先验数据集合D,参数,answer2feature
随机选取几个超参数进行f拟合，得到先验数据集合D,拟合,answer2feature
根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,参数,answer2feature
根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,参数,answer2feature
根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,原始数据,answer2feature
稳定性：同一组超参数的预测结果在不同轮次不一致,稳定性,answer2feature
稳定性：同一组超参数的预测结果在不同轮次不一致,参数,answer2feature
稳定性：同一组超参数的预测结果在不同轮次不一致,预测,answer2feature
f函数需要多次计算，资源耗费时间损失,函数,answer2feature
f函数需要多次计算，资源耗费时间损失,损失,answer2feature
难以确定比较通用的拟合模型f,拟合,answer2feature
基于均值和方差的平衡结果,均值,answer2feature
基于均值和方差的平衡结果,平衡,answer2feature
EI(期望提升),期望,answer2feature
贝叶斯公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wefkh3r5j30l203iq3c.jpg),贝叶斯公式,answer2feature
贝叶斯公式为当给定条件发生变化后，会导致事件发生的可能性发生何种变化。**key：概率变化**,贝叶斯公式,answer2feature
先验概率（priorprobability）：指根据以往经验和分析。在实验或采样前就可以得到的概率。key:简单的暴力统计,统计,answer2feature
有一个木桶，里面有M个白球，小明每分钟从桶中随机取出一个球涂成红色（无论白或红都涂红）再放回，问小明将桶中球全部涂红的期望时间是多少？,期望,answer2feature
P[i]代表M个球中已经有i个球是红色后，还需要的时间期望，去将所有球都变成红色。,期望,answer2feature
解释一下，每一次抽取，(i/M)概率不变，(1i/M)进入下一轮，额外加一次本次操作,抽取,answer2feature
期望值与真实值之间的波动程度，衡量的是**稳定性**,稳定性,answer2feature
期望值与真实值之间的一致差距，衡量的是**准确性**,准确性,answer2feature
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,优化,answer2feature
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,监督学习,answer2feature
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,优化,answer2feature
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,误差,answer2feature
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,误差,answer2feature
"以回归任务为例,其实更准确的公式为：**Err=bias^2+var+irreducibleerror^2**",回归,answer2feature
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,训练,answer2feature
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,训练,answer2feature
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,训练,answer2feature
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,期望,answer2feature
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,训练,answer2feature
方差：模型的稳定性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx3a2qc3j306400ot8j.jpg),稳定性,answer2feature
偏差：模型的准确性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx6rqg34j305g00odfn.jpg),准确性,answer2feature
"Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))为可解释规则误差",误差,answer2feature
f为真实值，固定；![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型不同数据预测结果的期望，固定；所以f![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)固定,预测,answer2feature
f为真实值，固定；![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型不同数据预测结果的期望，固定；所以f![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)固定,期望,answer2feature
过高复杂度的模型，对训练集进行过拟合,复杂度,answer2feature
过高复杂度的模型，对训练集进行过拟合,训练,answer2feature
过高复杂度的模型，对训练集进行过拟合,过拟合,answer2feature
带来的后果就是在训练集合上效果非常好，但是在校验集合上效果极差,训练,answer2feature
更加形象的理解就是用一条高次方程去拟合线性数据,高次方程,answer2feature
更加形象的理解就是用一条高次方程去拟合线性数据,拟合,answer2feature
在数据量不变的情况下，减少特征维度,特征,answer2feature
在数据量不变的情况下，减少特征维度,维度,answer2feature
减少的特征维度如果是共线性的维度，对原模型没有任何影响,特征,answer2feature
减少的特征维度如果是共线性的维度，对原模型没有任何影响,维度,answer2feature
减少的特征维度如果是共线性的维度，对原模型没有任何影响,维度,answer2feature
罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？,回归,answer2feature
罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？,特征,answer2feature
尝试获得更多的特征,特征,answer2feature
从数据入手，进行特征交叉，或者特征的embedding化,特征,answer2feature
从数据入手，进行特征交叉，或者特征的embedding化,特征,answer2feature
从数据入手，进行特征交叉，或者特征的embedding化,embedding,answer2feature
尝试增加多项式特征,多项式,answer2feature
尝试增加多项式特征,特征,answer2feature
从模型入手，增加更多线性及非线性变化，提高模型的复杂度,非线性,answer2feature
从模型入手，增加更多线性及非线性变化，提高模型的复杂度,复杂度,answer2feature
特征越稀疏，高方差的风险越高,特征,answer2feature
特征越稀疏，高方差的风险越高,稀疏,answer2feature
多个线性变换=一个线性变换，多个非线性变换不一定=一个多线性变换,非线性,answer2feature
从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,分解,answer2feature
从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,Bagging,answer2feature
从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,剪枝,answer2feature
从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,分解,answer2feature
从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,Boosting,answer2feature
从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,Boosting,answer2feature
bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等,bagging,answer2feature
bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等,boosting,answer2feature
"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",Var,answer2feature
"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",Var,answer2feature
"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",Var,answer2feature
Bagging,Bagging,answer2feature
Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyk4wjxkj302v00qa9u.jpg),Var,answer2feature
所以，化简以上的式子可得：Var(F)=m*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyw59y9oj300h00g0rq.jpg)*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lywg4870j300g00k0r2.jpg)+![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyzygc4lj304300lgle.jpg),Var,answer2feature
以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可,bagging,answer2feature
以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可,期望,answer2feature
Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz7qrpwsj304f015t8i.jpg),Var,answer2feature
整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似,期望,answer2feature
整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似,期望,answer2feature
整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,相关性,answer2feature
整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,过拟合,answer2feature
bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,bagging,answer2feature
bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,过拟合,answer2feature
bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,准确性,answer2feature
Boosting同理,Boosting,answer2feature
boosting的前提是弱模型之间高度相关，我们不妨设相关度为1,boosting,answer2feature
Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzwvy93dj302k00kmwx.jpg),Var,answer2feature
整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,期望,answer2feature
整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,期望,answer2feature
整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,期望,answer2feature
整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,拟合,answer2feature
整体模型的方差等于基模型的数量平方成正比，越多模型不稳定性越高，越容易过拟合。,过拟合,answer2feature
神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,拟合,answer2feature
神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,训练,answer2feature
神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,误差,answer2feature
但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,拟合,answer2feature
但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,误差,answer2feature
但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,误差,answer2feature
因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。,深度,answer2feature
因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。,误差,answer2feature
dropout,dropout,answer2feature
dense中的normalization,dense,answer2feature
dense中的normalization,normalization,answer2feature
"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",自变量,answer2feature
"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",因变量,answer2feature
"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",贝叶斯公式,answer2feature
求一个通过自变量能够表示出因变量的公式y=F(x)或者p(y/x)，核心是对于x找到一个最合适的公式得到y,自变量,answer2feature
求一个通过自变量能够表示出因变量的公式y=F(x)或者p(y/x)，核心是对于x找到一个最合适的公式得到y,因变量,answer2feature
明确一点：绝大多数情况下，判别模型都要比生成模型效果好，而且需要的数据量和前提假设都要小于生成模型，上面的概念中可得原因。,判别,answer2feature
但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,判别,answer2feature
但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,拟合,answer2feature
但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,拟合,answer2feature
如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,分类,answer2feature
如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,判别,answer2feature
如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,不确定性,answer2feature
FM/FFM,FM,answer2feature
FM/FFM,FFM,answer2feature
线性Dense,Dense,answer2feature
非线性激活,非线性,answer2feature
因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,knn,answer2feature
因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,svm,answer2feature
因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,判别,answer2feature
生成式模型：朴素贝叶斯，混合高斯模型，马尔科夫随机场，EM,混合,answer2feature
生成式模型：朴素贝叶斯，混合高斯模型，马尔科夫随机场，EM,EM,answer2feature
仔细看过这些模型细节的朋友都应该知道，他们最后都是判断x属于拟合一个正负样本分布，然后对比属于正负样本的概率,拟合,answer2feature
判别式模型：最大熵模型，CRF,判别式,answer2feature
判别式模型：最大熵模型，CRF,熵,answer2feature
判别式模型：最大熵模型，CRF,CRF,answer2feature
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,判别,answer2feature
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,分类,answer2feature
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,函数,answer2feature
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,分类,answer2feature
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,函数,answer2feature
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,分类,answer2feature
算出属于正负样本的概率在相互对比的就是生成模型，直接得到结果概率的就是判别模型,判别,answer2feature
生成模型得分布，判别模型得最优划分,判别,answer2feature
生成模型可以得到判别模型，反之不成立,判别,answer2feature
生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,判别,answer2feature
生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,判别,answer2feature
生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,分类,answer2feature
似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,函数,answer2feature
似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,函数,answer2feature
似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,参数,answer2feature
"MAP的基础使贝叶斯公式：P(θ/X)=P(θ,X)/P(X)。目的是通过观测值使得后验概率P(θ,X)最大即可",MAP,answer2feature
"MAP的基础使贝叶斯公式：P(θ/X)=P(θ,X)/P(X)。目的是通过观测值使得后验概率P(θ,X)最大即可",贝叶斯公式,answer2feature
最大似然估计中的采样满足所有采样都是独立同分布的假设,估计,answer2feature
概率估计：给定了θ，X=x的可能性,估计,answer2feature
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,DNN,answer2feature
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,DeepFM,answer2feature
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,DeepFM,answer2feature
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,特征,answer2feature
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,FM,answer2feature
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,特征,answer2feature
DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,DeepFM,answer2feature
DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,Wide,answer2feature
DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,优化,answer2feature
DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,特征,answer2feature
欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,欠拟合,answer2feature
欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,epoch,answer2feature
欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,learningrate,answer2feature
过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,过拟合,answer2feature
过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,dropout,answer2feature
过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,epoch,answer2feature
embedding向量可以通过FM初始化,embedding,answer2feature
embedding向量可以通过FM初始化,向量,answer2feature
embedding向量可以通过FM初始化,FM,answer2feature
embedding向量可以通过FM初始化,初始化,answer2feature
Deep层可以做优化,Deep,answer2feature
Deep层可以做优化,优化,answer2feature
NFM:把deep层的做NFM类型的处理，其实就是deep层在输入之前也做一个二阶特征的交叉处理和fm层一致,特征,answer2feature
FM层可以变得交叉更多阶,FM,answer2feature
XDeepFM,XDeepFM,answer2feature
截断补齐,截断,answer2feature
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",Xavier,answer2feature
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",输出,answer2feature
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",输出,answer2feature
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",正态分布,answer2feature
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",random,answer2feature
"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",relu,answer2feature
"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",神经元,answer2feature
"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",random,answer2feature
文本项目上也可以用预训练好的特征,特征,answer2feature
Attention机制，针对不同的广告，用户历史行为与该广告的权重是不同的。,Attention,answer2feature
基于Attention机制，结合当前需对比的不同目标，将用户历史行为进行不同的权重分配。,Attention,answer2feature
基于Attention机制，结合当前需对比的不同目标，将用户历史行为进行不同的权重分配。,目标,answer2feature
activationunit在这种思路上，认为面对不同的对象Va兴趣的权重Wi应该也是变换而不是固定的，所以用了g(ViVa)来动态刻画不同目标下的历史行为的不同重要性,目标,answer2feature
activationunit在这种思路上，认为面对不同的对象Va兴趣的权重Wi应该也是变换而不是固定的，所以用了g(ViVa)来动态刻画不同目标下的历史行为的不同重要性,重要性,answer2feature
先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,bn,answer2feature
先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,sigmoid,answer2feature
先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,加权,answer2feature
先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,平衡,answer2feature
"x_p=tf.sigmoid(tf.layers.batch_normalization(x,center=False,scale=False,training=True))",sigmoid,answer2feature
在用batch_normalization的时候，需要设置traning=True，否则在做test的时候，获取不到training过程中的各batch的期望,batch,answer2feature
在用batch_normalization的时候，需要设置traning=True，否则在做test的时候，获取不到training过程中的各batch的期望,期望,answer2feature
test的时候，方差计算利用的是期望的无偏估计计算方法:E(u^2)`*`m/(m1),期望,answer2feature
test的时候，方差计算利用的是期望的无偏估计计算方法:E(u^2)`*`m/(m1),估计,answer2feature
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,deepfm,answer2feature
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,FNN,answer2feature
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,特征,answer2feature
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,dnn,answer2feature
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,特征,answer2feature
向量级别的特征交互而不是元素级交互,向量,answer2feature
向量级别的特征交互而不是元素级交互,特征,answer2feature
经验上，vectorwise的方式构建的特征交叉关系比bitwise的方式更容易学习,特征,answer2feature
"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",deepfm,answer2feature
"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",拟合,answer2feature
"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",推荐,answer2feature
"思路一：改变Memorization为attention网络，强化feature直接的关系，对B进行""电脑""与""蔡徐坤""之间的绑定而不是""篮球""和""蔡徐坤""之间的绑定",attention,answer2feature
"思路一：改变Memorization为attention网络，强化feature直接的关系，对B进行""电脑""与""蔡徐坤""之间的绑定而不是""篮球""和""蔡徐坤""之间的绑定",feature,answer2feature
思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,特征,answer2feature
思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,DCN,answer2feature
思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,XDeepFM,answer2feature
显示是可以写出feature交互的公式，隐式相反,feature,answer2feature
元素级是以feature值交互，向量级是feature向量级点乘处理,feature,answer2feature
元素级是以feature值交互，向量级是feature向量级点乘处理,向量,answer2feature
元素级是以feature值交互，向量级是feature向量级点乘处理,feature,answer2feature
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,特征,answer2feature
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,DNN,answer2feature
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,特征,answer2feature
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,特征,answer2feature
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,FM,answer2feature
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,特征,answer2feature
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,DeepFm,answer2feature
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,特征,answer2feature
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,特征,answer2feature
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,特征,answer2feature
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,高频,answer2feature
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,特征,answer2feature
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,稀疏,answer2feature
高阶特征交互：DNN,特征,answer2feature
高阶特征交互：DNN,DNN,answer2feature
低价特征交互：DCN结构(![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)),特征,answer2feature
低价特征交互：DCN结构(![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)),DCN,answer2feature
这样的网络结构保证来来自X0的1，2，3...N阶的特征组合,特征,answer2feature
借鉴来DCN的交叉网络的特殊结构**自动构造有限高阶交叉特征**：![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg),DCN,answer2feature
借鉴来DCN的交叉网络的特殊结构**自动构造有限高阶交叉特征**：![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg),特征,answer2feature
"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",batch,answer2feature
"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",DNN,answer2feature
"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",Linear,answer2feature
"先记录\[batch,field,embedding_size]作为X0，并切分为embedding`*`\[batch,field,1]份",batch,answer2feature
"先记录\[batch,field,embedding_size]作为X0，并切分为embedding`*`\[batch,field,1]份",batch,answer2feature
设置三层隐层，单层结点数为200，单层操作如下（以X1为例）：,隐层,answer2feature
"获取上一次的layerout：X0，并进行切分：embedding`*`\[batch,field,1]",batch,answer2feature
"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",batch,answer2feature
"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",batch,answer2feature
"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",embedding,answer2feature
"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",batch,answer2feature
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",embedding,answer2feature
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",batch,answer2feature
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",压缩,answer2feature
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",embedding,answer2feature
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",batch,answer2feature
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",embedding,answer2feature
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",batch,answer2feature
加偏置项，并进行激活函数处理，完成一轮处理,激活函数,answer2feature
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",embedding,answer2feature
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",维度,answer2feature
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",pooling,answer2feature
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",batch,answer2feature
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",embedding,answer2feature
实际过程中，可以对每层对结果进行采样泛化；可以通过最后层输出的残差连接保证梯度消失等等,输出,answer2feature
实际过程中，可以对每层对结果进行采样泛化；可以通过最后层输出的残差连接保证梯度消失等等,梯度,answer2feature
DCN是bitwise的，而CIN是vectorwise的,DCN,answer2feature
DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,DCN,answer2feature
DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,特征,answer2feature
DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,特征,answer2feature
假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,DNN,answer2feature
假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,神经元,answer2feature
假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,向量,answer2feature
假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,深度,answer2feature
DNN:O(m`*`D`*`H+L`*`H`*`H),DNN,answer2feature
input数据中只拿了近20次的点击，部分用户是没有20次的历史行为的，所以我们记录了每一个用户实际点击的次数，在做embedding的时候，我们除以的是真实的historylength,embedding,answer2feature
历史上最高频的商品id/文章id,高频,answer2feature
train的时候是进行负采样的,负采样,answer2feature
负采样我们会进行剔除，把该次click下的同时show的样本进行剔除后采样,负采样,answer2feature
均衡采样，不会根据其他样本showtime进行加权,加权,answer2feature
为了尽可能多的修正全量样本，尽快达到收敛,全量,answer2feature
为了尽可能多的修正全量样本，尽快达到收敛,收敛,answer2feature
为了避免其他推荐产生的交叉影响,推荐,answer2feature
是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,初始化,answer2feature
是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,初始化,answer2feature
是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,向量,answer2feature
希望平衡样本构造时间对当前的影响,平衡,answer2feature
item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,nlp,answer2feature
item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,预测,answer2feature
item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,预测,answer2feature
在实际的推荐数据获取中，历史点击流收到若干种在线的推荐算法影响，并不全是像自然语言问题一样真实具有序列性,推荐,answer2feature
在实际的推荐数据获取中，历史点击流收到若干种在线的推荐算法影响，并不全是像自然语言问题一样真实具有序列性,推荐,answer2feature
负采样,负采样,answer2feature
工程妥协，避免几百万的item每次都要计算一边，而采取的ANN的邻近搜索加快速度,搜索,answer2feature
embedding,embedding,answer2feature
引入了doc2vec做init,doc2vec,answer2feature
权重共享，没有在softmax处重新构造,softmax,answer2feature
负采样,负采样,answer2feature
加快收敛,加快,answer2feature
加快收敛,收敛,answer2feature
click数据的预处理,预处理,answer2feature
设函数f(x)满足条件：,函数,answer2feature
"设函数f(x),g(x)满足条件：",函数,answer2feature
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,离散,answer2feature
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,离散,answer2feature
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,期望,answer2feature
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,收敛,answer2feature
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,特征,answer2feature
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,反映,answer2feature
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,期望,answer2feature
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,均值,answer2feature
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,函数,answer2feature
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,函数,answer2feature
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,连续性,answer2feature
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,函数,answer2feature
方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,期望,answer2feature
方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,均值,answer2feature
方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,均值,answer2feature
方差刻画了随机变量的取值对于其数学期望的离散程度。,期望,answer2feature
方差刻画了随机变量的取值对于其数学期望的离散程度。,离散,answer2feature
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,均值,answer2feature
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,均值,answer2feature
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,均值,answer2feature
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,差别,answer2feature
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,逼近,answer2feature
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,统计,answer2feature
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,估计,answer2feature
标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,平方和,answer2feature
标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,平方根,answer2feature
标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,反映,answer2feature
标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,离散,answer2feature
协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,回归,answer2feature
协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,统计分析,answer2feature
协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,回归,answer2feature
在概率论和统计学中，协方差用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。,误差,answer2feature
"\[1,1]之间，值越接近1，说明两个变量正相关性（线性）越强。越接近1，说明负相关性越强，当为0时，表示两个变量没有相关性",相关性,answer2feature
"\[1,1]之间，值越接近1，说明两个变量正相关性（线性）越强。越接近1，说明负相关性越强，当为0时，表示两个变量没有相关性",相关性,answer2feature
"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",离散,answer2feature
"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",均匀分布,answer2feature
"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",均匀分布,answer2feature
"伯努利分布：参数为p∈[0,1]，设随机变量X∈{0,1}，则概率分布函数为：",参数,answer2feature
"伯努利分布：参数为p∈[0,1]，设随机变量X∈{0,1}，则概率分布函数为：",函数,answer2feature
期望为p，方差为p(1p),期望,answer2feature
期望为np，方差为np(1p),期望,answer2feature
我们在做模型训练的之后，随机变量取值范围是实数，大多数情况下都假设变量服从高斯分布，原因：,训练,answer2feature
随机变量大多数情况下有若干个因素组合而成，中心极限定理表明，多个独立随机变量的和近似正态分布,正态分布,answer2feature
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,正态分布,answer2feature
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,熵,answer2feature
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,不确定性,answer2feature
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,熵,answer2feature
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,信息量,answer2feature
典型的一维正态分布的概率密度函数为:,正态分布,answer2feature
期望为u，方差为2γ^2,期望,answer2feature
拉普拉斯分布比高斯分布更加尖锐和狭窄，在正则化中通常会利用该性质,拉普拉斯,answer2feature
假设已知事件在单位时间（或者单位面积）内发生的平均次数为λ，则泊松分布描述了：事件在单位时间（或者单位面积）内发生的具体次数为k的概率。,泊松,answer2feature
期望：λ，方差为：λ,期望,answer2feature
"p(|xu|>k∂)<=1/(k^2),满足k>0,u为期望,∂为标准差",期望,answer2feature
绝大多数数据都应该在均值附近,均值,answer2feature
有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),有放回,answer2feature
有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),抽取,answer2feature
有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),抽取,answer2feature
无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),无放回,answer2feature
无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),抽取,answer2feature
无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),抽取,answer2feature
均匀分布：,均匀分布,answer2feature
问题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数,放回,answer2feature
问题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数,抽取,answer2feature
假设设抽到蓝球的概率为p，设抽到红球的概率为q，那么抽取到的次数为：1·p+2p·q+...+np·q^(n1),抽取,answer2feature
定义：f(x)在x0处的邻域内有n+1阶的导数，在x0的邻域内的任x，x和x0之间至少存在一个ζ，使得,导数,answer2feature
1范数：各列绝对值和的最大值,范数,answer2feature
1范数：各列绝对值和的最大值,最大值,answer2feature
"2范数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zwforqmj302i00n0si.jpg),矩阵$A^TA$的最大特征值开平方根",范数,answer2feature
"2范数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zwforqmj302i00n0si.jpg),矩阵$A^TA$的最大特征值开平方根",特征值,answer2feature
特征值分解可以得到特征值与特征向量,特征值,answer2feature
特征值分解可以得到特征值与特征向量,分解,answer2feature
特征值分解可以得到特征值与特征向量,特征值,answer2feature
特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,特征值,answer2feature
特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,特征,answer2feature
特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,特征,answer2feature
"矩阵A的特征值与其特征向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zxnzwesj300h00h0rz.jpg),特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zy0ybcpj300a00c0pd.jpg)满足：",特征值,answer2feature
"矩阵A的特征值与其特征向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zxnzwesj300h00h0rz.jpg),特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zy0ybcpj300a00c0pd.jpg)满足：",特征值,answer2feature
其中，Q为特征向量组成的矩阵，∑为特征值由大到小组成的矩阵,特征值,answer2feature
矩阵的特征值大于等于0，半正定,特征值,answer2feature
矩阵的特征值大于0，正定,特征值,answer2feature
Hessian矩阵正定性在梯度下降的应用,梯度,answer2feature
"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",函数,answer2feature
"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",偏导恒,answer2feature
"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",函数,answer2feature
在svm中核函数构造的基本假设,svm,answer2feature
在svm中核函数构造的基本假设,核,answer2feature
在svm中核函数构造的基本假设,函数,answer2feature
数据需要服从正态分布,正态分布,answer2feature
基于正态分布的离群点检测方法,正态分布,answer2feature
多元高斯分布检测：,多元,answer2feature
假设n维的数据集合![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算n维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg),均值,answer2feature
假设n维的数据集合![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算n维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg),向量,answer2feature
假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。,均值,answer2feature
假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。,向量,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9rdqiqjj300g00h0sl.jpg)统计检验,统计,answer2feature
"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第i维的均值，n是维度",均值,answer2feature
"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第i维的均值，n是维度",维度,answer2feature
去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,均值,answer2feature
去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,特征值,answer2feature
去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,特征值,answer2feature
去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,排序,answer2feature
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,特征,answer2feature
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,原始数据,answer2feature
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,维度,answer2feature
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,空间,answer2feature
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,特征,answer2feature
问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,空间,answer2feature
问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,优化,answer2feature
问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,压缩,answer2feature
问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,空间,answer2feature
假设dataMat是一个p维的数据集合，有N个样本，它的协方差矩阵是X。那么协方差矩阵就通过奇异值分解写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8man1o3trj302h00hmx1.jpg),分解,answer2feature
"其中P是一个(p,p)维的正交矩阵，它的每一列都是X的特征向量。D是一个(p,p)维的对角矩阵，包含了特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8manhjc7qj301n00ga9x.jpg)。",特征值,answer2feature
最后还需要拉回原空间：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mb1hmrmnj306w00i747.jpg),空间,answer2feature
特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度,特征值,answer2feature
特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度,反映,answer2feature
两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,向量,answer2feature
两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,空间,answer2feature
矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,向量,answer2feature
矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,向量,answer2feature
矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,空间,answer2feature
定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。,DBSCAN,answer2feature
定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。,聚类,answer2feature
经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),深度,answer2feature
经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),ceiling,answer2feature
经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),log2,answer2feature
经验2：树的个数在256棵以下,树,answer2feature
需要人为选择阈值,阈值,answer2feature
一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。,聚类,answer2feature
缺点也就是聚类的缺点，包括初始点对结果的影响，数据是否保持凸型对结果对影响，簇的个数的选择,聚类,answer2feature
平均值修正：可用前后两个观测值的平均值修正该异常值；,平均值,answer2feature
平均值修正：可用前后两个观测值的平均值修正该异常值；,平均值,answer2feature
生成列新特征：category异常,特征,answer2feature
不处理：直接在具有异常值的数据集上进行数据挖掘；,数据挖掘,answer2feature
"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",下采样,answer2feature
"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",高维,answer2feature
"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",特征,answer2feature
上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,上采样,answer2feature
上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,平衡,answer2feature
上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,训练,answer2feature
比如99%为正样本，1%为负样本，如果分类器把所以样本预测为正样本则准确率高达99%，显然不符合实际情况,预测,answer2feature
采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降,原始数据,answer2feature
采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降,泛化能力,answer2feature
采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,过拟合,answer2feature
采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,原始数据,answer2feature
采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,训练,answer2feature
采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,过拟合,answer2feature
无放回的简单抽样：每条样本被采到的概率相等且都为1/N,无放回,answer2feature
无放回的简单抽样：每条样本被采到的概率相等且都为1/N,抽样,answer2feature
有放回的简单抽样：每条样本可能多次被选中,有放回,answer2feature
有放回的简单抽样：每条样本可能多次被选中,抽样,answer2feature
上采样：即合理地增加少数类的样本,上采样,answer2feature
下采样：欠抽样技术是将数据从原始数据集中移除,下采样,answer2feature
下采样：欠抽样技术是将数据从原始数据集中移除,抽样,answer2feature
下采样：欠抽样技术是将数据从原始数据集中移除,原始数据,answer2feature
平衡采样：考虑正负样本比,平衡,answer2feature
分层采样：通过某一些feature对数据进行切分，按照切分后的占比分别进行采样,feature,answer2feature
"整体采样：先将数据集T中的数据分组成G个互斥的簇,然后再从G个簇中简单随机采样s个簇作为样本集",互斥,answer2feature
"相对于采样随机的方法进行上采样,还有两种比较流行的上采样的改进方式：",上采样,answer2feature
"相对于采样随机的方法进行上采样,还有两种比较流行的上采样的改进方式：",上采样,answer2feature
ADASYN,ADASYN,answer2feature
平衡欠采样,平衡,answer2feature
EasyEnsemble，利用模型融合的方法（Ensemble）,EasyEnsemble,answer2feature
EasyEnsemble，利用模型融合的方法（Ensemble）,融合,answer2feature
EasyEnsemble，利用模型融合的方法（Ensemble）,Ensemble,answer2feature
少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合,训练,answer2feature
少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合,融合,answer2feature
BalanceCascade，利用模型融合的方法（Boost）,BalanceCascade,answer2feature
BalanceCascade，利用模型融合的方法（Boost）,融合,answer2feature
BalanceCascade，利用模型融合的方法（Boost）,Boost,answer2feature
每次剔除预测正确的多数样本，加入新的未预测的多数样本,预测,answer2feature
每次剔除预测正确的多数样本，加入新的未预测的多数样本,预测,answer2feature
NearMiss,NearMiss,answer2feature
选择离各种情况下的少数样本位置最远的多数样本进行训练,训练,answer2feature
可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,训练,answer2feature
可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,回归,answer2feature
可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,adaboost,answer2feature
通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀,特征,answer2feature
通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀,分类,answer2feature
合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合,平衡,answer2feature
合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合,融合,answer2feature
避免异常点：比如对连续变量进行份桶离散化,离散,answer2feature
可解释性或者需要连续输出：比如评分卡模型中的iv+woe,输出,answer2feature
使得原始数据的信息量更大：比如log/sqrt变换,原始数据,answer2feature
使得原始数据的信息量更大：比如log/sqrt变换,信息量,answer2feature
缩放仅仅跟最大、最小值的差别有关，只是一个去量纲的过程,差别,answer2feature
标准化(zscore),标准化,answer2feature
缩放和所有点都相关，数据相对分布不会改变，集中的数据标准化后依旧集中,标准化,answer2feature
更快的收敛,收敛,answer2feature
异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降,特征,answer2feature
异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降,标准化,answer2feature
分布角度：使用标准化之前，要求数据需要近似满足高斯分布，不然会改变数据的分布，尤其是对数据分布有强假设的情况下,标准化,answer2feature
上线变动角度：归一化在上线的时候需要考虑上下约束届是否需要变动，标准化则不需要考虑变动,标准化,answer2feature
值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定,标准化,answer2feature
值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定,输出,answer2feature
模型角度：一般涉及距离计算，协方差计算，数据满足高斯分布的情况下用标准化，其他归一化或其他变换,标准化,answer2feature
knn：计算距离，不去量冈则结果受值域范围影响大,knn,answer2feature
neuralnetwork：梯度异常问题+激活函数问题,梯度,answer2feature
neuralnetwork：梯度异常问题+激活函数问题,激活函数,answer2feature
截断,截断,answer2feature
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",截断,answer2feature
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",截断,answer2feature
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",特征,answer2feature
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",截断,answer2feature
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",截断,answer2feature
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",特征,answer2feature
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",特征,answer2feature
参考异常点里面的outlier识别，以最大值填充或者以None,最大值,answer2feature
数据分布过于不平衡,平衡,answer2feature
分桶,分桶,answer2feature
离散化,离散,answer2feature
zscore标准化,标准化,answer2feature
范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg),范数,answer2feature
L1范数,L1,answer2feature
L1范数,范数,answer2feature
L2范数,L2,answer2feature
L2范数,范数,answer2feature
平方根缩放,平方根,answer2feature
它将大端长尾压缩为短尾，并将小端进行延伸,压缩,answer2feature
可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)log(y),较差,answer2feature
可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)log(y),特征,answer2feature
通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,因变量,answer2feature
通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,自变量,answer2feature
通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,参数,answer2feature
特征交叉,特征,answer2feature
人为分段交叉,分段,answer2feature
提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释,拟合,answer2feature
提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释,向量,answer2feature
离散变量的交并补,离散,answer2feature
连续变量的点积，attention类似,点积,answer2feature
连续变量的点积，attention类似,attention,answer2feature
交叉中需要并行特征筛选的步骤,特征,answer2feature
交叉中需要并行特征筛选的步骤,筛选,answer2feature
FM/FFM中的矩阵点积,FM,answer2feature
FM/FFM中的矩阵点积,FFM,answer2feature
FM/FFM中的矩阵点积,点积,answer2feature
NeuralNetwork里面的dense,dense,answer2feature
通过树或者类似的特征组合模型去做最低熵的特征选择,树,answer2feature
通过树或者类似的特征组合模型去做最低熵的特征选择,特征,answer2feature
通过树或者类似的特征组合模型去做最低熵的特征选择,熵,answer2feature
通过树或者类似的特征组合模型去做最低熵的特征选择,特征选择,answer2feature
非线性编码,非线性,answer2feature
核向量进行升维,核,answer2feature
核向量进行升维,向量,answer2feature
树模型的叶子结点的stack,树,answer2feature
谱聚类/pca/svd等信息抽取编码,聚类,answer2feature
谱聚类/pca/svd等信息抽取编码,pca,answer2feature
谱聚类/pca/svd等信息抽取编码,svd,answer2feature
谱聚类/pca/svd等信息抽取编码,抽取,answer2feature
lda/EM等分布拟合表示,lda,answer2feature
lda/EM等分布拟合表示,EM,answer2feature
lda/EM等分布拟合表示,拟合,answer2feature
计数编码,计数,answer2feature
"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",特征,answer2feature
"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",计数,answer2feature
"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",非线性,answer2feature
"对异常值比较敏感,特征取值有可能冲突",特征,answer2feature
计数排名编码,计数,answer2feature
Embedding,Embedding,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",离散,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",特征,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",欠拟合,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",过拟合,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",特征,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",稀疏,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",目标,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",特征,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",分类,answer2feature
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",回归,answer2feature
类别特征之间交叉组合,特征,answer2feature
类别特征和数值特征之间交叉组合,特征,answer2feature
类别特征和数值特征之间交叉组合,特征,answer2feature
均值、中位数、标准差、最大值和最小值,均值,answer2feature
均值、中位数、标准差、最大值和最小值,中位数,answer2feature
均值、中位数、标准差、最大值和最小值,最大值,answer2feature
分位数、方差、vif值、分段冲量,分段,answer2feature
预处理手段有哪些？,预处理,answer2feature
分词,分词,answer2feature
LDA,LDA,answer2feature
词干提取,词干,answer2feature
文档特征,特征,answer2feature
文本向量化,向量,answer2feature
word2vec,word2vec,answer2feature
glove,glove,answer2feature
bert,bert,answer2feature
文本相似性,相似性,answer2feature
分词过程中会考虑哪些方面？,分词,answer2feature
词性标注,词性,answer2feature
词形还原和词干提取,词形,answer2feature
词形还原和词干提取,词干,answer2feature
词形还原为了通用性特征的提取,词形,answer2feature
词形还原为了通用性特征的提取,特征,answer2feature
词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,词干,answer2feature
词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,训练,answer2feature
词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,注意力,answer2feature
词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,词干,answer2feature
文本中的统计信息一般有哪些？,统计,answer2feature
直接统计值：,统计,answer2feature
不同词性个数,词性,answer2feature
直接统计值的统计信息：,统计,answer2feature
直接统计值的统计信息：,统计,answer2feature
最小最大均值方差标准差,均值,answer2feature
直接对文本特征进行整理手段有哪些？,特征,answer2feature
将文本转换为连续序列，扩充样本特征,特征,answer2feature
"权重评分，去除掉一些低重要性的词，比如每篇文章都出现的""的""，""了""",重要性,answer2feature
LDA,LDA,answer2feature
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,主题,answer2feature
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,抽取,answer2feature
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,狄利克雷,answer2feature
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,拟合,answer2feature
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,主题,answer2feature
用来衡量概率分布之间的相似性,相似性,answer2feature
向量化,向量,answer2feature
word2vec,word2vec,answer2feature
glove,glove,answer2feature
bert,bert,answer2feature
建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,attention,answer2feature
建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,bert,answer2feature
建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,xlnet,answer2feature
耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,特征,answer2feature
耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,特征,answer2feature
耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,训练,answer2feature
过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,过拟合,answer2feature
过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,特征,answer2feature
过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,维度,answer2feature
共线性：单因子对目标的作用被稀释，解释力下降,目标,answer2feature
方差，是的feature内的方向更大，对目标区分度提高更高贡献,feature,answer2feature
方差，是的feature内的方向更大，对目标区分度提高更高贡献,目标,answer2feature
相关性，与区分目标有高相关的特征才有意义,相关性,answer2feature
相关性，与区分目标有高相关的特征才有意义,目标,answer2feature
相关性，与区分目标有高相关的特征才有意义,特征,answer2feature
移除低方差特征,特征,answer2feature
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,特征,answer2feature
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,阈值,answer2feature
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,特征值,answer2feature
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,特征,answer2feature
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,特征,answer2feature
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,较差,answer2feature
相关性,相关性,answer2feature
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征选择,answer2feature
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征,answer2feature
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,目标,answer2feature
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征,answer2feature
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,重要性,answer2feature
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,重要性,answer2feature
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征,answer2feature
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征,answer2feature
卡方检验,卡方,answer2feature
熵检验,熵,answer2feature
互信息熵,熵,answer2feature
"度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立",相关性,answer2feature
"度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立",相关性,answer2feature
KL散度,KL,answer2feature
KL散度,散度,answer2feature
相对熵,熵,answer2feature
均值,均值,answer2feature
中位数,中位数,answer2feature
有效性存疑，取决于特征列数,有效性,answer2feature
有效性存疑，取决于特征列数,存疑,answer2feature
有效性存疑，取决于特征列数,特征,answer2feature
生成的插值来源于其他列的特征，是不是意味着插值的结果已经是和其他列的组合高相关,特征,answer2feature
离散特征新增缺失的category,离散,answer2feature
离散特征新增缺失的category,特征,answer2feature
把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果,embedding,answer2feature
把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果,向量,answer2feature
可以参考YouTube中的新商品向量生成逻辑,YouTube,answer2feature
可以参考YouTube中的新商品向量生成逻辑,向量,answer2feature
可以参考YouTube中的新商品向量生成逻辑,逻辑,answer2feature
bert中的\[UNK]向量，\[unused]向量,bert,answer2feature
bert中的\[UNK]向量，\[unused]向量,向量,answer2feature
bert中的\[UNK]向量，\[unused]向量,向量,answer2feature
如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,原始数据,answer2feature
如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,高维,answer2feature
如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,多元,answer2feature
实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多,机器学习,answer2feature
实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多,离散,answer2feature
快速,快速,answer2feature
对原始数据的前提假设最少，也不会影响到非缺失列,原始数据,answer2feature
在深度学习中，hold位填充方法用的最多,深度,answer2feature
在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,拟合,answer2feature
在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,向量,answer2feature
在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,收敛,answer2feature
而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性,向量,answer2feature
而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性,向量,answer2feature
|模型|ID3|C4.5|CART|,ID3,answer2feature
|模型|ID3|C4.5|CART|,CART,answer2feature
|特征选择|信息增益|信息增益率|Gini系数/均方差|,特征选择,answer2feature
|特征选择|信息增益|信息增益率|Gini系数/均方差|,增益,answer2feature
|特征选择|信息增益|信息增益率|Gini系数/均方差|,增益,answer2feature
1.构建根节点，将所有训练数据都放在根节点,训练,answer2feature
2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,特征,answer2feature
2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,特征,answer2feature
2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,训练,answer2feature
2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,分类,answer2feature
3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,递归,answer2feature
3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,训练,answer2feature
3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,分类,answer2feature
3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,特征,answer2feature
其中，D为数据全集，C为不同因变量类别k上的子集(传统意义上的y的种类),因变量,answer2feature
条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,特征,answer2feature
条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,分类,answer2feature
条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,不确定性,answer2feature
信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：,增益,answer2feature
信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：,特征,answer2feature
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,特征,answer2feature
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,信息量,answer2feature
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,特征,answer2feature
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,特征,answer2feature
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,增益,answer2feature
"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",增益,answer2feature
"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",熵,answer2feature
"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",平衡,answer2feature
"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",分类,answer2feature
信息增益率：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9260xx5wej304b017q2r.jpg),增益,answer2feature
对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg),特征,answer2feature
对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg),特征,answer2feature
存在偏向于选择取值较多的特征问题,特征,answer2feature
主动进行的连续的特征离散化,特征,answer2feature
主动进行的连续的特征离散化,离散,answer2feature
比如m个样本的连续特征A有m个，先set在order，再两两组合求中间值，以该值点作为划分的待选点,特征,answer2feature
**连续特征可以再后序特征划分中仍可继续参与计算**,特征,answer2feature
**连续特征可以再后序特征划分中仍可继续参与计算**,特征,answer2feature
缺失问题优化,优化,answer2feature
训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,训练,answer2feature
训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,增益,answer2feature
训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,增益,answer2feature
预测：直接跳过该节点，并将此样本划入所有子节点，划分后乘以系数计算，系数为不缺失部分的样本分布,预测,answer2feature
分类情况下的变量特征选择,分类,answer2feature
分类情况下的变量特征选择,特征选择,answer2feature
离散变量：二分划分,离散,answer2feature
回归情况下，连续变量不再采取中间值划分，采用最小方差法,回归,answer2feature
我们的算法从根节点开始，用训练集递归的建立CART树。,训练,answer2feature
我们的算法从根节点开始，用训练集递归的建立CART树。,递归,answer2feature
我们的算法从根节点开始，用训练集递归的建立CART树。,CART,answer2feature
我们的算法从根节点开始，用训练集递归的建立CART树。,树,answer2feature
对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,阈值,answer2feature
对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,特征,answer2feature
对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,决策,answer2feature
对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,递归,answer2feature
计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归,阈值,answer2feature
计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归,递归,answer2feature
计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数,特征,answer2feature
计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数,特征值,answer2feature
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征,answer2feature
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征值,answer2feature
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征,answer2feature
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征值,answer2feature
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征,answer2feature
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征值,answer2feature
递归1～4,递归,answer2feature
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),特征,answer2feature
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),特征,answer2feature
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),特征值,answer2feature
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),均值,answer2feature
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),均值,answer2feature
回归树：利用最终叶子的均值或者中位数来作为输出结果,回归,answer2feature
回归树：利用最终叶子的均值或者中位数来作为输出结果,树,answer2feature
回归树：利用最终叶子的均值或者中位数来作为输出结果,均值,answer2feature
回归树：利用最终叶子的均值或者中位数来作为输出结果,中位数,answer2feature
回归树：利用最终叶子的均值或者中位数来作为输出结果,输出,answer2feature
分类树：利用最终叶子的大概率的分类类别来作为输出结果,分类,answer2feature
分类树：利用最终叶子的大概率的分类类别来作为输出结果,树,answer2feature
分类树：利用最终叶子的大概率的分类类别来作为输出结果,分类,answer2feature
分类树：利用最终叶子的大概率的分类类别来作为输出结果,输出,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,目标,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,函数,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,参数,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,线性回归,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,训练,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,预测,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,误差,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,分类,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,树,answer2feature
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,回归,answer2feature
当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,CART,answer2feature
当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,树,answer2feature
当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,CART,answer2feature
当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,树,answer2feature
当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,树,answer2feature
当然，这是两种极端情况。一般来说，𝛼越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的𝛼，一定存在使损失函数𝐶𝛼(𝑇)最小的唯一子树。,剪枝,answer2feature
当然，这是两种极端情况。一般来说，𝛼越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的𝛼，一定存在使损失函数𝐶𝛼(𝑇)最小的唯一子树。,损失,answer2feature
当然，这是两种极端情况。一般来说，𝛼越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的𝛼，一定存在使损失函数𝐶𝛼(𝑇)最小的唯一子树。,函数,answer2feature
"由枝剪到根结点及不枝剪两种情况可得：𝛼=(𝐶(𝑇)−𝐶(𝑇𝑡))/(|𝑇𝑡|−1),C(T)为根结点误差",误差,answer2feature
计算出每个子树是否剪枝的阈值𝛼,剪枝,answer2feature
计算出每个子树是否剪枝的阈值𝛼,阈值,answer2feature
选择阈值𝛼集合中的最小值,阈值,answer2feature
分别针对不同的最小值𝛼所对应的剪枝后的最优子树做交叉验证,剪枝,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,分类,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,树,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,回归,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,树,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,离散,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,树,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,梯度,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,导数,answer2feature
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,树,answer2feature
缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散,特征,answer2feature
缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散,离散,answer2feature
可以解决线性及非线性问题,非线性,answer2feature
有特征选择等辅助功能,特征选择,answer2feature
正负量级有偏样本的样本效果较差,较差,answer2feature
单棵树的拟合效果欠佳，容易过拟合,拟合,answer2feature
单棵树的拟合效果欠佳，容易过拟合,过拟合,answer2feature
从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2·||W||·||W||)subjectto：y(wx+b)>=1，其中||·||为2范数,分类,answer2feature
从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2·||W||·||W||)subjectto：y(wx+b)>=1，其中||·||为2范数,优化,answer2feature
从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2·||W||·||W||)subjectto：y(wx+b)>=1，其中||·||为2范数,范数,answer2feature
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,优化,answer2feature
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,优化,answer2feature
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,优化,answer2feature
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,拉格朗日,answer2feature
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,对偶,answer2feature
最后再利用SMO（序列最小优化）来解决这个对偶问题,优化,answer2feature
最后再利用SMO（序列最小优化）来解决这个对偶问题,对偶,answer2feature
在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。换句话说，就是超平面附近决定超平面位置的那些参与计算锁定平面位置的点,向量,answer2feature
支持向量的添加才会提高，否则无效,向量,answer2feature
可以，把loss函数变为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93jrdscfrj309l011dfo.jpg),函数,answer2feature
非线性问题,非线性,answer2feature
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,SVM,answer2feature
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,拉格朗日,answer2feature
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,核,answer2feature
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,函数,answer2feature
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,非线性,answer2feature
硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器,SVM,answer2feature
硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器,训练,answer2feature
软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器,SVM,answer2feature
软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器,训练,answer2feature
kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,训练,answer2feature
kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,核,answer2feature
kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,函数,answer2feature
kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,非线性,answer2feature
svm原问题是：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),svm,answer2feature
svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg),svm,answer2feature
svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg),对偶,answer2feature
"拉格朗日乘子法：求f的最小值时，有h=0的限制条件，那么就构造∑λh+f=Loss,作为新loss",拉格朗日,answer2feature
引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件,拉格朗日,answer2feature
引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件,限制性,answer2feature
在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,求偏,answer2feature
在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,求偏,answer2feature
在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,导数,answer2feature
因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,限制性,answer2feature
因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,规划,answer2feature
因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,对偶,answer2feature
引入了核函数,核,answer2feature
引入了核函数,函数,answer2feature
原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题,限制性,answer2feature
原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题,对偶,answer2feature
因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0,向量,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,求偏,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,求偏,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,导数,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,联立方程,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,不等式,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,优化,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,优化,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,KKT,answer2feature
KKT乘子λ>=0,KKT,answer2feature
原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),损失,answer2feature
原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),函数,answer2feature
优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),优化,answer2feature
优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),损失,answer2feature
优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),函数,answer2feature
要求KKT乘子λ>=0,KKT,answer2feature
核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,核,answer2feature
核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,函数,answer2feature
核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,特征,answer2feature
核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,高维,answer2feature
线性核函数：主要用于线性可分的情形。参数少，速度快。,核,answer2feature
线性核函数：主要用于线性可分的情形。参数少，速度快。,函数,answer2feature
线性核函数：主要用于线性可分的情形。参数少，速度快。,参数,answer2feature
多项式核函数：,多项式,answer2feature
多项式核函数：,核,answer2feature
多项式核函数：,函数,answer2feature
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,核,answer2feature
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,函数,answer2feature
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,参数,answer2feature
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,分类,answer2feature
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,参数,answer2feature
sigmoid核函数：,sigmoid,answer2feature
sigmoid核函数：,核,answer2feature
sigmoid核函数：,函数,answer2feature
拉普拉斯核函数：,拉普拉斯,answer2feature
拉普拉斯核函数：,核,answer2feature
拉普拉斯核函数：,函数,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,核,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,函数,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,核,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,函数,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,特征,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,核,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,函数,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,特征,answer2feature
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,非线性,answer2feature
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,机器学习,answer2feature
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,核,answer2feature
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,函数,answer2feature
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,LibSVM,answer2feature
"2)多项式：K(v1,v2)=(r<v1,v2>+c)^n",多项式,answer2feature
"3)Radialbasisfunction：K(v1,v2)=exp(r||v1v2||^2)",exp,answer2feature
"4)Sigmoid：tanh(r<v1,v2>+c)",Sigmoid,answer2feature
Mercer定理：核函数矩阵是对称半正定的,核,answer2feature
Mercer定理：核函数矩阵是对称半正定的,函数,answer2feature
计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),非线性,answer2feature
计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),分类,answer2feature
计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),复杂度,answer2feature
在使用核函数的时候参数假设全靠试，时间成本过高,核,answer2feature
在使用核函数的时候参数假设全靠试，时间成本过高,函数,answer2feature
在使用核函数的时候参数假设全靠试，时间成本过高,参数,answer2feature
e的n次方的泰勒展开得到了一个无穷维度的映射,泰勒展开,answer2feature
e的n次方的泰勒展开得到了一个无穷维度的映射,维度,answer2feature
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,svm,answer2feature
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,svm,answer2feature
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,margin,answer2feature
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,决策,answer2feature
逻辑回归，线性svm,逻辑,answer2feature
逻辑回归，线性svm,回归,answer2feature
逻辑回归，线性svm,svm,answer2feature
非线性：,非线性,answer2feature
贝叶斯，决策树，核svm，DNN,核,answer2feature
贝叶斯，决策树，核svm，DNN,svm,answer2feature
贝叶斯，决策树，核svm，DNN,DNN,answer2feature
数据量大特征多：,特征,answer2feature
逻辑回归,逻辑,answer2feature
逻辑回归,回归,answer2feature
数据量少特征少：,特征,answer2feature
核svm,核,answer2feature
核svm,svm,answer2feature
树模型,树,answer2feature
LR是参数模型，SVM为非参数模型。,LR,answer2feature
LR是参数模型，SVM为非参数模型。,参数,answer2feature
LR是参数模型，SVM为非参数模型。,SVM,answer2feature
LR是参数模型，SVM为非参数模型。,参数,answer2feature
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,LR,answer2feature
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,损失,answer2feature
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,函数,answer2feature
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,logisticalloss,answer2feature
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,SVM,answer2feature
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,hingeloss,answer2feature
在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,SVM,answer2feature
在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,分类,answer2feature
在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,向量,answer2feature
LR的模型相对简单，在进行大规模线性分类时比较方便。,LR,answer2feature
LR的模型相对简单，在进行大规模线性分类时比较方便。,分类,answer2feature
"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",mse,answer2feature
"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",均方,answer2feature
"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",误差,answer2feature
最小二乘,二乘,answer2feature
损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93458dklvj306l011t8j.jpg),损失,answer2feature
损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93458dklvj306l011t8j.jpg),函数,answer2feature
使右侧为0可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934a6q83tj304300kdfm.jpg),右侧,answer2feature
如果X点乘X的转置可逆则有唯一解，否则无法如此求解,可逆,answer2feature
梯度下降,梯度,answer2feature
损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg),损失,answer2feature
损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg),函数,answer2feature
求导可得梯度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934mwtnodj307401fdfp.jpg),梯度,answer2feature
加上l2的线性回归：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934px37g4j305b017glf.jpg),l2,answer2feature
加上l2的线性回归：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934px37g4j305b017glf.jpg),线性回归,answer2feature
在用最小二乘推导的过程和上面一样，最后在结果上进行了平滑，保证有解：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934t64beij304w00kmwy.jpg),二乘,answer2feature
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,特征,answer2feature
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,稀疏,answer2feature
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,线性关系,answer2feature
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,特征,answer2feature
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,特征,answer2feature
kmeans是两个步骤交替进行，可以分别看成E步和M步,kmeans,answer2feature
M步中将每类的中心更新为分给该类各点的均值，可以认为是在「各类分布均为单位方差的高斯分布」的假设下，最大化似然值；,均值,answer2feature
E步中将每个点分给中心距它最近的类（硬分配），可以看成是EM算法中E步（软分配）的近似,EM,answer2feature
M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,参数,answer2feature
M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,MSE,answer2feature
M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,MSE,answer2feature
M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,收敛,answer2feature
先层次聚类，再在不同层次上选取初始点进行kmeans聚类,聚类,answer2feature
先层次聚类，再在不同层次上选取初始点进行kmeans聚类,kmeans,answer2feature
先层次聚类，再在不同层次上选取初始点进行kmeans聚类,聚类,answer2feature
贝叶斯公式是完整的数学公式P(A/B)=P(A)P(B/A)/P(B),贝叶斯公式,answer2feature
贝叶斯公式是完整的数学公式P(A/B)=P(A)P(B/A)/P(B),数学公式,answer2feature
"朴素贝叶斯=贝叶斯公式+条件独立假设，在实际使用过程中，朴素贝叶斯完全只需要关注P(A,B)=P(A)P(B/A)即可",贝叶斯公式,answer2feature
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,多项式,answer2feature
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,多项式,answer2feature
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,离散,answer2feature
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,特征,answer2feature
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,领域,answer2feature
因为统计次数，所以会出现0次可能，所以实际中进行了平滑操作,统计,answer2feature
两者形式非常像，区别就在先验平滑分母考虑的是平滑类别y个数，后验平滑分母考虑的是平滑特征对应特征x可选的个数,特征,answer2feature
两者形式非常像，区别就在先验平滑分母考虑的是平滑类别y个数，后验平滑分母考虑的是平滑特征对应特征x可选的个数,特征,answer2feature
高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),特征,answer2feature
高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),高斯公式,answer2feature
高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),blob,answer2feature
高斯模型假设在对应类别下的每一维特征都服从高斯分布（正态分布）,特征,answer2feature
高斯模型假设在对应类别下的每一维特征都服从高斯分布（正态分布）,正态分布,answer2feature
伯努利：伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次,离散,answer2feature
伯努利：伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次,特征,answer2feature
拉普拉斯平滑,拉普拉斯,answer2feature
优点：对小规模数据表现很好，适合多分类任务，适合增量式训练,分类,answer2feature
优点：对小规模数据表现很好，适合多分类任务，适合增量式训练,训练,answer2feature
缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）,离散,answer2feature
生成模型和判别模型,判别,answer2feature
分布函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93b9whhwuj306z01amwz.jpg),函数,answer2feature
密度函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93bdnikzbj306e01pjr8.jpg),函数,answer2feature
其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,参数,answer2feature
其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,参数,answer2feature
其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,logistic,answer2feature
y=sigmoid(f(x)),sigmoid,answer2feature
可以看作是一次线性拟合+一次sigmoid的非线性变化,拟合,answer2feature
可以看作是一次线性拟合+一次sigmoid的非线性变化,sigmoid,answer2feature
可以看作是一次线性拟合+一次sigmoid的非线性变化,非线性,answer2feature
对于lr来说事情只有发生和不发生两种可能，对于已知样本来说，满足伯努利的概率假设：,lr,answer2feature
第i个样本正确预测的概率如上可得,预测,answer2feature
几率odds,odds,answer2feature
数据特征下属于正例及反例的比值,特征,answer2feature
极大似然,极大似然,answer2feature
第i个样本正确预测的概率如上可得每条样本的情况下,预测,answer2feature
综合全部样本发生的概率都要最大的话，采取极大似然连乘可得：,极大似然,answer2feature
损失函数,损失,answer2feature
损失函数,函数,answer2feature
通常会对极大似然取对数，得到损失函数，方便计算,极大似然,answer2feature
通常会对极大似然取对数，得到损失函数，方便计算,损失,answer2feature
通常会对极大似然取对数，得到损失函数，方便计算,函数,answer2feature
梯度下降,梯度,answer2feature
损失函数求偏导，更新θ,损失,answer2feature
损失函数求偏导，更新θ,函数,answer2feature
损失函数求偏导，更新θ,求偏,answer2feature
首先需要理解梯度下降的更新公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cn8ok1fj307a01ft8k.jpg),梯度,answer2feature
同一条样本不同特征维度进行拆分，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)处并行，把![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93csl2l0pj301400ia9t.jpg)内的xi和Wi拆分成块分别计算后合并，再把外层![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)同样拆分成若干块进行计算,特征,answer2feature
同一条样本不同特征维度进行拆分，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)处并行，把![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93csl2l0pj301400ia9t.jpg)内的xi和Wi拆分成块分别计算后合并，再把外层![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)同样拆分成若干块进行计算,维度,answer2feature
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,特征,answer2feature
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,拟合,answer2feature
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,比率,answer2feature
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,回归,answer2feature
1.点击行为为正向，未点击行为为负向，ctr需要得到点击行为的概率，lr可以产出正向行为的概率，完美match,lr,answer2feature
2.实现简单，方便并行，计算迭代速度很快,迭代,answer2feature
3.可解释性强，可结合正则化等优化方法,优化方法,answer2feature
特征之间尽可能独立,特征,answer2feature
不独立所以我们把不独立的特征交叉了,特征,answer2feature
还记得FM的思路？,FM,answer2feature
离散特征,离散,answer2feature
离散特征,特征,answer2feature
连续特征通常没有特别含义，31岁和32岁差在哪？,特征,answer2feature
离散特征方便交叉考虑,离散,answer2feature
离散特征方便交叉考虑,特征,answer2feature
使的lr满足分布假设,lr,answer2feature
在某种确定分类上的特征分布满足高斯分布,分类,answer2feature
在某种确定分类上的特征分布满足高斯分布,特征,answer2feature
C1和C2为正负类，观测样本中该特征在正负类中出现概率的比值满足线性条件的前提就是P服从正太分布,特征,answer2feature
实际中不满足的很多，不满足我们通常就离散化，oneHotEncode,离散,answer2feature
此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中,贝叶斯公式,answer2feature
此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中,blob,answer2feature
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,lr,answer2feature
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,odds,answer2feature
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,线性回归,answer2feature
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,odds,answer2feature
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,LR,answer2feature
思路二：Exponentialmodel的形式是这样的：假设第i个特征对第k类的贡献是![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmq11x6zj300s00f3y9.jpg)，则数据点![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmqpotqfj302700imwx.jpg)属于第k类的概率正比于![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmrp4qv4j305b00i0sj.jpg)。,特征,answer2feature
化简即为sigmoid,sigmoid,answer2feature
思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,glm,answer2feature
思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,lr,answer2feature
思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,sigmoid,answer2feature
思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,函数,answer2feature
直接对分类模型进行建模，前提假设为非常弱的指定类别上自变量的条件分布满足高斯,分类,answer2feature
直接对分类模型进行建模，前提假设为非常弱的指定类别上自变量的条件分布满足高斯,自变量,answer2feature
由预测0/1的类别扩展到了预测01的概率值,预测,answer2feature
由预测0/1的类别扩展到了预测01的概率值,预测,answer2feature
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,线性回归,answer2feature
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,全量,answer2feature
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,敏感度,answer2feature
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,sigmoid,answer2feature
sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,sigmoid,answer2feature
sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,逻辑,answer2feature
sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,回归,answer2feature
sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,参数,answer2feature
更新速度只与真实的x和y相关，与激活函数无关，更新平稳,激活函数,answer2feature
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,mse,answer2feature
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,激活函数,answer2feature
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,sigmoid,answer2feature
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,sigmoid,answer2feature
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,函数,answer2feature
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,梯度,answer2feature
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,快速,answer2feature
mse下的lr损失函数非凸，难以得到解析解,mse,answer2feature
mse下的lr损失函数非凸，难以得到解析解,lr,answer2feature
mse下的lr损失函数非凸，难以得到解析解,损失,answer2feature
mse下的lr损失函数非凸，难以得到解析解,函数,answer2feature
way1:把01的sigmoid的lr结果Y映射为2y1，推导不变,sigmoid,answer2feature
way1:把01的sigmoid的lr结果Y映射为2y1，推导不变,lr,answer2feature
"way2:把激活函数换成tanh，因为tanh的值域范围为\[1,1],满足结果，推导不变",激活函数,answer2feature
way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,sigmoid,answer2feature
way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,函数,answer2feature
way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,函数,answer2feature
way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,极大似然,answer2feature
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,损失,answer2feature
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,函数,answer2feature
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,收敛,answer2feature
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,特征,answer2feature
每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了,特征,answer2feature
每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了,特征,answer2feature
增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,训练,answer2feature
增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,收敛,answer2feature
增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,收敛,answer2feature
增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,估计,answer2feature
泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,泛化能力,answer2feature
泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,训练,answer2feature
泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,特征,answer2feature
结论：可以，加l2正则项后可用,l2,answer2feature
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,核,answer2feature
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,逻辑,answer2feature
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,回归,answer2feature
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,拟合,answer2feature
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,参数,answer2feature
凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,L2,answer2feature
凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,核,answer2feature
凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,函数,answer2feature
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",L1,answer2feature
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",参数,answer2feature
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",拉普拉斯,answer2feature
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",lr,answer2feature
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",损失,answer2feature
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",函数,answer2feature
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",L2,answer2feature
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",参数,answer2feature
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",均值,answer2feature
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",lr,answer2feature
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",损失,answer2feature
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",函数,answer2feature
这个问题还可以换一个说法，l1和l2的各自作用。,l1,answer2feature
这个问题还可以换一个说法，l1和l2的各自作用。,l2,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,l1,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,参数,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,拉普拉斯,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,l2,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,参数,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,均值,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,拉普拉斯,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,特征,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,l1,answer2feature
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,l2,answer2feature
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,最小化,answer2feature
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,最小化,answer2feature
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,训练,answer2feature
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,误差,answer2feature
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,最小化,answer2feature
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,预测,answer2feature
特征交叉，类似fm,特征,answer2feature
核逻辑回归，类似svm,核,answer2feature
核逻辑回归，类似svm,逻辑,answer2feature
核逻辑回归，类似svm,回归,answer2feature
核逻辑回归，类似svm,svm,answer2feature
线性变换+非线性激活，类似neuralnetwork,非线性,answer2feature
**模型中对数据对处理一般都有一个标答是提升数据表达能力，也就是使数据含有的可分信息量更大**,信息量,answer2feature
加速收敛,加速,answer2feature
加速收敛,收敛,answer2feature
提高计算效率,效率,answer2feature
梯度下降过程稳定,梯度,answer2feature
[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),标准化,answer2feature
[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),blob,answer2feature
[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),预处理,answer2feature
原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,离散,answer2feature
原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,非线性,answer2feature
原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,拟合,answer2feature
离散后结合正则化可以进行特征筛选，更好防止过拟合,离散,answer2feature
离散后结合正则化可以进行特征筛选，更好防止过拟合,特征,answer2feature
离散后结合正则化可以进行特征筛选，更好防止过拟合,筛选,answer2feature
离散后结合正则化可以进行特征筛选，更好防止过拟合,过拟合,answer2feature
离散变量的计算相对于连续变量更快,离散,answer2feature
lr的output是彼此之间相对谁的可能性更高，而不是概率，概率是事情发生的可能，lr的output不代表可能,lr,answer2feature
lr的output是彼此之间相对谁的可能性更高，而不是概率，概率是事情发生的可能，lr的output不代表可能,lr,answer2feature
lr和线性回归,lr,answer2feature
lr和线性回归,线性回归,answer2feature
lr解用的极大似然，线性回归用的最小二乘,lr,answer2feature
lr解用的极大似然，线性回归用的最小二乘,极大似然,answer2feature
lr解用的极大似然，线性回归用的最小二乘,线性回归,answer2feature
lr解用的极大似然，线性回归用的最小二乘,二乘,answer2feature
lr用于分类，线性回归用于回归,lr,answer2feature
lr用于分类，线性回归用于回归,分类,answer2feature
lr用于分类，线性回归用于回归,线性回归,answer2feature
lr用于分类，线性回归用于回归,回归,answer2feature
但两者都是广义线性回归GLM问题,线性回归,answer2feature
但两者都是广义线性回归GLM问题,GLM,answer2feature
两者对非线性问题的处理能力都是欠佳的,非线性,answer2feature
lr和最大熵,lr,answer2feature
lr和最大熵,熵,answer2feature
lr和svm,lr,answer2feature
lr和svm,svm,answer2feature
都可分类，都是判别式模型思路,分类,answer2feature
都可分类，都是判别式模型思路,判别式,answer2feature
lr是交叉熵，svm是HingeLoss,lr,answer2feature
lr是交叉熵，svm是HingeLoss,熵,answer2feature
lr是交叉熵，svm是HingeLoss,svm,answer2feature
lr是交叉熵，svm是HingeLoss,HingeLoss,answer2feature
lr是全量数据拟合，svm是支持向量拟合,lr,answer2feature
lr是全量数据拟合，svm是支持向量拟合,全量,answer2feature
lr是全量数据拟合，svm是支持向量拟合,拟合,answer2feature
lr是全量数据拟合，svm是支持向量拟合,svm,answer2feature
lr是全量数据拟合，svm是支持向量拟合,向量,answer2feature
lr是全量数据拟合，svm是支持向量拟合,拟合,answer2feature
lr是参数估计有参数的前提假设，svm没有,lr,answer2feature
lr是参数估计有参数的前提假设，svm没有,参数,answer2feature
lr是参数估计有参数的前提假设，svm没有,svm,answer2feature
lr依赖的是极大似然，svm依赖的是距离,lr,answer2feature
lr依赖的是极大似然，svm依赖的是距离,极大似然,answer2feature
lr依赖的是极大似然，svm依赖的是距离,svm,answer2feature
lr和朴素贝叶斯,lr,answer2feature
如果朴素贝叶斯也有在某一类上的数据x满足高斯分布的假设前提，lr和朴素贝叶斯一致,lr,answer2feature
lr是判别模型，朴素贝叶斯是生成模型,lr,answer2feature
lr是判别模型，朴素贝叶斯是生成模型,判别,answer2feature
lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,lr,answer2feature
lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,feature,answer2feature
lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,feature,answer2feature
lr和最大熵模型,lr,answer2feature
lr和最大熵模型,熵,answer2feature
最大熵模型在解决二分类问题就是逻辑回归,熵,answer2feature
最大熵模型在解决二分类问题就是逻辑回归,逻辑,answer2feature
最大熵模型在解决二分类问题就是逻辑回归,回归,answer2feature
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,熵,answer2feature
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,分类,answer2feature
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,逻辑,answer2feature
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,回归,answer2feature
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,回归,answer2feature
随机梯度下降,梯度,answer2feature
批梯度下降,梯度,answer2feature
mini批梯度下降,梯度,answer2feature
除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,梯度,answer2feature
除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,参数,answer2feature
除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,深度,answer2feature
简单，易部署，训练速度快,训练,answer2feature
数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),平衡,answer2feature
数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),blob,answer2feature
数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),预处理,answer2feature
数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),平衡,answer2feature
特征筛选，特征的系数决定该特征的重要性,特征,answer2feature
特征筛选，特征的系数决定该特征的重要性,筛选,answer2feature
特征筛选，特征的系数决定该特征的重要性,特征,answer2feature
特征筛选，特征的系数决定该特征的重要性,特征,answer2feature
特征筛选，特征的系数决定该特征的重要性,重要性,answer2feature
sklearn.linear_model.LogisticRegression,sklearn,answer2feature
sklearn.linear_model.LogisticRegression,LogisticRegression,answer2feature
看部分参数的解释,参数,answer2feature
比如输出值的形式，输出的格式,输出,answer2feature
比如输出值的形式，输出的格式,输出,answer2feature
penalty是正则化，solver是函数优化方法,函数,answer2feature
penalty是正则化，solver是函数优化方法,优化方法,answer2feature
penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,l1,answer2feature
penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,l2,answer2feature
penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,坐标轴,answer2feature
penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,梯度,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,梯度,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,l1,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,损失,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,函数,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,一阶,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,导数,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,坐标轴,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,l1,answer2feature
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,l2,answer2feature
l1和l2选择参考上面讲的正则化部分,l1,answer2feature
l1和l2选择参考上面讲的正则化部分,l2,answer2feature
随机梯度下降在数据较少的时候最好别用，但是速度比较快。默认的是坐标轴下降法,梯度,answer2feature
随机梯度下降在数据较少的时候最好别用，但是速度比较快。默认的是坐标轴下降法,坐标轴,answer2feature
首先，决定是否为多分类的参数是multi_class,分类,answer2feature
首先，决定是否为多分类的参数是multi_class,参数,answer2feature
在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,LabelEncoder,answer2feature
在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,目标值,answer2feature
在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,离散,answer2feature
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,逻辑,answer2feature
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,回归,answer2feature
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,特征,answer2feature
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,函数,answer2feature
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,梯度,answer2feature
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,参数,answer2feature
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,逻辑,answer2feature
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,回归,answer2feature
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,非线性,answer2feature
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,核,answer2feature
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,svm,answer2feature
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,逻辑,answer2feature
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,回归,answer2feature
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,特征,answer2feature
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,核,answer2feature
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,线性回归,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,逻辑,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,回归,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,鲁棒,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,SVM,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,logistic,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,回归,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,向量,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,判别,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,logistic,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,回归,answer2feature
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,参数,answer2feature
随机森林=bagging+决策树,随机森林,answer2feature
随机森林=bagging+决策树,bagging,answer2feature
随机：特征选择随机+数据采样随机,特征选择,answer2feature
特征随机是在决策树**每个结点上选择的时候随机**，并不是在每棵树创建的时候随机,特征,answer2feature
每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,特征选择,answer2feature
每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,全量,answer2feature
每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,特征,answer2feature
数据采样，是有放回的采样,有放回,answer2feature
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,回归,answer2feature
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,回归,answer2feature
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,输出,answer2feature
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,分类,answer2feature
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,输出,answer2feature
CART树,CART,answer2feature
CART树,树,answer2feature
从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,特征,answer2feature
从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,特征,answer2feature
从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,特征,answer2feature
不需要剪枝，直到该节点的所有训练样例都属于同一类,剪枝,answer2feature
不需要剪枝，直到该节点的所有训练样例都属于同一类,训练,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),离散,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),CART,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),分类,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),树,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),blob,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),机器学习,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),CART,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),回归,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),树,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),blob,answer2feature
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),机器学习,answer2feature
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",分类,answer2feature
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",RF,answer2feature
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",CART,answer2feature
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",分类,answer2feature
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",树,answer2feature
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",标准,answer2feature
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",增益,answer2feature
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,回归,answer2feature
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,RF,answer2feature
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,CART,answer2feature
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,回归,answer2feature
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,树,answer2feature
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,mse,answer2feature
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,标准,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),损失,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),函数,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),CART,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),分类,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),树,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),blob,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),机器学习,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),CART,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),回归,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),树,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),blob,answer2feature
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),机器学习,answer2feature
增加树的数量,树,answer2feature
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),bagging,answer2feature
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),期望,answer2feature
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),期望,answer2feature
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),Bagging,answer2feature
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),Boosting,answer2feature
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),blob,answer2feature
随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,过拟合,answer2feature
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,特征选择,answer2feature
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,特征,answer2feature
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,特征,answer2feature
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,差别,answer2feature
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,特征,answer2feature
通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）,特征值,answer2feature
是使用uniform或者gaussian抽取随机值替换原特征,抽取,answer2feature
是使用uniform或者gaussian抽取随机值替换原特征,特征,answer2feature
除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,随机森林,answer2feature
除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,特征,answer2feature
除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,特征,answer2feature
要调整的参数主要是n_estimators和max_features,参数,answer2feature
n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,树,answer2feature
n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,树,answer2feature
n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,临界值,answer2feature
max_features是分割节点时考虑的特征的随机子集的大小。这个值越低，方差减小得越多，但是偏差的增大也越多,特征,answer2feature
回归：max_features=n_features,回归,answer2feature
分类：max_features=sqrt(n_features),分类,answer2feature
其他参数中,参数,answer2feature
max_depth=None和min_samples_split=2结合，为不限制生成一个不修剪的完全树,修剪,answer2feature
max_depth=None和min_samples_split=2结合，为不限制生成一个不修剪的完全树,树,answer2feature
不同决策树可以由不同主机并行训练生成，效率很高,训练,answer2feature
不同决策树可以由不同主机并行训练生成，效率很高,效率,answer2feature
随机森林算法继承了CART的优点,随机森林,answer2feature
随机森林算法继承了CART的优点,CART,answer2feature
将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题,bagging,answer2feature
将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题,过拟合,answer2feature
初始化训练一个弱学习器，初始化下的各条样本的权重一致,初始化,answer2feature
初始化训练一个弱学习器，初始化下的各条样本的权重一致,训练,answer2feature
初始化训练一个弱学习器，初始化下的各条样本的权重一致,初始化,answer2feature
基于调整后的样本及样本权重训练下一个弱学习器,训练,answer2feature
预测时直接串联综合各学习器的加权结果,预测,answer2feature
预测时直接串联综合各学习器的加权结果,串联,answer2feature
预测时直接串联综合各学习器的加权结果,加权,answer2feature
回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值,回归,answer2feature
回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值,均值,answer2feature
分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性,分枝,answer2feature
分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性,误差,answer2feature
属性上有多个值，则需要遍历所有可能的属性值，挑选使平方误差最小的划分属性值作为本属性的划分值,误差,answer2feature
递归重复以上步骤，直到满足叶子结点上值的要求,递归,answer2feature
adaboost，gbdt等等,adaboost,answer2feature
adaboost，gbdt等等,gbdt,answer2feature
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,boostingtree,answer2feature
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,拟合,answer2feature
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,mse,answer2feature
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,回归,answer2feature
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,损失,answer2feature
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,函数,answer2feature
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,分类,answer2feature
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,gbdt,answer2feature
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,拟合,answer2feature
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,损失,answer2feature
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,函数,answer2feature
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,梯度,answer2feature
Carttree，但是都是回归树,回归,answer2feature
Carttree，但是都是回归树,树,answer2feature
mse:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bmopzfjj303s011t8i.jpg),mse,answer2feature
负梯度：yh(x),梯度,answer2feature
初始模型F0由目标变量的平均值给出,目标,answer2feature
初始模型F0由目标变量的平均值给出,平均值,answer2feature
绝对损失:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bne7cipj303400q742.jpg),损失,answer2feature
负梯度：sign(yh(x)),梯度,answer2feature
初始模型F0由目标变量的中值给出,目标,answer2feature
Huber损失：mse和绝对损失的结合,损失,answer2feature
Huber损失：mse和绝对损失的结合,mse,answer2feature
Huber损失：mse和绝对损失的结合,损失,answer2feature
负梯度：yh(x)和sign(yh(x))分段函数,梯度,answer2feature
负梯度：yh(x)和sign(yh(x))分段函数,分段,answer2feature
负梯度：yh(x)和sign(yh(x))分段函数,函数,answer2feature
它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,MSE,answer2feature
它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,损失,answer2feature
它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,损失,answer2feature
它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,MSE,answer2feature
对数似然损失函数,损失,answer2feature
对数似然损失函数,函数,answer2feature
负梯度：y/(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥))),梯度,answer2feature
多元：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94cj1yce6j306x01iglg.jpg),多元,answer2feature
"指数损失函数:𝐿(𝑦,𝑓(𝑥))=𝑒𝑥𝑝(−𝑦𝑓(𝑥))",损失,answer2feature
"指数损失函数:𝐿(𝑦,𝑓(𝑥))=𝑒𝑥𝑝(−𝑦𝑓(𝑥))",函数,answer2feature
负梯度：y·𝑒𝑥𝑝(−𝑦𝑓(𝑥)),梯度,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,梯度,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,梯度,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,拟合,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,搜索,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,多元,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,GBDT,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,分类,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,GBDT,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,分类,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,GBDT,answer2feature
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,回归,answer2feature
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,函数,answer2feature
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,均方,answer2feature
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,误差,answer2feature
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,gbdt,answer2feature
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,梯度,answer2feature
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,boostingtree,answer2feature
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,拟合,answer2feature
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",损失,answer2feature
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",函数,answer2feature
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",梯度,answer2feature
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",回归,answer2feature
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",树,answer2feature
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),回归,answer2feature
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),树,answer2feature
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),特征,answer2feature
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),均方,answer2feature
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),误差,answer2feature
"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值𝑐𝑡𝑗,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",损失,answer2feature
"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值𝑐𝑡𝑗,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",函数,answer2feature
"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值𝑐𝑡𝑗,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",拟合,answer2feature
"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值𝑐𝑡𝑗,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",输出,answer2feature
首先，根据feature切分后的损失均方差大小，选取最优的特征切分,feature,answer2feature
首先，根据feature切分后的损失均方差大小，选取最优的特征切分,损失,answer2feature
首先，根据feature切分后的损失均方差大小，选取最优的特征切分,特征,answer2feature
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,feature,answer2feature
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,损失,answer2feature
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,函数,answer2feature
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,拟合,answer2feature
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,输出,answer2feature
本轮最终得到的强学习器的表达式如下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94binx5prj307o01k0sl.jpg),表达式,answer2feature
泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg),泰勒展开,answer2feature
泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg),一阶,answer2feature
"对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)",泰勒展开,answer2feature
"对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)",梯度,answer2feature
"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",拟合,answer2feature
"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",梯度,answer2feature
"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",损失,answer2feature
前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（𝛽）,梯度,answer2feature
前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（𝛽）,梯度,answer2feature
前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（𝛽）,步长,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,优化,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,反映,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,优化,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,回归,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,梯度,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,逼近,answer2feature
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,逼近,answer2feature
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,树,answer2feature
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,梯度,answer2feature
会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,特征,answer2feature
会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,特征,answer2feature
会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,重要性,answer2feature
每一棵树基于原始原本的一个子集进行训练,训练,answer2feature
rf是有放回采样，gbdt是无放回采样,有放回,answer2feature
rf是有放回采样，gbdt是无放回采样,gbdt,answer2feature
rf是有放回采样，gbdt是无放回采样,无放回,answer2feature
特征子采样可以来控制模型整体的方差,特征,answer2feature
利用Shrinkage收缩，控制每一棵子树的贡献度,树,answer2feature
每棵Cart树的枝剪,树,answer2feature
对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受,预处理,answer2feature
对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受,离散,answer2feature
bagging，关注于提升分类器的泛化能力,bagging,answer2feature
bagging，关注于提升分类器的泛化能力,泛化能力,answer2feature
boosting，关注于提升分类器的精度,boosting,answer2feature
数据要求比较低，不需要前提假设，能处理缺失值，连续值，离散值,离散,answer2feature
使用一些健壮的损失函数，对异常值的鲁棒性非常强,损失,answer2feature
使用一些健壮的损失函数，对异常值的鲁棒性非常强,函数,answer2feature
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,RF,answer2feature
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,迭代,answer2feature
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,训练,answer2feature
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,有放回,answer2feature
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,抽样,answer2feature
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,GBDT,answer2feature
gbdt对异常值比rf更加敏感,gbdt,answer2feature
gbdt是串行，rf是并行,gbdt,answer2feature
gbdt是cart回归树，rf是cart分类回归树都可以,gbdt,answer2feature
gbdt是cart回归树，rf是cart分类回归树都可以,回归,answer2feature
gbdt是cart回归树，rf是cart分类回归树都可以,树,answer2feature
gbdt是cart回归树，rf是cart分类回归树都可以,分类,answer2feature
gbdt是cart回归树，rf是cart分类回归树都可以,回归,answer2feature
gbdt是cart回归树，rf是cart分类回归树都可以,树,answer2feature
gbdt是提高降低偏差提高性能，rf是通过降低方差提高性能,gbdt,answer2feature
gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,gbdt,answer2feature
gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,输出,answer2feature
gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,加权,answer2feature
gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,输出,answer2feature
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,决策,answer2feature
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,线性回归,answer2feature
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,决策,answer2feature
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,逻辑,answer2feature
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,回归,answer2feature
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,决策,answer2feature
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,GBDT,answer2feature
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,决策,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,高维,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,稀疏,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,特征,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,LR,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,GBDT,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,LR,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,参数,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,GBDT,answer2feature
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,过拟合,answer2feature
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,迭代,answer2feature
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,训练,answer2feature
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,训练,answer2feature
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,训练,answer2feature
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,训练,answer2feature
预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,排序,answer2feature
预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,特征值,answer2feature
预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,特征,answer2feature
预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,排序,answer2feature
levelwise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数,训练,answer2feature
levelwise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数,增益,answer2feature
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,特征,answer2feature
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,离散,answer2feature
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,离散,answer2feature
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,特征,answer2feature
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,直方图,answer2feature
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,特征选择,answer2feature
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,直方图,answer2feature
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,离散,answer2feature
每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环,增益,answer2feature
直方图做差加速,直方图,answer2feature
直方图做差加速,加速,answer2feature
单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,梯度,answer2feature
单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,梯度,answer2feature
单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,损失,answer2feature
单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,增益,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",互斥,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",稀疏,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",互斥,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature
显示的把树模型复杂度作为正则项加到优化目标中,树,answer2feature
显示的把树模型复杂度作为正则项加到优化目标中,复杂度,answer2feature
显示的把树模型复杂度作为正则项加到优化目标中,优化,answer2feature
显示的把树模型复杂度作为正则项加到优化目标中,目标,answer2feature
优化目标计算中用到二阶泰勒展开代替一阶，更加准确,优化,answer2feature
优化目标计算中用到二阶泰勒展开代替一阶，更加准确,目标,answer2feature
优化目标计算中用到二阶泰勒展开代替一阶，更加准确,泰勒展开,answer2feature
优化目标计算中用到二阶泰勒展开代替一阶，更加准确,一阶,answer2feature
近似算法（分桶）,分桶,answer2feature
更加高效和快速,快速,answer2feature
数据事先排序并且以block形式存储，有利于并行计算,排序,answer2feature
数据事先排序并且以block形式存储，有利于并行计算,block,answer2feature
基于分布式通信框架rabit，可以运行在MPI和yarn上,分布式,answer2feature
实现做了面向体系结构的优化，针对cache和内存做了性能优化,优化,answer2feature
实现做了面向体系结构的优化，针对cache和内存做了性能优化,优化,answer2feature
模型优化上：,优化,answer2feature
基模型的优化：,优化,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),gbdt,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),回归,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),树,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),xgboost,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),L1,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),L2,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),逻辑,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),回归,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),分类,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),线性回归,answer2feature
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),回归,answer2feature
损失函数上的优化：,损失,answer2feature
损失函数上的优化：,函数,answer2feature
损失函数上的优化：,优化,answer2feature
gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,gbdt,answer2feature
gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,一阶,answer2feature
gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,xgboost,answer2feature
gbdt没有在loss中带入结点个数和预测值的正则项,gbdt,answer2feature
特征选择上的优化：,特征选择,answer2feature
特征选择上的优化：,优化,answer2feature
实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,加速,answer2feature
实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,gbdt,answer2feature
实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,搜索,answer2feature
节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权,gbdt,answer2feature
节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权,加权,answer2feature
正则化的优化：,优化,answer2feature
特征采样,特征,answer2feature
工程优化上：,优化,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,xgboost,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,特征,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,block,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,排序,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,特征,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,增益,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,特征,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,特征,answer2feature
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,增益,answer2feature
支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit,分布式,answer2feature
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒一阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mrtqxv0j30480173yc.jpg),一阶,answer2feature
MAE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mxhhvg8j303l011q2q.jpg),MAE,answer2feature
MAPE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mx7uyuej303f0170sj.jpg),MAPE,answer2feature
利用可导的函数逼近MAE或MAPE,函数,answer2feature
利用可导的函数逼近MAE或MAPE,逼近,answer2feature
利用可导的函数逼近MAE或MAPE,MAE,answer2feature
利用可导的函数逼近MAE或MAPE,MAPE,answer2feature
mse,mse,answer2feature
法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大,特征,answer2feature
法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大,特征,answer2feature
近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,特征值,answer2feature
近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,特征值,answer2feature
近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,统计,answer2feature
近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,特征,answer2feature
离散值直接分桶,离散,answer2feature
离散值直接分桶,分桶,answer2feature
连续值分位数分桶,分桶,answer2feature
训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个,训练,answer2feature
训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个,损失,answer2feature
预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,预测,answer2feature
预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,训练,answer2feature
预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,预测,answer2feature
预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,分类,answer2feature
特征预排序,特征,answer2feature
特征预排序,排序,answer2feature
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,特征,answer2feature
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,block,answer2feature
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,特征,answer2feature
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,block,answer2feature
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,特征,answer2feature
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,block,answer2feature
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,排序,answer2feature
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,block,answer2feature
block可以仅存放样本的索引，而不是样本本身，这样节省了大量的存储空间,block,answer2feature
’weight‘：代表着某个特征被选作分裂结点的次数；,特征,answer2feature
’gain‘：使用该特征作为分类结点的信息增益；,特征,answer2feature
’gain‘：使用该特征作为分类结点的信息增益；,分类,answer2feature
’gain‘：使用该特征作为分类结点的信息增益；,增益,answer2feature
’cover‘：某特征作为划分结点，覆盖样本总数的平均值；,特征,answer2feature
’cover‘：某特征作为划分结点，覆盖样本总数的平均值；,平均值,answer2feature
在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,目标,answer2feature
在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,函数,answer2feature
在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,树,answer2feature
在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,L2,answer2feature
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,阈值,answer2feature
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,目标,answer2feature
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,函数,answer2feature
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,增益,answer2feature
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,阈值,answer2feature
当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂,阈值,answer2feature
XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,XGBoost,answer2feature
XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,树,answer2feature
XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,深度,answer2feature
XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,剪枝,answer2feature
降低树的深度,树,answer2feature
降低树的深度,深度,answer2feature
减小learningrate，提高estimator,learningrate,answer2feature
减小learningrate，提高estimator,estimator,answer2feature
先确定learningrate和estimator,learningrate,answer2feature
先确定learningrate和estimator,estimator,answer2feature
再确定全局信息：比如最小分裂增益，子采样参数，正则参数,增益,answer2feature
再确定全局信息：比如最小分裂增益，子采样参数，正则参数,参数,answer2feature
再确定全局信息：比如最小分裂增益，子采样参数，正则参数,参数,answer2feature
重新降低learningrate，得到最优解,learningrate,answer2feature
对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,RNN,answer2feature
对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,RNN,answer2feature
对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,决策,answer2feature
对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,决策,answer2feature
对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序,CNN,answer2feature
对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序,CNN,answer2feature
首先，我们可以理解为Attention把input重新进行了一轮编码，获得一个新的序列,Attention,answer2feature
除以![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98734eqn7j300y00na9t.jpg)的目的是为了平衡qk的值，避免softmax之后过小,平衡,answer2feature
除以![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98734eqn7j300y00na9t.jpg)的目的是为了平衡qk的值，避免softmax之后过小,softmax,answer2feature
qk除了点击还可以直接拼接再内接一个参数变量等等,参数,answer2feature
MultiAttention只是重复了h次的Attention，最后把结果进行拼接,Attention,answer2feature
可以直接随机初始化,初始化,answer2feature
也可以参考Google的sin/cos位置初始化方法,初始化,answer2feature
Q：指的是query，相当于decoder的内容,decoder,answer2feature
K：指的是key，相当于encoder的内容,encoder,answer2feature
V：指的是value，相当于encoder的内容,encoder,answer2feature
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,解码,answer2feature
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,注意力,answer2feature
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,encoder,answer2feature
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,attention,answer2feature
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,encoder,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,seq2seq,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Encoder,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,向量,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Decoder,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,预测,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Decoder,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,token,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,损失,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Encoder,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,向量,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Decoder,answer2feature
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Decoder,answer2feature
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,目标,answer2feature
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,目标,answer2feature
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,embedding,answer2feature
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,FFN,answer2feature
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,Transformer,answer2feature
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,seq2seq,answer2feature
假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,向量,answer2feature
假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,均值,answer2feature
假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,点积,answer2feature
假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,均值,answer2feature
"针对Q和K中的每一维i都有qi和ki相互独立且均值0方差1，不妨记![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envjoy8oj301h00gdfl.jpg),![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envuw3p0j301g00ga9t.jpg)",均值,answer2feature
所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,维度,answer2feature
所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,均值,answer2feature
所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,维度,answer2feature
所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,标准化,answer2feature
按batch进行期望和标准差计算,batch,answer2feature
按batch进行期望和标准差计算,期望,answer2feature
对整体数据进行标准化,标准化,answer2feature
对标准化的数据进行线性变换,标准化,answer2feature
out=alhpa*(XXmu)/np.sqrt(var+eps)+beta,beta,answer2feature
和bn过程近似，只是作用的方向是在维度上，而不是batch上,bn,answer2feature
和bn过程近似，只是作用的方向是在维度上，而不是batch上,维度,answer2feature
和bn过程近似，只是作用的方向是在维度上，而不是batch上,batch,answer2feature
这样做的好处就是不会受到batch大小不一致的影响,batch,answer2feature
常见结构，CV里面用的比较多,CV,answer2feature
虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,求偏,answer2feature
虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,导数,answer2feature
虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,梯度,answer2feature
防止梯度消失,梯度,answer2feature
对输出的变化更敏感,输出,answer2feature
我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络,xdeepfm,answer2feature
我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络,输出,answer2feature
Bert和Transform中attention部分残差网络用的比较频繁,Bert,answer2feature
Bert和Transform中attention部分残差网络用的比较频繁,Transform,answer2feature
Bert和Transform中attention部分残差网络用的比较频繁,attention,answer2feature
mask+attention，mask的word结合全部其他encoderword的信息,mask,answer2feature
mask+attention，mask的word结合全部其他encoderword的信息,attention,answer2feature
mask+attention，mask的word结合全部其他encoderword的信息,mask,answer2feature
mask+attention，mask的word结合全部其他encoderword的信息,word,answer2feature
MLM：将完整句子中的部分字mask，预测该mask词,mask,answer2feature
MLM：将完整句子中的部分字mask，预测该mask词,预测,answer2feature
MLM：将完整句子中的部分字mask，预测该mask词,mask,answer2feature
NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,训练,answer2feature
NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,语料库,answer2feature
NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,预测,answer2feature
mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,mask,answer2feature
mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,mask,answer2feature
mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,mask,answer2feature
随机替换也帮助训练修正了\[unused]和\[UNK],训练,answer2feature
input_id是语义表达，和传统的w2v一样，方法也一样的lookup,w2v,answer2feature
input_id是语义表达，和传统的w2v一样，方法也一样的lookup,lookup,answer2feature
"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",BERT,answer2feature
"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",向量,answer2feature
"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",lookup,answer2feature
"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",有序,answer2feature
"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",词袋模型,answer2feature
"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",lookup,answer2feature
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",encoder,answer2feature
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",输出,answer2feature
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",分类,answer2feature
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",输出,answer2feature
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",向量,answer2feature
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",维度,answer2feature
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",softmax,answer2feature
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",mask,answer2feature
"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",分类,answer2feature
"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",输出,answer2feature
"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",向量,answer2feature
"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",softmax,answer2feature
MLM+NSP即为最后的损失,损失,answer2feature
"tf.multal(tf.nn.softmax(tf.multiply(tf.multal(q,k,transpose_b=True),1/math.sqrt(float(size_per_head)))),v)",softmax,answer2feature
headonly：保存前510个token（留两个位置给\[CLS]和\[SEP]）,token,answer2feature
tailonly：保存最后510个token,token,answer2feature
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,token,answer2feature
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,token,answer2feature
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,token,answer2feature
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,token,answer2feature
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,tokens,answer2feature
"首先定义处理好输入的tokens的对应的id作为input_id,因为不是训练所以input_mask和segment_id都是采取默认的1即可",tokens,answer2feature
"首先定义处理好输入的tokens的对应的id作为input_id,因为不是训练所以input_mask和segment_id都是采取默认的1即可",训练,answer2feature
进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,Transform,answer2feature
进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,transformer,answer2feature
进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,隐藏层,answer2feature
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],输出,answer2feature
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],向量,answer2feature
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],向量,answer2feature
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],输出,answer2feature
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],向量,answer2feature
BasicTokenizer：根据空格等进行普通的分词,BasicTokenizer,answer2feature
BasicTokenizer：根据空格等进行普通的分词,分词,answer2feature
包括了一些预处理的方法：去除无意义词，跳过'\t'这些词，unicode变换，中文字符筛选等等,预处理,answer2feature
包括了一些预处理的方法：去除无意义词，跳过'\t'这些词，unicode变换，中文字符筛选等等,筛选,answer2feature
WordpieceTokenizer：前者的结果再细粒度的切分为WordPiece,WordpieceTokenizer,answer2feature
WordpieceTokenizer：前者的结果再细粒度的切分为WordPiece,WordPiece,answer2feature
get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,token,answer2feature
get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,输出,answer2feature
get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,向量,answer2feature
Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,Transform,answer2feature
Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,dense,answer2feature
Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,dropout,answer2feature
所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output,dense,answer2feature
所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output,dropout,answer2feature
残差结构能够很好的消除层数加深所带来的信息损失问题,损失,answer2feature
在线预测只能一条一条的入参，实际上在可承受的计算量内batch越大整体的计算性能性价比越高,预测,answer2feature
在线预测只能一条一条的入参，实际上在可承受的计算量内batch越大整体的计算性能性价比越高,batch,answer2feature
mask机制,mask,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,elmo,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,LSTM,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,GPT,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,bert,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,Transformer,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,Transformer,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,LSTM,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,elmo,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,静态,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,向量,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,LSTM,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,GPT,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,bert,answer2feature
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,Transformer,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,GPT,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,elmo,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,bert,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,elmo,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,融合,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,特征,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,bert,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,融合,answer2feature
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,特征,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,GPT,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,bert,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,Transformer,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,Transformer,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,GPT,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,decoder,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,decoder,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,bert,answer2feature
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,encoder,answer2feature
则条件概率分布P(Y|X)为条件随机场CRF,条件随机场,answer2feature
则条件概率分布P(Y|X)为条件随机场CRF,CRF,answer2feature
"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",CRF,answer2feature
"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",判别,answer2feature
"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",HMM,answer2feature
CRF是无向图，HMM是有向图,CRF,answer2feature
CRF是无向图，HMM是有向图,HMM,answer2feature
CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,CRF,answer2feature
CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,输出,answer2feature
CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,HMM,answer2feature
CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,统计,answer2feature
Bert把中文文本进行了embedding，得到每个字的表征向量,Bert,answer2feature
Bert把中文文本进行了embedding，得到每个字的表征向量,embedding,answer2feature
Bert把中文文本进行了embedding，得到每个字的表征向量,向量,answer2feature
dense操作得到了每个文本文本对应的未归一化的tag概率,dense,answer2feature
CRF在选择每个词的tag的过程其实就是一个最优Tag路径的选择过程,CRF,answer2feature
CRF层能从训练数据中获得约束性的规则,CRF,answer2feature
CRF层能从训练数据中获得约束性的规则,训练,answer2feature
"其实，一句话解释就是想构造一个向量表征方式，使得向量的点击和共现矩阵中的对应关系一致。因为共现矩阵中的对应关系证明了，存在i，k，j三个不同的文本，如果i和k相关，j和k相关，那么p(i,j)=p(j,k)近似于1，其他情况都过大和过小。",向量,answer2feature
"其实，一句话解释就是想构造一个向量表征方式，使得向量的点击和共现矩阵中的对应关系一致。因为共现矩阵中的对应关系证明了，存在i，k，j三个不同的文本，如果i和k相关，j和k相关，那么p(i,j)=p(j,k)近似于1，其他情况都过大和过小。",向量,answer2feature
公现矩阵，NXN的，N为词袋量,词袋,answer2feature
W2V的工程实现结果相对来说支持的更多，比如most_similarty等功能,W2V,answer2feature
"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",词性,answer2feature
"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",训练,answer2feature
"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",词性,answer2feature
"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",向量,answer2feature
从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg),狄利克雷,answer2feature
从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg),主题,answer2feature
多项式分布的共轭分布是狄利克雷分布,多项式,answer2feature
多项式分布的共轭分布是狄利克雷分布,狄利克雷,answer2feature
二项式分布的共轭分布是Beta分布,二项式,answer2feature
从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),主题,answer2feature
从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),多项式,answer2feature
从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),主题,answer2feature
从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg),狄利克雷,answer2feature
从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg),主题,answer2feature
从词语的多项式分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)中采样最终生成词语![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8uydisuj300n00e0s0.jpg),多项式,answer2feature
采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,EM,answer2feature
采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,主题,answer2feature
采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,主题,answer2feature
采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,收敛,answer2feature
这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点,LDA,answer2feature
这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点,nlp,answer2feature
	先随机给每个词附上主题,主题,answer2feature
	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,多项式,answer2feature
	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,狄利克雷,answer2feature
	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,狄利克雷,answer2feature
	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,主题,answer2feature
	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,多项式,answer2feature
	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,狄利克雷,answer2feature
	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,主题,answer2feature
	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,主题,answer2feature
	有了联合概率分布，去除词wi后，就可以得到其他词主题条件概率分布,主题,answer2feature
	根据条件概率分布使用坐标轮换的吉布斯采样方法，得到词对应的平稳矩阵及词对应的主题,主题,answer2feature
	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,收敛,answer2feature
	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,统计,answer2feature
	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,主题,answer2feature
	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,主题,answer2feature
	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,统计,answer2feature
	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,主题,answer2feature
	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,主题,answer2feature
	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）,特征,answer2feature
	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）,优化,answer2feature
	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）,特征,answer2feature
	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）,维度,answer2feature
	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）,加快,answer2feature
	MCMC中什么叫做蒙特卡洛方法？,蒙特卡洛,answer2feature
	马尔科夫链收敛性质？,收敛,answer2feature
		连通性，不能有断点,连通性,answer2feature
		连通性，不能有断点,断点,answer2feature
		先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵,迭代,answer2feature
		先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵,收敛,answer2feature
"			给定任意的转移矩阵Q，已知π(i)p(i,j)=π(j)p(j,i)，近似拟合π(i)Q(i,j)a(i,j)=π(j)Q(j,i)a(j,i)",拟合,answer2feature
"			左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度",加快,answer2feature
"			左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度",收敛,answer2feature
		Gibbs采样,Gibbs,answer2feature
			同上，差别在固定n−1个特征在某一个特征采样及坐标轮换采样,差别,answer2feature
			同上，差别在固定n−1个特征在某一个特征采样及坐标轮换采样,特征,answer2feature
			同上，差别在固定n−1个特征在某一个特征采样及坐标轮换采样,特征,answer2feature
		其为一对样本，有点像Lasso回归中的固定n1维特征求一维特征求极值的思路,回归,answer2feature
		其为一对样本，有点像Lasso回归中的固定n1维特征求一维特征求极值的思路,特征,answer2feature
变分推断EM算法,EM,answer2feature
	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,LDA,answer2feature
	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,主题,answer2feature
	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,主题,answer2feature
	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,beta,answer2feature
	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,期望,answer2feature
	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,期望,answer2feature
	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,beta,answer2feature
	变分推断在于隐藏变量没法直接求，**用三个独立分布的变分分步去拟合三个隐藏变量的条件分布**,拟合,answer2feature
		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵,kl,answer2feature
		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵,散度,answer2feature
		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵,最小化,answer2feature
		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵,KL,answer2feature
		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵,散度,answer2feature
		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵,熵,answer2feature
	EM过程,EM,answer2feature
		E：最小化相对熵，偏导为0得到变分参数,最小化,answer2feature
		E：最小化相对熵，偏导为0得到变分参数,熵,answer2feature
		E：最小化相对熵，偏导为0得到变分参数,偏导,answer2feature
		E：最小化相对熵，偏导为0得到变分参数,参数,answer2feature
		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值,参数,answer2feature
		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值,梯度,answer2feature
		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值,beta,answer2feature
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,多项式,answer2feature
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,狄利克雷,answer2feature
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,多项式,answer2feature
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,多项式,answer2feature
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,狄利克雷,answer2feature
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,狄利克雷,answer2feature
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,LDA,answer2feature
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,多项式,answer2feature
LDA是加了狄利克雷先验的PLSA,LDA,answer2feature
LDA是加了狄利克雷先验的PLSA,狄利克雷,answer2feature
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,EM,answer2feature
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,估计,answer2feature
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,LDA,answer2feature
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,狄利克雷,answer2feature
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,多项式,answer2feature
LDA是贝叶斯思想，PLSA是MLE,LDA,answer2feature
LDA是贝叶斯思想，PLSA是MLE,MLE,answer2feature
困惑度越小，越容易过拟合,过拟合,answer2feature
某个词属于某个主题的困惑度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b7zjns8uj305i012jr7.jpg)，某个文章的困惑度即为词的连乘：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b83z3d22j304q01dweb.jpg),主题,answer2feature
LDA比较是doc，word2vec是词,LDA,answer2feature
LDA比较是doc，word2vec是词,word2vec,answer2feature
LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,LDA,answer2feature
LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,主题,answer2feature
LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,Word2Vec,answer2feature
LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,特征,answer2feature
LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,LDA,answer2feature
LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,主题,answer2feature
LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,Word2Vec,answer2feature
LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,LDA,answer2feature
LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,word,answer2feature
LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,Word2Vec,answer2feature
通常alpha为1/k，k为类别数，beta一般为0.01,beta,answer2feature
alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,主题,answer2feature
alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,主题,answer2feature
alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,主题,answer2feature
beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,beta,answer2feature
beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,beta,answer2feature
beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,压缩,answer2feature
chucksize大一些更新的过程比较平稳，收敛更加平稳,收敛,answer2feature
迭代次数一般不超过2000次，200万doc大约在2300次收敛,迭代,answer2feature
迭代次数一般不超过2000次，200万doc大约在2300次收敛,收敛,answer2feature
层次softmax,softmax,answer2feature
负采样,负采样,answer2feature
最大化对数似然函数,函数,answer2feature
输入层：是上下文的词语的词向量,向量,answer2feature
投影层：对其求和，所谓求和，就是简单的向量加法,向量,answer2feature
输出层：输出最可能的word,输出,answer2feature
输出层：输出最可能的word,输出,answer2feature
输出层：输出最可能的word,word,answer2feature
沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,树,answer2feature
沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,logistics,answer2feature
沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,函数,answer2feature
对每层每个变量求偏导，参考sgd,求偏,answer2feature
对每层每个变量求偏导，参考sgd,sgd,answer2feature
统计每个词出现对概率，丢弃词频过低对词,统计,answer2feature
统计每个词出现对概率，丢弃词频过低对词,词频,answer2feature
每次选择softmax的负样本的时候，从丢弃之后的词库里选择（选择是需要参考出现概率的）,softmax,answer2feature
负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,负采样,answer2feature
负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,负采样,answer2feature
负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,输出,answer2feature
负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,输出,answer2feature
**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,Mikolov,answer2feature
**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,CBOW,answer2feature
**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,高频,answer2feature
"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",复杂度,answer2feature
"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",CBOW,answer2feature
"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",复杂度,answer2feature
从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,item2vec,answer2feature
从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,向量,answer2feature
从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,判别,answer2feature
从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,维度,answer2feature
对item2vec得到的词向量进行聚类或者可视化,item2vec,answer2feature
对item2vec得到的词向量进行聚类或者可视化,向量,answer2feature
对item2vec得到的词向量进行聚类或者可视化,聚类,answer2feature
word2vec是基于邻近词共现，glove是基于全文共现,word2vec,answer2feature
word2vec是基于邻近词共现，glove是基于全文共现,glove,answer2feature
word2vec利用了负采样或者层次softmax加速，相对更快,word2vec,answer2feature
word2vec利用了负采样或者层次softmax加速，相对更快,负采样,answer2feature
word2vec利用了负采样或者层次softmax加速，相对更快,softmax,answer2feature
word2vec利用了负采样或者层次softmax加速，相对更快,加速,answer2feature
glove用了全局共现矩阵，更占内存资源,glove,answer2feature
word2vec是“predictive”的模型，而GloVe是“countbased”的模型,word2vec,answer2feature
对于中文依赖分词结果的好坏,分词,answer2feature
基础概念,AutoML,root2first
AutoML,AutoML问题构成?,second2question
AutoML,特征工程选择思路？,second2question
AutoML,模型相关的选择思路?,second2question
AutoML,常见优化算法思路？,second2question
AutoML,AutoML参数选择所使用的方法？,second2question
AutoML,讲讲贝叶斯优化如何在automl上应用？,second2question
AutoML,以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,second2question
基础概念,先验概率和后验概率,root2first
先验概率和后验概率,写出全概率公式&贝叶斯公式,second2question
先验概率和后验概率,说说你怎么理解为什么有全概率公式&贝叶斯公式,second2question
先验概率和后验概率,什么是先验概率,second2question
先验概率和后验概率,什么是后验概率,second2question
先验概率和后验概率,经典概率题,second2question
基础概念,方差与偏差,root2first
方差与偏差,解释方差：,second2question
方差与偏差,解释偏差：,second2question
方差与偏差,模型训练为什么要引入偏差和方差？请理论论证。,second2question
方差与偏差,什么情况下引发高方差？,second2question
方差与偏差,如何解决高方差问题？,second2question
方差与偏差,以上方法是否一定有效？,second2question
方差与偏差,如何解决高偏差问题？,second2question
方差与偏差,以上方法是否一定有效？,second2question
方差与偏差,遇到过的机器学习中的偏差与方差问题？,second2question
方差与偏差,就理论角度论证Bagging、Boosting的方差偏差问题,second2question
方差与偏差,遇到过的深度学习中的偏差与方差问题？,second2question
方差与偏差,方差、偏差与模型的复杂度之间的关系？,second2question
基础概念,生成与判别模型,root2first
生成与判别模型,什么叫生成模型？,second2question
生成与判别模型,什么叫判别模型？,second2question
生成与判别模型,什么时候会选择生成/判别模型？,second2question
生成与判别模型,CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,second2question
生成与判别模型,我的理解：,second2question
基础概念,频率概率,root2first
频率概率,极大似然估计 - MLE,second2question
频率概率,最大后验估计 - MAP,second2question
频率概率,极大似然估计与最大后验概率的区别？,second2question
频率概率,到底什么是似然什么是概率估计？,second2question
推荐,DeepFM,root2first
DeepFM,DNN与DeepFM之间的区别?,second2question
DeepFM,Wide&Deep与DeepFM之间的区别?,second2question
DeepFM,你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,second2question
DeepFM,DeepFM怎么优化的？,second2question
DeepFM,不定长文本数据如何输入deepFM？,second2question
DeepFM,deepfm的embedding初始化有什么值得注意的地方吗？,second2question
推荐,DIN,root2first
DIN,主要使用了什么机制?,second2question
DIN,activation unit的作用,second2question
DIN,DICE怎么设计的,second2question
DIN,DICE使用的过程中，有什么需要注意的地方,second2question
推荐,XDeepFM,root2first
XDeepFM,选用的原因？,second2question
XDeepFM,什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,second2question
XDeepFM,简单介绍一下XDeepFm的思想？,second2question
XDeepFM,和DCN比，有哪些核心的变化？,second2question
XDeepFM,时间复杂度多少？,second2question
推荐,YouTubeNet,root2first
YouTubeNet,变长数据如何处理的？,second2question
YouTubeNet,input是怎么构造的,second2question
YouTubeNet,最后一次点击实际如何处理的？,second2question
YouTubeNet,output的是时候train和predict如何处理的,second2question
YouTubeNet,如何进行负采样的？,second2question
YouTubeNet,item向量在softmax的时候你们怎么选择的？,second2question
YouTubeNet,Example Age的理解？,second2question
YouTubeNet,什么叫做不对称的共同浏览（asymmetric co-watch）问题？,second2question
YouTubeNet,为什么不采取类似RNN的Sequence model？,second2question
YouTubeNet,YouTube如何避免百万量级的softmax问题的？,second2question
YouTubeNet,serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,second2question
YouTubeNet,Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,second2question
YouTubeNet,在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,second2question
YouTubeNet,整个过程中有什么亮点？有哪些决定性的提升？,second2question
数学,gcd,root2first
gcd,辗转相除法,second2question
gcd,其他方法,second2question
数学,导数,root2first
导数,四则运算,second2question
导数,常见导数,second2question
导数,复合函数的运算法则,second2question
导数,莱布尼兹公式,second2question
数学,平面曲线的切线和法线,root2first
平面曲线的切线和法线,切线方程,second2question
平面曲线的切线和法线,法线方程,second2question
数学,微分中值定理,root2first
微分中值定理,费马定理,second2question
微分中值定理,拉格朗日中值定理,second2question
微分中值定理,柯西中值定理,second2question
数学,期望、方差、标准差和协方差,root2first
期望、方差、标准差和协方差,期望,second2question
期望、方差、标准差和协方差,方差,second2question
期望、方差、标准差和协方差,标准差,second2question
期望、方差、标准差和协方差,协方差,second2question
期望、方差、标准差和协方差,相关系数,second2question
数学,概率密度分布,root2first
概率密度分布,均匀分布,second2question
概率密度分布,伯努利分布,second2question
概率密度分布,二项分布,second2question
概率密度分布,高斯分布,second2question
概率密度分布,拉普拉斯分布,second2question
概率密度分布,泊松分布,second2question
数学,概率论,root2first
概率论,条件概率,second2question
概率论,独立,second2question
概率论,概率基础公式,second2question
概率论,全概率：,second2question
概率论,贝叶斯,second2question
概率论,切比雪夫不等式,second2question
概率论,抽球,second2question
概率论,纸牌问题,second2question
概率论,棍子/绳子问题,second2question
概率论,贝叶斯,second2question
概率论,选择时间问题,second2question
概率论,0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,second2question
概率论,抽红蓝球球,second2question
数学,欧拉公式,root2first
数学,泰勒公式,root2first
泰勒公式,泰勒公式,second2question
泰勒公式,常见泰勒公式,second2question
数学,牛顿法,root2first
牛顿法,迭代公式推导,second2question
牛顿法,实现它,second2question
数学,矩阵,root2first
矩阵,范数,second2question
矩阵,特征值分解，特征向量,second2question
矩阵,正定性,second2question
数据预处理,异常点识别,root2first
异常点识别,统计方法,second2question
异常点识别,矩阵分解方法,second2question
异常点识别,特征值和特征向量的本质是什么？,second2question
异常点识别,矩阵乘法的实际意义？,second2question
异常点识别,密度的离群点检测,second2question
异常点识别,聚类的离群点检测,second2question
异常点识别,如何处理异常点？,second2question
数据预处理,数据平衡,root2first
数据平衡,为什么要对数据进行采样平衡,second2question
数据平衡,是否一定需要对原始数据进行采样平衡,second2question
数据平衡,有哪些常见的采样方法？,second2question
数据平衡,能否避免采样？,second2question
数据平衡,你平时怎么用采样方法？,second2question
数据预处理,特征提取,root2first
特征提取,为什么需要对数据进行变换？,second2question
特征提取,归一化和标准化之间的关系？,second2question
特征提取,连续特征常用方法,second2question
特征提取,离散特征常用方法,second2question
特征提取,文本特征,second2question
特征提取,画一个最简单的最快速能实现的框架,second2question
数据预处理,特征选择,root2first
特征选择,为什么要做特征选择？,second2question
特征选择,从哪些方面可以做特征选择？,second2question
特征选择,既然说了两个方向，分别介绍一些吧,second2question
数据预处理,缺失值处理,root2first
缺失值处理,是不是一定需要对缺失值处理？,second2question
缺失值处理,直接填充方法有哪些？,second2question
缺失值处理,模型插值方法有哪些？及方法的问题,second2question
缺失值处理,如何直接离散化？,second2question
缺失值处理,hold位填充方法有哪些？,second2question
缺失值处理,怎么理解分布补全？,second2question
缺失值处理,random方法,second2question
缺失值处理,总结,second2question
机器学习,决策树,root2first
决策树,常见决策树,second2question
决策树,简述决策树构建过程,second2question
决策树,详述信息熵计算方法及存在问题,second2question
决策树,详述信息增益计算方法,second2question
决策树,详述信息增益率计算方法,second2question
决策树,解释Gini系数,second2question
决策树,ID3存在的问题,second2question
决策树,C4.5相对于ID3的改进点,second2question
决策树,CART的连续特征改进点,second2question
决策树,CART分类树建立算法的具体流程,second2question
决策树,CART回归树建立算法的具体流程,second2question
决策树,CART输出结果的逻辑？,second2question
决策树,CART树算法的剪枝过程是怎么样的？,second2question
决策树,树形结构为何不需要归一化？,second2question
决策树,决策树的优缺点,second2question
机器学习,支持向量机,root2first
支持向量机,简单介绍SVM?,second2question
支持向量机,什么叫最优超平面？,second2question
支持向量机,什么是支持向量？,second2question
支持向量机,SVM 和全部数据有关还是和局部数据有关?,second2question
支持向量机,加大训练数据量一定能提高SVM准确率吗？,second2question
支持向量机,如何解决多分类问题？,second2question
支持向量机,可以做回归吗，怎么做？,second2question
支持向量机,SVM 能解决哪些问题？,second2question
支持向量机,介绍一下你知道的不同的SVM分类器？,second2question
支持向量机,什么叫软间隔？,second2question
支持向量机,SVM 软间隔与硬间隔表达式,second2question
支持向量机,SVM原问题和对偶问题的关系/解释原问题和对偶问题？,second2question
支持向量机,为什么要把原问题转换为对偶问题？,second2question
支持向量机,为什么求解对偶问题更加高效？,second2question
支持向量机,alpha系数有多少个？,second2question
支持向量机,KKT限制条件，KKT条件有哪些，完整描述,second2question
支持向量机,引入拉格朗日的优化方法后的损失函数解释,second2question
支持向量机,核函数的作用是啥,second2question
支持向量机,核函数的种类和应用场景,second2question
支持向量机,如何选择核函数,second2question
支持向量机,常用核函数的定义？,second2question
支持向量机,核函数需要满足什么条件？,second2question
支持向量机,为什么在数据量大的情况下常常用lr代替核SVM？,second2question
支持向量机,高斯核可以升到多少维？为什么,second2question
支持向量机,SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,second2question
支持向量机,"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",second2question
支持向量机,Linear SVM 和 LR 有什么异同？,second2question
机器学习,线性回归,root2first
线性回归,损失函数是啥,second2question
线性回归,最小二乘/梯度下降手推,second2question
线性回归,介绍一下岭回归,second2question
线性回归,什么时候使用岭回归？,second2question
线性回归,什么时候用Lasso回归？,second2question
机器学习,聚类,root2first
聚类,请问从EM角度理解kmeans?,second2question
聚类,为什么kmeans一定会收敛?,second2question
聚类,kmeans初始点除了随机选取之外的方法？,second2question
机器学习,贝叶斯,root2first
贝叶斯,解释一下朴素贝叶斯中考虑到的条件独立假设,second2question
贝叶斯,讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,second2question
贝叶斯,朴素贝叶斯中出现的常见模型有哪些,second2question
贝叶斯,出现估计概率值为 0 怎么处理,second2question
贝叶斯,朴素贝叶斯的优缺点？,second2question
贝叶斯,朴素贝叶斯与 LR 区别？,second2question
机器学习,逻辑回归,root2first
逻辑回归,logistic分布函数和密度函数，手绘大概的图像,second2question
逻辑回归,LR推导，基础5连问,second2question
逻辑回归,梯度下降如何并行化？,second2question
逻辑回归,LR明明是分类模型为什么叫回归？,second2question
逻辑回归,为什么LR可以用来做CTR预估？,second2question
逻辑回归,满足什么样条件的数据用LR最好？,second2question
逻辑回归,LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,second2question
逻辑回归,利用几率odds的意义在哪？,second2question
逻辑回归,Sigmoid函数到底起了什么作用？,second2question
逻辑回归,LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,second2question
逻辑回归,LR中若标签为+1和-1，损失函数如何推导？,second2question
逻辑回归,如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,second2question
逻辑回归,LR可以用核么？可以怎么用？,second2question
逻辑回归,LR中的L1/L2正则项是啥？,second2question
逻辑回归,lr加l1还是l2好？,second2question
逻辑回归,正则化是依据什么理论实现模型优化？,second2question
逻辑回归,LR可以用来处理非线性问题么？,second2question
逻辑回归,为什么LR需要归一化或者取对数?,second2question
逻辑回归,为什么LR把特征离散化后效果更好？离散化的好处有哪些？,second2question
逻辑回归,逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,second2question
逻辑回归,LR对比万物？,second2question
逻辑回归,LR梯度下降方法？,second2question
逻辑回归,LR的优缺点？,second2question
逻辑回归,除了做分类，你还会用LR做什么？,second2question
逻辑回归,你有用过sklearn中的lr么？你用的是哪个包？,second2question
逻辑回归,看过源码么？为什么去看？,second2question
逻辑回归,谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,second2question
逻辑回归,谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,second2question
逻辑回归,我的总结,second2question
机器学习,随机森林,root2first
随机森林,解释下随机森林?,second2question
随机森林,随机森林用的是什么树？,second2question
随机森林,随机森林的生成过程？,second2question
随机森林,解释下随机森林节点的分裂策略？,second2question
随机森林,随机森林的损失函数是什么？,second2question
随机森林,为了防止随机森林过拟合可以怎么做?,second2question
随机森林,随机森林特征选择的过程？,second2question
随机森林,是否用过随机森林，有什么技巧?,second2question
随机森林,RF的参数有哪些，如何调参？,second2question
随机森林,RF的优缺点 ？,second2question
机器学习,集成学习,root2first
集成学习,GBDT,first2second
GBDT,介绍一下Boosting的思想？,second2question
GBDT,最小二乘回归树的切分过程是怎么样的？,second2question
GBDT,有哪些直接利用了Boosting思想的树模型？,second2question
GBDT,gbdt和boostingtree的boosting分别体现在哪里？,second2question
GBDT,gbdt的中的tree是什么tree？有什么特征？,second2question
GBDT,常用回归问题的损失函数？,second2question
GBDT,常用分类问题的损失函数？,second2question
GBDT,什么是gbdt中的损失函数的负梯度？,second2question
GBDT,如何用损失函数的负梯度实现gbdt？,second2question
GBDT,拟合损失函数的负梯度为什么是可行的？,second2question
GBDT,即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,second2question
GBDT,Shrinkage收缩的作用？,second2question
GBDT,feature属性会被重复多次使用么？,second2question
GBDT,gbdt如何进行正则化的？,second2question
GBDT,为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,second2question
GBDT,gbdt的优缺点？,second2question
GBDT,gbdt和randomforest区别？,second2question
GBDT,GBDT和LR的差异？,second2question
集成学习,LightGBM,first2second
LightGBM,XGboost缺点,second2question
LightGBM,LightGBM对Xgboost的优化,second2question
LightGBM,LightGBM亮点,second2question
集成学习,Xgboost,first2second
Xgboost,xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,second2question
Xgboost,xgboost和gbdt的区别？,second2question
Xgboost,xgboost优化目标/损失函数改变成什么样？,second2question
Xgboost,xgboost如何使用MAE或MAPE作为目标函数？,second2question
Xgboost,xgboost如何寻找分裂节点的候选集？,second2question
Xgboost,xgboost如何处理缺失值？,second2question
Xgboost,xgboost在计算速度上有了哪些点上提升？,second2question
Xgboost,xgboost特征重要性是如何得到的？,second2question
Xgboost,XGBoost中如何对树进行剪枝？,second2question
Xgboost,XGBoost模型如果过拟合了怎么解决？,second2question
Xgboost,xgboost如何调参数？,second2question
深度学习,Attention,root2first
Attention,Attention对比RNN和CNN，分别有哪点你觉得的优势？,second2question
Attention,写出Attention的公式？,second2question
Attention,解释你怎么理解Attention的公式的？,second2question
Attention,Attention模型怎么避免词袋模型的顺序问题的困境的？,second2question
Attention,"Attention机制，里面的q,k,v分别代表什么？",second2question
Attention,为什么self-attention可以替代seq2seq？,second2question
Attention,维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,second2question
深度学习,batch_normalization,root2first
batch_normalization,你觉得bn过程是什么样的？,second2question
batch_normalization,手写一下bn过程？,second2question
batch_normalization,知道LN么？讲讲原理,second2question
深度学习,残差网络,root2first
残差网络,介绍残差网络,second2question
残差网络,残差网络为什么能解决梯度消失的问题,second2question
残差网络,残差网络残差作用,second2question
残差网络,你平时有用过么？或者你在哪些地方遇到了,second2question
自然语言处理,Bert,root2first
Bert,Bert的双向体现在什么地方？,second2question
Bert,Bert的是怎样实现mask构造的？,second2question
Bert,在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,second2question
Bert,为什么BERT有3个嵌入层，它们都是如何实现的？,second2question
Bert,bert的损失函数？,second2question
Bert,手写一个multi-head attention？,second2question
Bert,长文本预测如何构造Tokens？,second2question
Bert,你用过什么模块？bert流程是怎么样的？,second2question
Bert,知道分词模块：FullTokenizer做了哪些事情么？,second2question
Bert,Bert中如何获得词意和句意？,second2question
Bert,源码中Attention后实际的流程是如何的？,second2question
Bert,为什么要在Attention后使用残差结构？,second2question
Bert,平时用官方Bert包么？耗时怎么样？,second2question
Bert,你觉得BERT比普通LM的新颖点？,second2question
Bert,elmo、GPT、bert三者之间有什么区别？,second2question
自然语言处理,CRF,root2first
CRF,阐述CRF原理？,second2question
CRF,线性链条件随机场的公式是？,second2question
CRF,CRF与HMM区别?,second2question
CRF,Bert+crf中的各部分作用详解？,second2question
自然语言处理,GloVe,root2first
GloVe,GolVe的损失函数？,second2question
GloVe,解释GolVe的损失函数？,second2question
GloVe,为什么GolVe会用的相对比W2V少？,second2question
GloVe,如何处理未出现词？,second2question
自然语言处理,LDA,root2first
LDA,详述LDA原理？,second2question
LDA,LDA中的主题矩阵如何计算?词分布矩阵如何计算？,second2question
LDA,LDA的共轭分布解释下?,second2question
LDA,PLSA和LDA的区别?,second2question
LDA,怎么确定LDA的topic个数,second2question
LDA,LDA和Word2Vec区别？LDA和Doc2Vec区别？,second2question
LDA,LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,second2question
自然语言处理,Word2Vec,root2first
Word2Vec,从隐藏层到输出的Softmax层的计算有哪些方法？,second2question
Word2Vec,层次softmax流程？,second2question
Word2Vec,负采样流程？,second2question
Word2Vec,word2vec两种方法各自的优势?,second2question
Word2Vec,怎么衡量学到的embedding的好坏?,second2question
Word2Vec,word2vec和glove区别？,second2question
Word2Vec,你觉得word2vec有哪些问题？,second2question
