:ID,name,:LABEL
-760795198447686328,AutoML问题构成?,question
-2103435428496205022,特征选择,answer
4041928090793981474,模型选择,answer
7658352582883739683,算法选择,answer
-2075868268917888440,特征工程选择思路？,question
913846038743650802,有监督的特征选择,answer
-763630344506983242,基于模型，lr的系数，树模型的importance等等,answer
7860774281964427877,基于选择，前项后项选择,answer
-3771065287648917614,无监督的特征选择,answer
-7191526152840636190,基于统计信息的，熵、相关性、KL系数,answer
6418982922889454147,基于方差，因子分解，PCA主成分分享，方差系数,answer
4901152847416684831,模型相关的选择思路?,question
4963933650214818288,各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,answer
-9067626527982856525,参数选择,answer
-577076147715421701,grid_search,answer
2773173942682946348,random_search,answer
1245082203728238683,...,answer
-6920708857250016975,常见优化算法思路？,question
8640362550470026186,SGD,answer
-4057685107823617437,GD,answer
-789812526419960589,LBFGS,answer
3975766294592884533,FTRL,answer
4631031604782500179,AutoML参数选择所使用的方法？,question
-2139967496852381041,暴力搜索,answer
5898354269198050453,拟合搜索,answer
-8248763977408450079,贝叶斯优化,answer
6940232707766487251,其他方法,answer
-5067127709965480728,Meta学习,answer
-6926725997540355936,转移学习,answer
-4885738194256334899,讲讲贝叶斯优化如何在automl上应用？,question
-4792242359058403464,目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,answer
-8542547077723901709,步骤：,answer
-3271280540798858277,随机选取几个超参数进行f拟合，得到先验数据集合D,answer
3709391548306387043,根据先验数据D得到模型M,answer
-4121662003414715997,根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,answer
8759183425868713579,循环23两步直至达到条件,answer
2527977387435122326,问题：,answer
19230187226653455,稳定性：同一组超参数的预测结果在不同轮次不一致,answer
523414173424813563,f函数需要多次计算，资源耗费时间损失,answer
-7757263892166995469,难以确定比较通用的拟合模型f,answer
2445049760795710498,手记：,answer
2345562186161423224,![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9n45r6f3nj30q40tkjz8.jpg),answer
3878977373958787710,以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,question
-7060626487502033747,随机生成若干超参点，更新gp模型,answer
-2589081292951083084,根据gp模型选取最优推荐值,answer
-1382548492277825421,推荐值附近随机生成点，根据cquisitionfunction选取附近点极值点，acquisitionfunction通常：,answer
-6756416680312170743,基于GPUCB的最大置信上界,answer
6371517177325308206,基于均值和方差的平衡结果,answer
7830901375912220150,ThompsonSampling,answer
4403946144715351439,EI(期望提升),answer
-1642043111504719937,重复以上步骤,answer
-8161701324740748635,写出全概率公式&贝叶斯公式,question
-5316532734163977495,全概率公式：设事件![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wed60nzaj305i01cmx2.jpg)构成一个完备事件组，即它们两两不相容，和为全集且![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wedhjqtej304w01cjra.jpg)，则对任一事件A有：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8weetxaqxj30dk01e74b.jpg),answer
2550358841723109107,贝叶斯公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wefkh3r5j30l203iq3c.jpg),answer
-893511198770585034,说说你怎么理解为什么有全概率公式&贝叶斯公式,question
-5013530208652397274,全概率公式为全概率就是表示达到某个目的，有多种方式，算到达目的的概率。**key：算概率**,answer
2017368835292621579,贝叶斯公式为当给定条件发生变化后，会导致事件发生的可能性发生何种变化。**key：概率变化**,answer
-3211970729549838174,什么是先验概率,question
1768280824663429715,先验概率（priorprobability）：指根据以往经验和分析。在实验或采样前就可以得到的概率。key:简单的暴力统计,answer
-2325716000997563512,什么是后验概率,question
-4857441579708849800,后验概率（posteriorprobability）：指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。key：条件概率,answer
5124875688811623836,经典概率题,question
-7843950277069824291,有一个木桶，里面有M个白球，小明每分钟从桶中随机取出一个球涂成红色（无论白或红都涂红）再放回，问小明将桶中球全部涂红的期望时间是多少？,answer
-1921336595448745491,P[i]代表M个球中已经有i个球是红色后，还需要的时间期望，去将所有球都变成红色。,answer
22540938199230777,P[i]=(i/M)*P[i]+(1i/M)*P[i+1]+1,answer
-3363636048235361005,解释一下，每一次抽取，(i/M)概率不变，(1i/M)进入下一轮，额外加一次本次操作,answer
5018345540594936677,解释方差：,question
2122935172097327069,期望值与真实值之间的波动程度，衡量的是**稳定性**,answer
1631215494348242441,解释偏差：,question
-2850337277066073733,期望值与真实值之间的一致差距，衡量的是**准确性**,answer
-6630719895861107233,模型训练为什么要引入偏差和方差？请理论论证。,question
-6507403449420574758,优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,answer
-2214716850207361842,**Err=bias+var+irreducibleerror**,answer
9065648270968707104,"以回归任务为例,其实更准确的公式为：**Err=bias^2+var+irreducibleerror^2**",answer
-733014017357138578,符号的定义：一个真实的任务可以理解为Y=f(x)+e，其中f(x)为规律部分，e为噪声部分,answer
1670983890316119987,训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,answer
3692345326902018649,方差：模型的稳定性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx3a2qc3j306400ot8j.jpg),answer
2507288662872147717,偏差：模型的准确性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx6rqg34j305g00odfn.jpg),answer
-204043643825471948,"Err(x)=Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))+Err(f,Y)",answer
4776590992016386698,"Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))为可解释规则误差",answer
8021360176209240835,"Err(f,Y)为噪声e部分，即为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxc64l5yj300g00b0pn.jpg)",answer
2422716465892603723,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxyj2g67j301k00mmwx.jpg)可推导如下：,answer
-7212167727018406164,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxkvx39gj30gh01b3yj.jpg),answer
4392633406249571669,f为真实值，固定；![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型不同数据预测结果的期望，固定；所以f![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)固定,answer
-391154022892487624,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxlzjp9hj305s00mt8j.jpg)中![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxm8gzmrj301z00mq2p.jpg)为常数。所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxlzjp9hj305s00mt8j.jpg)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxo2v9ozj30bw00m0sn.jpg)=0,answer
-6180074391962158636,Err(x)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxvudtfgj30cm00mjra.jpg),answer
1359687012772021203,什么情况下引发高方差？,question
-7011484731201233718,过高复杂度的模型，对训练集进行过拟合,answer
-840504421600434630,带来的后果就是在训练集合上效果非常好，但是在校验集合上效果极差,answer
1316961732726362637,更加形象的理解就是用一条高次方程去拟合线性数据,answer
1347268961153926339,如何解决高方差问题？,question
-4271585456982147704,在模型复杂程度不变的情况下，增加更多数据,answer
9143994318414101698,在数据量不变的情况下，减少特征维度,answer
-2055517854797455394,在数据和模型都不变的情况下，加入正则化,answer
-1143048225014598560,以上方法是否一定有效？,question
-2976073563989116388,增加数据如果和原数据分布一致，无论增加多少必定解决不了高方差,answer
-1737141008157665326,smote对样本进行扩充是否必定可以避免高方差？,answer
-4479359075820615729,过采样是否解决高方差问题？,answer
-1236896535914032826,减少的特征维度如果是共线性的维度，对原模型没有任何影响,answer
6839129160301418045,罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？,answer
6760743838274040488,正则化通常都是有效的,answer
-4235737746418844384,如何解决高偏差问题？,question
3883133702476587606,尝试获得更多的特征,answer
8837019951801615787,从数据入手，进行特征交叉，或者特征的embedding化,answer
5601151593920995311,尝试增加多项式特征,answer
4458052543164542933,从模型入手，增加更多线性及非线性变化，提高模型的复杂度,answer
-8476190329994616305,尝试减少正则化程度λ,answer
-5092488147528657195,特征越稀疏，高方差的风险越高,answer
-6920770915802972980,多个线性变换=一个线性变换，多个非线性变换不一定=一个多线性变换,answer
-599589028796307766,遇到过的机器学习中的偏差与方差问题？,question
7354006734636029435,从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,answer
-4073679096827835830,从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,answer
7879153837735600148,就理论角度论证Bagging、Boosting的方差偏差问题,question
-6911500800146836861,基础：,answer
-4699364746634772356,bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等,answer
6717364313699229663,"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",answer
-8564949186885683729,Bagging,answer
888836136289232879,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyk4wjxkj302v00qa9u.jpg),answer
4439042625989132488,=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyp35ynkj308l00qglh.jpg),answer
6751332456274658856,其中,answer
-7747921201510590472,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lypqw1tjj300d00c0q9.jpg)可以直接提取出来,answer
-4101869730773578712,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyuaep07j308v00ya9x.jpg),answer
8783668830224350292,所以，化简以上的式子可得：Var(F)=m*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyw59y9oj300h00g0rq.jpg)*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lywg4870j300g00k0r2.jpg)+![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyzygc4lj304300lgle.jpg),answer
6401504875843429929,以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可,answer
-348454429935094448,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz7qrpwsj304f015t8i.jpg),answer
-1868777633596507557,E(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz97enuuj3059011743.jpg),answer
583049835857465244,结论：,answer
-1504590206347226056,整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似,answer
694071875157860599,整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,answer
-1843826979696985927,bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,answer
-7528437266861774049,Boosting同理,answer
-8619923185843295769,boosting的前提是弱模型之间高度相关，我们不妨设相关度为1,answer
6051476986340193471,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzwvy93dj302k00kmwx.jpg),answer
8629624915525790465,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzj06hv1j304300qwea.jpg),answer
-4573762226682797351,整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,answer
3674247588268963480,整体模型的方差等于基模型的数量平方成正比，越多模型不稳定性越高，越容易过拟合。,answer
4186210090869327396,GradientBoostingDecisionTree为典型例子,answer
3046614069242078042,遇到过的深度学习中的偏差与方差问题？,question
3204588436375604308,神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,answer
5351616697921521118,但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,answer
5078622462571457239,因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。,answer
-5719848535722755282,dropout,answer
5287360046315345933,dense中的normalization,answer
5659064747747620984,数据的shuffle,answer
-1391058988667858983,方差、偏差与模型的复杂度之间的关系？,question
8450591247457169144,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8ly6pdyouj30dn07eq38.jpg),answer
5853301435760088479,什么叫生成模型？,question
4819144975587044971,"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",answer
6473324779957002950,说白了就是玩的一手x在不同类别下出现的概率+不同类别的概率+x出现的概率进行复合计算，复合的时候考虑独立性的问题。,answer
-8359707602930982806,什么叫判别模型？,question
-2057590856015979601,求一个通过自变量能够表示出因变量的公式y=F(x)或者p(y/x)，核心是对于x找到一个最合适的公式得到y,answer
2340275878823081677,什么时候会选择生成/判别模型？,question
5200094549228167010,明确一点：绝大多数情况下，判别模型都要比生成模型效果好，而且需要的数据量和前提假设都要小于生成模型，上面的概念中可得原因。,answer
1526839625073562025,所以这个问题更想问的是什么时候要去用生成模型：,answer
-7041210013419388811,但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,answer
3357323141881905260,如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,answer
5815988042756072742,一般会追问，如何构造？,answer
-1383138343991650556,FM/FFM,answer
-163664745497501079,NeuralNetwork,answer
-3642155314186992578,线性Dense,answer
3802089480011676110,非线性激活,answer
6746622855080806576,CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,question
2793727365790697745,因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,answer
-4515934164242611552,生成式模型：朴素贝叶斯，混合高斯模型，马尔科夫随机场，EM,answer
-1790570935531700415,仔细看过这些模型细节的朋友都应该知道，他们最后都是判断x属于拟合一个正负样本分布，然后对比属于正负样本的概率,answer
-8822031343605586311,判别式模型：最大熵模型，CRF,answer
-233063588217617255,我的理解：,question
-5636105324845060786,无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,answer
-7306605294320597816,算出属于正负样本的概率在相互对比的就是生成模型，直接得到结果概率的就是判别模型,answer
-1738214754572794854,生成模型得分布，判别模型得最优划分,answer
-6950341451861544807,生成模型可以得到判别模型，反之不成立,answer
5473148620752780075,生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,answer
-1696229910865698849,极大似然估计 - MLE,question
-7605289102165585013,原理：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。,answer
159679227244321695,似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,answer
3707097639791704864,最大后验估计 - MAP,question
5519047409645820106,"MAP的基础使贝叶斯公式：P(θ/X)=P(θ,X)/P(X)。目的是通过观测值使得后验概率P(θ,X)最大即可",answer
-2359689809339291204,极大似然估计与最大后验概率的区别？,question
7815527886836580889,最大似然估计中的采样满足所有采样都是独立同分布的假设,answer
1100788327784246010,最大后验概率在考虑了p(X/θ)的同时，还考虑了p(θ),answer
-5598920719335079954,到底什么是似然什么是概率估计？,question
5074935591658214080,似然：给定了x求θ真实的可能性,answer
-7158872601133996879,概率估计：给定了θ，X=x的可能性,answer
4322221967311684793,DNN与DeepFM之间的区别?,question
-3731620789115354377,DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,answer
-5277376401305018885,Wide&Deep与DeepFM之间的区别?,question
-8419838087451552332,DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,answer
7970083223646440776,你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,question
9200318499420529669,欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,answer
-3611657747496778334,过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,answer
8382886378727396832,DeepFM怎么优化的？,question
8698977729312276067,embedding向量可以通过FM初始化,answer
8969456447148984364,Deep层可以做优化,answer
9018046688980418450,NFM:把deep层的做NFM类型的处理，其实就是deep层在输入之前也做一个二阶特征的交叉处理和fm层一致,answer
3585958378914034940,FM层可以变得交叉更多阶,answer
-5394779047016045734,XDeepFM,answer
-2420452844847899030,不定长文本数据如何输入deepFM？,question
216929694822553820,截断补齐,answer
9104162060795596778,结合文本id+文本长度，在做文本处理之前，先做不等长的sum_pooled的操作,answer
-5863683152225083187,deepfm的embedding初始化有什么值得注意的地方吗？,question
1253877235615432345,"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",answer
477238958923450626,"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",answer
8985207697620957026,文本项目上也可以用预训练好的特征,answer
-3448099698632601332,主要使用了什么机制?,question
6941934465148500683,Attention机制，针对不同的广告，用户历史行为与该广告的权重是不同的。,answer
-5754827622520933087,activation unit的作用,question
2605191490587757169,基于Attention机制，结合当前需对比的不同目标，将用户历史行为进行不同的权重分配。,answer
7649213093085373348,![](https://tva1.sinaimg.cn/large/006tNbRwgy1ga15ugjlkmj308901imx1.jpg),answer
2335308856474141051,通常用户兴趣可以由历史行为(点击/浏览/收藏)等合并得到，及![](https://tva1.sinaimg.cn/large/006tNbRwgy1ga15vxtcu8j302w01imwy.jpg),answer
-6738568454080623565,activationunit在这种思路上，认为面对不同的对象Va兴趣的权重Wi应该也是变换而不是固定的，所以用了g(ViVa)来动态刻画不同目标下的历史行为的不同重要性,answer
8074075437499887269,DICE怎么设计的,question
742400133774514548,先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,answer
-5027536859274208434,"x_p=tf.sigmoid(tf.layers.batch_normalization(x,center=False,scale=False,training=True))",answer
-1209258354781349149,aplha*(1x_p)*x+x_p*x,answer
-794460191961475147,DICE使用的过程中，有什么需要注意的地方,question
8550286657403277349,在用batch_normalization的时候，需要设置traning=True，否则在做test的时候，获取不到training过程中的各batch的期望,answer
5886150795539071019,test的时候，方差计算利用的是期望的无偏估计计算方法:E(u^2)`*`m/(m1),answer
5419731649903846599,选用的原因？,question
-1539369156964012348,类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,answer
6349813992489246648,向量级别的特征交互而不是元素级交互,answer
-2233012644824104230,经验上，vectorwise的方式构建的特征交叉关系比bitwise的方式更容易学习,answer
-1315645112412297424,我也不知道具体好在哪，如果有大佬会可以指导一下，感恩,answer
1827897526533455488,"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",answer
2555786661696966302,"思路一：改变Memorization为attention网络，强化feature直接的关系，对B进行""电脑""与""蔡徐坤""之间的绑定而不是""篮球""和""蔡徐坤""之间的绑定",answer
-2476307248236212223,思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,answer
8942259571777549000,什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,question
8674784096997363342,显示是可以写出feature交互的公式，隐式相反,answer
1899672678848853803,元素级是以feature值交互，向量级是feature向量级点乘处理,answer
-7860734794646293057,高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,answer
4038081751342663937,简单介绍一下XDeepFm的思想？,question
-5118271235254843765,借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,answer
1243196257966772001,高阶特征交互：DNN,answer
-4502397175118797565,低价特征交互：DCN结构(![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)),answer
955817711600635413,这样的网络结构保证来来自X0的1，2，3...N阶的特征组合,answer
-5251904250361578996,借鉴来DCN的交叉网络的特殊结构**自动构造有限高阶交叉特征**：![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg),answer
3148419891788998806,流程概述：,answer
40000585434539938,featureembedding,answer
-3476047547211462050,"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",answer
3536680756119308874,CIN中：,answer
7433357258575394208,"先记录\[batch,field,embedding_size]作为X0，并切分为embedding`*`\[batch,field,1]份",answer
-3998430547018744313,设置三层隐层，单层结点数为200，单层操作如下（以X1为例）：,answer
6541870093517486381,"获取上一次的layerout：X0，并进行切分：embedding`*`\[batch,field,1]",answer
8343889960334004682,"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",answer
-6494259259423768511,"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",answer
3929721988894366452,加偏置项，并进行激活函数处理，完成一轮处理,answer
2427328439807711065,"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",answer
-2976669333966267655,实际过程中，可以对每层对结果进行采样泛化；可以通过最后层输出的残差连接保证梯度消失等等,answer
5973609548819161561,和DCN比，有哪些核心的变化？,question
-7542148996172784953,DCN是bitwise的，而CIN是vectorwise的,answer
5300980280227158191,DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,answer
-2732828395226151488,![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)和![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg)差异的![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9in06u4m6j300j00f0sh.jpg)导致的,answer
-7184560090684525391,时间复杂度多少？,question
-8821118894610593742,假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,answer
-8253545362480690443,CIN:O(m`*`L`*`H`*`H),answer
4781375303850922612,DNN:O(m`*`D`*`H+L`*`H`*`H),answer
1726570748939866687,变长数据如何处理的？,question
-1096789570459494455,input数据中只拿了近20次的点击，部分用户是没有20次的历史行为的，所以我们记录了每一个用户实际点击的次数，在做embedding的时候，我们除以的是真实的historylength,answer
6991916320296638446,20次点击过去一周内的行为，曾经尝试扩大历史点击次数到40，60没有很明显的效果提升,answer
-2979891755179191862,点击行为是处理过的，停留时间过短的click不要,answer
226757955700860852,点击行为是处理过的，连续多次的重复点击会去重,answer
-1268491427235833046,点击行为是处理过的，session内的点击次数需要在约定范围内,answer
8635324457756903156,input是怎么构造的,question
5393061819201529265,最近历史20次点击商品id/文章id，如果不足不需要补充,answer
7198299250116778904,最近历史20次点击商品id对应的品牌/文章id对应的类目，如果不足不需要补充,answer
7695495398530729161,最近历史20次点击商品id对应的类别/文章id对应的栏目，如果不足不需要补充,answer
7647426171228187500,最后一次点击商品id/文章id,answer
6775188245542094186,历史上最高频的商品id/文章id,answer
-6748217631791407239,exampleage,answer
3571068476832561360,user_info:age/gender/地理位置/注册时长,answer
5129988222959969400,cross_info:最后一次点击距click时间，最后一次点击商品浏览次数,answer
6989066252162582279,phone_info:设备信息，登录状态,answer
-8658699901894495495,最后一次点击实际如何处理的？,question
-7610826092743125701,我们会以日进行切分，每日首次点击的lastclick会以\[unknow]进行替代，隔日的点击不会进行计算,answer
2852564861225681232,output的是时候train和predict如何处理的,question
-1406447243637505986,train的时候是进行负采样的,answer
-8335721295944316353,predict的时候是进行的all_embeddingdot,answer
-6026373868502902333,如何进行负采样的？,question
-4564121772185578318,该次点击时间之前所以的item或者article作为候选集,answer
5793736121184555402,负采样我们会进行剔除，把该次click下的同时show的样本进行剔除后采样,answer
5245658738494194348,均衡采样，不会根据其他样本showtime进行加权,answer
-8267551977384273558,为了尽可能多的修正全量样本，尽快达到收敛,answer
8018170279958915803,为了避免其他推荐产生的交叉影响,answer
9170315617208402730,item向量在softmax的时候你们怎么选择的？,question
4980177711123510028,是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,answer
-6634526480433587570,Example Age的理解？,question
4208197667025527278,官方：upload_timeclick_time,answer
-1550789274193128823,希望更倾向于新上视频,answer
-1935451954663698383,民间：click_timenow,answer
3486512301100372229,希望平衡样本构造时间对当前的影响,answer
1816412042671728610,什么叫做不对称的共同浏览（asymmetric co-watch）问题？,question
-7467423662790064210,item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,answer
5024601942554871712,为什么不采取类似RNN的Sequence model？,question
3873911655695373258,在实际的推荐数据获取中，历史点击流收到若干种在线的推荐算法影响，并不全是像自然语言问题一样真实具有序列性,answer
-3655443734443093934,YouTube如何避免百万量级的softmax问题的？,question
-8366941154788979990,负采样,answer
9101607865394201137,serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,question
-4011829743154869308,工程妥协，避免几百万的item每次都要计算一边，而采取的ANN的邻近搜索加快速度,answer
-3619221641409508465,Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,question
-2018672648225329185,在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,question
-6005586898084224264,不对称的共同浏览问题，避免引入futureinformation，产生与事实不符的数据穿越,answer
-9150762617080584843,整个过程中有什么亮点？有哪些决定性的提升？,question
8341065089536721446,embedding,answer
4507300408847522204,引入了doc2vec做init,answer
4022966367187489968,权重共享，没有在softmax处重新构造,answer
2074767556870417210,限定采样集合在click时间发生之前已经有的item,answer
-8855249330631245772,剔除该次点击click时同时展现的其他item,answer
-5506862634946320955,均衡采样,answer
2822655016451351523,加快收敛,answer
-7995611222953948815,避免热门商品item的过度影响,answer
-5925751573099583488,click数据的预处理,answer
1698469675055523686,historyclick,answer
6412419929740853821,停留时间过短的click不要,answer
6690718122798995415,连续多次的重复点击会去重,answer
3121268649013914834,session内的点击次数需要在约定范围内,answer
2192841826434386516,lastclick,answer
7445838770015729374,每日初次点击的lastclick以\[unknown]替代，不做隔日的数据连接,answer
7171201128281073677,在history引入multiheadattention,answer
8835560683667717532,ctr提高了，但是有效点击没有变,answer
5226874859901596829,辗转相除法,question
-1851214313614129820,```python,answer
-2088075203108016683,"defsolve(a,b):",answer
-3878464456023675033,"returnaifb==0elsesolve(b,a%b)",answer
2221344312005706128,```,answer
5621764112396355393,穷举法,answer
2479990942912131001,辗转相减法,answer
6991770472426875543,四则运算,question
-6419411637945390593,(u+v)'=u'+v',answer
-5269435342632425916,(uv)'=u'v',answer
-769874878791152740,(uv)'=u'v+uv',answer
-7991860942203501570,(u/v)'=(u'vuv')/v^2,answer
-5355003644046037778,常见导数,question
8158454173042422280,"y=c(常数),y'=0",answer
-393416357690934955,"y=pow(x,a),y'=a·pow(x,a1)",answer
-6211041571506591558,"y=pow(a,x),y'=pow(a,x)·ln(a)",answer
-2691319516548064673,"y=log(a,x),y'=1/(xlna);特别的ln(x)=1/x",answer
5671144745355790619,"y=sin(x),y'=cos(x)",answer
-3725582857412979054,"y=cos(x),y'=sin(x)",answer
7004811862121870615,"y=tan(x),y'=1/(cos(x)^2)",answer
-2774611997239814991,复合函数的运算法则,question
-6364571325739000757,"若y=f(g(x)),y'=f'(g(x))·g'(x),前提是g在x处可导，f在g(x)处可导",answer
-448667838935971536,莱布尼兹公式,question
-8304786406637992254,"若u(x),v(x)均n阶可导，则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xl2xwg4ij307000udfv.jpg)",answer
1097579729071702837,切线方程,question
6654817774177864099,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xko8hysoj306c0123yi.jpg),answer
-5940383905461096879,法线方程,question
4349951249975468960,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xkomfj6lj309c01amx9.jpg),answer
-725543027068645563,费马定理,question
-278510597889827605,f在x0的邻域内有定义，且恒满足：f(x)<=f(x0)｜f(x)>=f(x0),answer
-5327129162614682250,f在x0处可导，则满足f'(x0)=0,answer
1585529481579859869,拉格朗日中值定理,question
3287690773160557949,设函数f(x)满足条件：,answer
7078082751749508433,"\[a,b]上连续",answer
6511827801560628731,"\(a,b)内可导，则\(a,b)存在ζ，使得f(b)f(a)=f'(ζ)(ba)",answer
-1712288761157982664,柯西中值定理,question
6623850494262172031,"设函数f(x),g(x)满足条件：",answer
224403496526638992,"\(a,b)内可导，且f'(x)和g'(x)存在，且g'(x)!=0",answer
-8056908197675354312,"则\(a,b)存在ζ，使得(f(b)f(a))g'(ζ)=f'(ζ)(g(b)g(a))",answer
-2966802032878178908,期望,question
6561923314848222903,离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,answer
-5863039179139063574,若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,answer
-6302154571360341183,E(ax+by+c)=aE(x)+bE(y)+c,answer
-2162189494647673688,如果x和y独立，E(xy)=E(x)E(y),answer
9043962032853393811,方差,question
-53013097870470249,方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,answer
6899078626756994332,方差刻画了随机变量的取值对于其数学期望的离散程度。,answer
2526329363020383811,方差深入：,answer
8286720439213621551,很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,answer
883415775508340379,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zkdv6ukj308700kweb.jpg),answer
-9057868565375155353,如果x和y独立，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zl5np3yj308600kt8j.jpg),answer
-807621070423342161,标准差,question
1953976528444577042,标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,answer
5423873318277177385,协方差,question
-7372356708439201780,协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,answer
1713348782905521640,在概率论和统计学中，协方差用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。,answer
8114071719926431735,"Cov(x,y)=E((xE(x))(yE(y)))",answer
282327079994266824,"Cov(c+ax,d+by)=abCov(x,y)",answer
5072244282816324191,相关系数,question
-8285129350657782143,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zq1gbe7j306601ct8k.jpg),answer
-3847484638029298675,"\[1,1]之间，值越接近1，说明两个变量正相关性（线性）越强。越接近1，说明负相关性越强，当为0时，表示两个变量没有相关性",answer
6785135892459336296,均匀分布,question
8610936714333747765,"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",answer
3400671768908777120,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91ysz3sxsj30aw023glh.jpg),answer
-8858472519123582777,伯努利分布,question
7919292386288231291,"伯努利分布：参数为p∈[0,1]，设随机变量X∈{0,1}，则概率分布函数为：",answer
8116449803011058922,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91yyoulv0j306m00lmwz.jpg),answer
3136829534100409745,期望为p，方差为p(1p),answer
-6328973675783661092,二项分布,question
5953151189161948948,独立重复地进行n次试验中，成功x次的概率:,answer
-6313296499794387181,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z2sy9m5j308800ja9w.jpg),answer
8874597114899192891,期望为np，方差为np(1p),answer
206728880193059552,高斯分布,question
-8712734336172781723,我们在做模型训练的之后，随机变量取值范围是实数，大多数情况下都假设变量服从高斯分布，原因：,answer
8548163085986028196,随机变量大多数情况下有若干个因素组合而成，中心极限定理表明，多个独立随机变量的和近似正态分布,answer
6723606046435716643,在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,answer
3329139864906746918,典型的一维正态分布的概率密度函数为:,answer
3085731683275538933,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z5lcnmcj30dk02fa9z.jpg),answer
-5818016140031778507,拉普拉斯分布,question
-7592403064084806089,概率密度函数：,answer
-8733266718117768566,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z66211aj309c01hjr9.jpg),answer
-8094477874823255696,期望为u，方差为2γ^2,answer
6742956313533239306,拉普拉斯分布比高斯分布更加尖锐和狭窄，在正则化中通常会利用该性质,answer
4211817258629448001,泊松分布,question
1709487402032323563,假设已知事件在单位时间（或者单位面积）内发生的平均次数为λ，则泊松分布描述了：事件在单位时间（或者单位面积）内发生的具体次数为k的概率。,answer
-3246206372398309549,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z8oplp1j306f01g0sk.jpg),answer
-8303726745290962951,期望：λ，方差为：λ,answer
-1281972474977320676,条件概率,question
242897760026382990,P(A/B)=P(AB)/P(B),answer
-7300420164915352591,独立,question
-5942843378076937941,P(AB)=P(A)P(B),answer
8066926376943784911,概率基础公式,question
-2693868727601049167,加法：P(A+B)=P(A)+P(B)P(AB),answer
5747394601710814701,减法：P(AB)=P(A)P(AB),answer
7578293378069828861,乘法：P(AB)=P(A)P(B/A),answer
-7707513113877203403,全概率：,question
8879992570336779437,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920h3l62yj305u00qweb.jpg),answer
-8442782436782295534,贝叶斯,question
-721131962831853951,P(B/A)=P(B)*P(A/B)/P(A),answer
5199853639570631415,切比雪夫不等式,question
-7354846326910509857,"p(|xu|>k∂)<=1/(k^2),满足k>0,u为期望,∂为标准差",answer
-3796879499793283906,绝大多数数据都应该在均值附近,answer
708492972345550094,抽球,question
-7843880446284488613,有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),answer
6950074001357719072,无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),answer
7138155078292811173,纸牌问题,question
-3006485380985932218,问题：54张牌，分成6份，每份9张牌，大小王在一起的概率？,answer
-4725958549048758722,54张牌分成6等份，共有M=(C54取9)*(C45取9)*...种分法。,answer
4800888254794111355,其中大小王在同一份的分法有N=(C6取1)*(C52取7)*(C45取9)*...种。,answer
8137367792907543912,因此所求概率为P=N/M,answer
3105618433220790235,棍子/绳子问题,question
-7824303744078065883,问题：一根棍子折三段能组成三角形的概率？,answer
103677003216988331,假设：棍子长度为1，第一段长度为x，第二段长度为y，第三段长度1xy,answer
2362312339900490198,分母：总样本空间为：1*1=1,answer
6765844282768289589,分子：两边之和大于第三边，得1/8,answer
-6059535317857575625,问题：某城市发生一起汽车撞人逃跑事件，该城市只有两种颜色的车，蓝20%绿80%，事发时现场只有一个目击者，他指正是蓝车，但根据专家分析，当时那种条件下能看正确的可能性是80%，那么肇事的车是蓝车的概率是多少？,answer
3031678749101902912,假设事件A为目击者指正蓝车，事件B为肇事车为蓝车，事件C为肇事车为绿车，那么有：,answer
7176958568392767927,0.2`*`0.8/(0.2`*`0.8+0.8`*`0.2)=0.5,answer
-1464576635479891763,选择时间问题,question
-4507534976841792048,"问题：一个活动,n个女生手里拿着长短不一的玫瑰花,无序的排成一排,一个男生从头走到尾,试图拿更长的玫瑰花,一旦拿了一朵就不能再拿其他的,错过了就不能回头,问最好的策略及其概率?",answer
7003394913176383180,1/e,answer
3510063677937010394,0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,question
7776727688794084776,均匀分布：,answer
6024669128193226916,E(x)=(a+b)/2,answer
1475161169215007139,标准差：D(x)=(ba)^2/12,answer
5092425697139714872,所以只需要对x做变换：sqrt(12(x1/2))即可,answer
2396283109794290723,抽红蓝球球,question
4260017884481129217,问题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数,answer
7858854040411054434,假设设抽到蓝球的概率为p，设抽到红球的概率为q，那么抽取到的次数为：1·p+2p·q+...+np·q^(n1),answer
1253538768912298362,"可得E=p\[1+2q+...+nq^(n1)],令1+2q+...+nq^(n1)=s，再由s为等比公式和ssq得，E=1/p",answer
-5389898035445895273,泰勒公式,question
8705897928441978325,定义：f(x)在x0处的邻域内有n+1阶的导数，在x0的邻域内的任x，x和x0之间至少存在一个ζ，使得,answer
3049793549390890440,f(x)=f(x0)+f'(x0)(xx0)+1/2!f''(x0)(xx0)^2+...+Rn(x),answer
3777390215761286807,其中，Rn(x)=f<n+1>(ζ)/(n+1)!(xx0)^(n+1)，为泰勒余项,answer
-2871248942578116006,常见泰勒公式,question
6850832732153225104,![](https://i.bmp.ovh/imgs/2019/11/0a10d9591cc0c4ac.png),answer
6357816442943426159,迭代公式推导,question
-2846449081046200343,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfple4x4j303x00i742.jpg),answer
7341081381123544121,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfr2t8flj306l00mmwz.jpg),answer
-3057611148037247114,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfruh1klj3043015wea.jpg),answer
-1919599392443612141,这边是2次，所以直接以y=x^2化简了,answer
6355160086335492094,实现它,question
-3646821570676394763,"defget_ans(nums,count=10000):",answer
1155428306009933858,ans=nums,answer
-2913559977826046863,ifnotans:,answer
1330732700853179263,returnans,answer
-7844548105954774602,Times=0,answer
-4678871516115215712,whileTimes<count:,answer
893422785288171158,ans=0.5*(ans+nums/ans),answer
5022371395463138013,Times+=1,answer
-2067415441552551001,范数,question
6471280280827314514,1范数：各列绝对值和的最大值,answer
-3791827058965126802,"2范数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zwforqmj302i00n0si.jpg),矩阵$A^TA$的最大特征值开平方根",answer
-1381918510893845359,特征值分解，特征向量,question
4024443754798874969,特征值分解可以得到特征值与特征向量,answer
-3447201635020927927,特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,answer
5515626025977230648,"矩阵A的特征值与其特征向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zxnzwesj300h00h0rz.jpg),特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zy0ybcpj300a00c0pd.jpg)满足：",answer
3719337037186314750,也可写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9201wcuefj303600qa9u.jpg),answer
6814750028694920201,其中，Q为特征向量组成的矩阵，∑为特征值由大到小组成的矩阵,answer
1237103148883053696,正定性,question
-7587290786712379180,如何判断矩阵的正定性？,answer
-6427019373406520185,矩阵的特征值大于等于0，半正定,answer
-1964831700261697881,矩阵的特征值大于0，正定,answer
562790841596578188,正定性的用途？,answer
-6016552776261888694,Hessian矩阵正定性在梯度下降的应用,answer
9140862082363415505,"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",answer
-4296649260230637919,在svm中核函数构造的基本假设,answer
3095269185501462695,统计方法,question
-4415695384910422538,3∂原则,answer
4151088691598871099,数据需要服从正态分布,answer
1576717704334346736,只能解决一维问题：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m954lh5tj301h00gdfl.jpg),answer
-8018429297363776537,基于正态分布的离群点检测方法,answer
5700397476019990954,一元高斯分布校验：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m91basuwj306201h0sl.jpg)，如果概率值大小离群则代表为异常点,answer
-6381794096613393478,多元高斯分布检测：,answer
7466264943733087084,假设n维的数据集合![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算n维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg),answer
6961577936197373001,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fdr3xfj30140080sl.jpg)的协方差矩阵：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fly7rpj306g00ja9z.jpg),answer
-5264855477297911555,得到![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9gzmd1cj30bz01874b.jpg),answer
-3134845913432826567,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m92kshc2j30d6073dft.jpg),answer
5919971798910773045,马氏距离,answer
1919405134982180219,假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。,answer
3657162960337993535,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kguzdgj307e00lmx3.jpg),answer
-2234906094558320723,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9rdqiqjj300g00h0sl.jpg)统计检验,answer
8123095226541758355,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ryphxxj304u00kmx2.jpg),answer
6734256808223819819,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第i维的均值，n是维度",answer
-1656766438680601100,箱型图,answer
2860979937952398771,"IQR，\[Q13/2(Q3Q1),Q3+3/2(Q3Q1)]",answer
5849469982824099920,矩阵分解方法,question
-4231732632408560867,PCA,answer
842326597379328029,去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,answer
19264605160471329,核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,answer
-379953004773212748,问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,answer
-8374883323951181034,SVD,answer
8192368858953626188,假设dataMat是一个p维的数据集合，有N个样本，它的协方差矩阵是X。那么协方差矩阵就通过奇异值分解写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8man1o3trj302h00hmx1.jpg),answer
-2696267579992309711,"其中P是一个(p,p)维的正交矩阵，它的每一列都是X的特征向量。D是一个(p,p)维的对角矩阵，包含了特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8manhjc7qj301n00ga9x.jpg)。",answer
6843551864432745591,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mar6eb43j304300h746.jpg)可以认为是dataMat在主成分topj上的映射,answer
897107352973402709,最后还需要拉回原空间：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mb1hmrmnj306w00i747.jpg),answer
5348803601696498436,异常值分数（outlierscore）：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8maynk5wkj30a300mjrc.jpg)+![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mayu64x7j305500mt8m.jpg),answer
4637383168815041322,特征值和特征向量的本质是什么？,question
-2489839743169891054,一个特征向量可以看成2维平面上面的一条线，或者高维空间里面的一个超平面,answer
-4309672187449971985,特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度,answer
-6232159833030152063,矩阵乘法的实际意义？,question
4077764194149126316,两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,answer
6632381298958005304,矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,answer
-8889067267231713980,密度的离群点检测,question
-4171816755710025319,定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。,answer
8996197060978468180,我们可以通过随机选择联通点，人为设置联通点附近最小半径a，半径内最小容忍点个数b，再考虑密度可达，形成蓝色方框内的正常数据区域，剩下的黄色区域内的点即为异常点。,answer
-7335635866719831823,LocalOutlierFactor算法,answer
-1190430548417035448,孤立森林:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mcoswixrj30iy0em0tb.jpg),answer
675397106794938765,经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),answer
5964058821692571043,经验2：树的个数在256棵以下,answer
-4193286709383767093,缺点：,answer
2064738164042609827,计算量大：o(n^2),answer
7937734001490548638,需要人为选择阈值,answer
-4634696187433983046,聚类的离群点检测,question
3721574207380062665,一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。,answer
-4627437899916407147,缺点也就是聚类的缺点，包括初始点对结果的影响，数据是否保持凸型对结果对影响，簇的个数的选择,answer
-8890674241826804384,如何处理异常点？,question
3638128001610077390,删除含有异常值的记录：直接将含有异常值的记录删除；,answer
4877419086850196027,视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；,answer
-8742006723217211794,平均值修正：可用前后两个观测值的平均值修正该异常值；,answer
8067668981339319211,生成列新特征：category异常,answer
-2641158538884515097,不处理：直接在具有异常值的数据集上进行数据挖掘；,answer
-418649848180980998,为什么要对数据进行采样平衡,question
3134633603086858949,"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",answer
1622411140970498375,上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,answer
-2676737863783670069,比如99%为正样本，1%为负样本，如果分类器把所以样本预测为正样本则准确率高达99%，显然不符合实际情况,answer
-3950916339468957555,是否一定需要对原始数据进行采样平衡,question
1227671378249493085,否。,answer
8674001385248714904,采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降,answer
607218849341284723,采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,answer
3527642468920719739,有哪些常见的采样方法？,question
2424019200324656778,随机采样,answer
-7116986342290192271,无放回的简单抽样：每条样本被采到的概率相等且都为1/N,answer
5022968471735253116,有放回的简单抽样：每条样本可能多次被选中,answer
5591784523057089922,上采样：即合理地增加少数类的样本,answer
1612916689629341139,下采样：欠抽样技术是将数据从原始数据集中移除,answer
-1743100014438749882,平衡采样：考虑正负样本比,answer
4937072691742720637,分层采样：通过某一些feature对数据进行切分，按照切分后的占比分别进行采样,answer
-1388663601699248265,"整体采样：先将数据集T中的数据分组成G个互斥的簇,然后再从G个簇中简单随机采样s个簇作为样本集",answer
-3728837311649438854,合成采样,answer
-6057615709363668905,"相对于采样随机的方法进行上采样,还有两种比较流行的上采样的改进方式：",answer
-6697995379474958779,SMOTE,answer
-5570844803910172736,"x_new=x+rand(0,1)*(x′−x)",answer
-2385427454031233568,**带来新样本的同时有可能造成不同类别样本之间的重合**,answer
-3322398126167609217,BorderlineSMOTE为了解决上面的问题，在x_new生成之前，会先判断x这个点是否周围都是同类别的点,answer
7615414869616595270,ADASYN,answer
7456239091783266824,同上，也是在构造样本点的过程中考虑了正负样本比,answer
-4963009477270132045,平衡欠采样,answer
-2520074413569032287,EasyEnsemble，利用模型融合的方法（Ensemble）,answer
4172473738201676822,少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合,answer
-510085077402568446,BalanceCascade，利用模型融合的方法（Boost）,answer
-4538172138116110249,每次剔除预测正确的多数样本，加入新的未预测的多数样本,answer
-5251982481923931951,NearMiss,answer
-6222807498045476328,选择离各种情况下的少数样本位置最远的多数样本进行训练,answer
4075053328421109209,能否避免采样？,question
815634199483811378,可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,answer
-2615989551301480846,你平时怎么用采样方法？,question
4177976655899400904,尽量避免使用合成采样的方式去做数据填充，总结如下：,answer
-5691495769256144595,由于项目中时间的充裕问题，填充的结果往往是正负样本交叠且无感知的，会干扰分类器,answer
-7085528429964064358,通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀,answer
216496178197878670,合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合,answer
-4122941945510944880,为什么需要对数据进行变换？,question
3326460904590817708,避免异常点：比如对连续变量进行份桶离散化,answer
6458054994557855403,可解释性或者需要连续输出：比如评分卡模型中的iv+woe,answer
-1691121148448134459,使得原始数据的信息量更大：比如log/sqrt变换,answer
1132565811986725001,归一化和标准化之间的关系？,question
-3311551332216045368,归一化(maxmin),answer
3680892127532703166,缩放仅仅跟最大、最小值的差别有关，只是一个去量纲的过程,answer
4414843696841411523,标准化(zscore),answer
770186259633043700,缩放和所有点都相关，数据相对分布不会改变，集中的数据标准化后依旧集中,answer
-9170233356865741259,作用,answer
-7134886329958100039,解决部分模型由于数据值域不同对模型产生的影响，尤其是距离模型,answer
-8273232194861923493,更快的收敛,answer
-5277907160681781010,去量纲化,answer
-6208183191526074728,避免数值计算溢出,answer
-1635726538116411628,总结,answer
-2206182933192786422,异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降,answer
3257091615682339680,分布角度：使用标准化之前，要求数据需要近似满足高斯分布，不然会改变数据的分布，尤其是对数据分布有强假设的情况下,answer
-2135458807859338833,上线变动角度：归一化在上线的时候需要考虑上下约束届是否需要变动，标准化则不需要考虑变动,answer
5896624254032259990,值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定,answer
-2201801667526920509,模型角度：一般涉及距离计算，协方差计算，数据满足高斯分布的情况下用标准化，其他归一化或其他变换,answer
-5760680234285690293,常用模型,answer
6882586511074760178,knn：计算距离，不去量冈则结果受值域范围影响大,answer
9147019972524682323,neuralnetwork：梯度异常问题+激活函数问题,answer
4972708430679000736,连续特征常用方法,question
-1774662618539124969,截断,answer
-1928201268380901771,"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",answer
7377088296328901225,参考异常点里面的outlier识别，以最大值填充或者以None,answer
-3110207725812687339,二值化,answer
752281092566519210,数据分布过于不平衡,answer
-3984642960446343730,空值/异常值过多,answer
-4543386733455604354,分桶,answer
-6119754376436052507,小范围连续数据内不存在逻辑关系，比如31岁和32岁之间不存在明显的差异，可以归为一类,answer
4002809853616148566,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2hcomdwj30k60djdh4.jpg),answer
5292208101229339221,离散化,answer
-261426240279096270,数值无意义，比如学历、祖籍等等,answer
-6824274898277745438,缩放,answer
-4594790772471535310,zscore标准化,answer
4515812075414668257,minmax归一化,answer
2821725940751757168,范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg),answer
-9077081183277299832,L1范数,answer
5123694816533148526,L2范数,answer
7874857220885429302,平方根缩放,answer
1898996666941300520,对数缩放,answer
7016311765485812573,对数缩放适用于处理长尾分且取值为正数的数值变量,answer
-251851826308455689,它将大端长尾压缩为短尾，并将小端进行延伸,answer
-3444595091980527265,可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)log(y),answer
1850676661713968316,可以把有偏分布修正为近似正太分布,answer
-4806085092529578949,BoxCox转换,answer
-1029763714194874341,![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8mfjjwir3j309m038gln.jpg),answer
-3570035466893520470,通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,answer
-941744902829702305,特征交叉,answer
4273207711425543053,人为分段交叉,answer
6833458274035591627,提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释,answer
3833004458702484394,离散变量的交并补,answer
-3259342777725822039,连续变量的点积，attention类似,answer
8508563392777176129,交叉中需要并行特征筛选的步骤,answer
-6315501291060348030,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2rf5aawj30ka0j6q48.jpg),answer
-8072695281199943108,自动组合,answer
306067229453021882,FM/FFM中的矩阵点积,answer
-6520112037257045291,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2un2ckzj30eb07jaan.jpg),answer
593404648412984324,NeuralNetwork里面的dense,answer
-630079123085929784,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2vb1nnij30co07lt94.jpg),answer
-6983502987336693022,条件选择,answer
-964471993016704821,通过树或者类似的特征组合模型去做最低熵的特征选择,answer
3561861798822795055,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2t44a2mj30im0bt0tz.jpg),answer
-8482301269619683190,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2v20wjbj30hs0dx0tq.jpg),answer
3568522929824122283,非线性编码,answer
5397636694830619232,核向量进行升维,answer
5845723301308313208,树模型的叶子结点的stack,answer
7314332664283922915,谱聚类/pca/svd等信息抽取编码,answer
1051843997975095144,lda/EM等分布拟合表示,answer
-3173068017537690213,离散特征常用方法,question
2800794364662258939,onehotencoder,answer
9072342418279279464,分层编码,answer
5592149515559937233,有一定规律的类别数据，邮政编码，手机号等等,answer
-2710944792290174436,计数编码,answer
-8532124786808383850,"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",answer
-5020077705416243210,"对异常值比较敏感,特征取值有可能冲突",answer
6396493549361628071,计数排名编码,answer
-7113075244485091268,解决上述问题，以排名代替值,answer
-7514575797729824566,Embedding,answer
6577370221288660760,"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",answer
-1002011933614030467,类别特征之间交叉组合,answer
4131846521805581452,笛卡尔交叉,answer
-6500619303806593441,类别特征和数值特征之间交叉组合,answer
-1987462782100214408,均值、中位数、标准差、最大值和最小值,answer
-4298282517135301171,分位数、方差、vif值、分段冲量,answer
-6024337161022505307,文本特征,question
6133838842064412515,预处理手段有哪些？,answer
-5403768567780753497,将字符转化为小写,answer
6660537529191775649,分词,answer
-8507105929164988334,去除无用字符,answer
4823488233830373929,繁体转中文,answer
-1550918299055294328,去除停用词,answer
-3090705656616282031,去除稀有词,answer
-8229368997219984797,半角全角切换,answer
-7667787518275697966,错词纠正,answer
-4864792811299776612,关键词标记,answer
865863976471828224,TFIDF,answer
387789603749420571,LDA,answer
8436307962828893655,LSA,answer
-7200519235038823323,提取词根,answer
4230043963354778772,词干提取,answer
1807723679972733223,标点符号编码,answer
-2586884235479598545,文档特征,answer
6951404094047500289,实体插入和提取,answer
2479337842830606706,文本向量化,answer
-5709142152971405345,word2vec,answer
943775773905966107,glove,answer
6244538889714546946,bert,answer
-4823595886462580316,文本相似性,answer
6419888012585676735,如何做样本构造？,answer
8904867417995201059,按标点切分,answer
-3519959436399253037,按句切分,answer
-4946643044764240759,对话session切分,answer
4939845087116185110,按文章切分,answer
-2962770543338003709,按场景切分,answer
-6069791034742171295,分词过程中会考虑哪些方面？,answer
2614324146237355610,词性标注,answer
5426820721046046240,词形还原和词干提取,answer
2842700057657671914,词形还原为了通用性特征的提取,answer
7882806460297052377,词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,answer
9173995419284484390,文本中的统计信息一般有哪些？,answer
3450262033199128691,直接统计值：,answer
-6286846050187034886,文本的长度,answer
8112053238770974163,单词个数,answer
4667579653258833564,数字个数,answer
-1077594417247477620,字母个数,answer
751481962743803479,大小写单词个数,answer
1795898323969880972,大小写字母个数,answer
9004287310808593808,标点符号个数,answer
-1560977427348827942,特殊字符个数,answer
-3247666470255204180,数字占比,answer
-5064283660876796242,字母占比,answer
9142629557516148136,特殊字符占比,answer
2974786080324582288,不同词性个数,answer
644129275744728124,直接统计值的统计信息：,answer
-2662974966206365643,最小最大均值方差标准差,answer
1349715098533794411,分位数，最早/最晚出现位置,answer
1896895112468454774,直接对文本特征进行整理手段有哪些？,answer
8972419663608528117,NGram模型,answer
806416907320681910,将文本转换为连续序列，扩充样本特征,answer
7789048961223751794,连续语意的提取,answer
4403542260974077709,"权重评分，去除掉一些低重要性的词，比如每篇文章都出现的""的""，""了""",answer
-3715691284375857740,主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,answer
-8717597577760587451,相似度,answer
799500328783436775,余弦相似度,answer
-2461260702295557586,Jaccard相似度,answer
4129680945359481,共现性,answer
840576163489889200,Levenshtein(编辑距离),answer
-211759389016908968,文本近似程度,answer
-7337563317609520020,海林格距离,answer
4549766925834115313,用来衡量概率分布之间的相似性,answer
-800116642538254678,JSD,answer
8512207532405472231,衡量prob1和prob2两个分布的相似程度,answer
1951040531698555010,向量化,answer
881542614989936576,文本处理有大量可以讲，可以谈的，以上只是做了一个最简单的汇总，详细的会在自然语言处理的专题一条一条分析，面试官也一定会就每个知识点进行展开,answer
219750475798354868,画一个最简单的最快速能实现的框架,question
8995657523803971228,建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,answer
6457024832675608169,建议从简单的开始，然后面试官说还有其他方法么？再做延展：,answer
-8692340636012301687,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mzet073zj31jw0u046i.jpg),answer
1772120328217447924,为什么要做特征选择？,question
-8938212046843835971,耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,answer
5374000857060083537,过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,answer
3142538555521949560,共线性：单因子对目标的作用被稀释，解释力下降,answer
5487959735353150542,从哪些方面可以做特征选择？,question
-427146063690700711,方差，是的feature内的方向更大，对目标区分度提高更高贡献,answer
8309501393054160289,相关性，与区分目标有高相关的特征才有意义,answer
-3028305923008324553,既然说了两个方向，分别介绍一些吧,question
-4657223536162234765,移除低方差特征,answer
4469566215687145296,移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,answer
6687370207200526152,考虑有值数据中的占比，异常数据的占比，正常范围数据过少的数据也可以移除,answer
3639078117871803180,相关性,answer
-6428788755977427989,单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,answer
-7425564482781800968,皮尔森相关系数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n46pedyoj303e019t8i.jpg),answer
-8463700618608555004,Fisher得分:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n4jajze8j31hc0u0dt2.jpg),answer
1552412220900823149,假设检验,answer
129166973830849837,卡方检验,answer
8978193742322776397,ANOVA,answer
6059815348613970821,熵检验,answer
-247140686760996993,互信息熵,answer
-1980276525226555783,"度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立",answer
2767605847364834019,KL散度,answer
-8981160026603262759,相对熵,answer
1649208307132488775,是不是一定需要对缺失值处理？,question
2751898605942868051,当缺失值占比在可接受的范围以内的时候才需要进行填充，如果缺失值大于50%以上的时候，可以选择进行二分化，如果缺失值大于80%可以完整删除该列而不是强行去填充,answer
2805310221068350204,直接填充方法有哪些？,question
6140427023371334911,均值,answer
6352266784151079557,中位数,answer
-4827430372910249192,众数,answer
-7824175230272283810,分位数,answer
8594247061784985067,模型插值方法有哪些？及方法的问题,question
1716141430763174802,有效性存疑，取决于特征列数,answer
7792642917788444824,生成的插值来源于其他列的特征，是不是意味着插值的结果已经是和其他列的组合高相关,answer
7516291432019932269,如何直接离散化？,question
-1585780739742253344,离散特征新增缺失的category,answer
-8123902680909124093,hold位填充方法有哪些？,question
7754404383894402945,把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果,answer
-7266658202125239495,可以参考YouTube中的新商品向量生成逻辑,answer
-1872667475214739559,bert中的\[UNK]向量，\[unused]向量,answer
-2250534152338887859,怎么理解分布补全？,question
564836237117221977,如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,answer
-7402838550821843834,random方法,question
4527451360104514364,在缺失量特别少(通常认为小于1%)的时候，可以随机生成,answer
2105774144428525465,实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多,answer
-764901511307572754,快速,answer
-669056546037531529,对原始数据的前提假设最少，也不会影响到非缺失列,answer
-6536907941612427211,在深度学习中，hold位填充方法用的最多,answer
8618951644992582997,在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,answer
2333294403168199808,而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性,answer
4964843942407859812,常见决策树,question
4032014142206304551,|模型|ID3|C4.5|CART|,answer
-6727263851183339829,|:|:|:|::|,answer
-7074338663297552718,|结构|多叉树|多叉树|二叉树|,answer
-8383179388050446834,|特征选择|信息增益|信息增益率|Gini系数/均方差|,answer
7067180623678077665,|连续值处理|不支持|支持|支持|,answer
-993076351150159783,|缺失值处理|不支持|支持|支持|,answer
2025018696532643525,|枝剪|不支持|支持|支持|,answer
8304239060363221408,简述决策树构建过程,question
-6123536016875313341,1.构建根节点，将所有训练数据都放在根节点,answer
1108745329145300611,2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,answer
560062837766080912,3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,answer
-9114237831711368319,详述信息熵计算方法及存在问题,question
-1261668865309638344,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925k1ns0yj305y018glg.jpg),answer
3255394274076776487,其中，D为数据全集，C为不同因变量类别k上的子集(传统意义上的y的种类),answer
-3980923341247225810,详述信息增益计算方法,question
5837483698663672759,条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,answer
6586897814204056927,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925puuv1cj308p01kdfr.jpg),answer
-2573621381393844843,信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：,answer
-7696615380335845838,"I(D,A)=H(D)H(D/A)",answer
-8576454313897456954,简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,answer
7044681880113989878,详述信息增益率计算方法,question
5343655595408134998,"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",answer
2934727294446148073,信息增益率：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9260xx5wej304b017q2r.jpg),answer
-8913630609921417935,解释Gini系数,question
6243666713478946985,Gini系数二分情况下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92705elt9j308000q744.jpg),answer
4718062533517902727,对于决策树样本D来说，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9273px3t2j30a4018wee.jpg),answer
-3430778148225903817,对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg),answer
3423959750699159029,ID3存在的问题,question
4626936776762395129,存在偏向于选择取值较多的特征问题,answer
1959432194637618940,连续值不支持,answer
3005118915295687163,缺失值不支持,answer
-1604525092406788139,无法枝剪,answer
6926561881633368036,C4.5相对于ID3的改进点,question
-2456320921870923272,主动进行的连续的特征离散化,answer
6125605694768013773,比如m个样本的连续特征A有m个，先set在order，再两两组合求中间值，以该值点作为划分的待选点,answer
-5561817035954879366,**连续特征可以再后序特征划分中仍可继续参与计算**,answer
3451276064547348248,缺失问题优化,answer
3610456325895580493,训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,answer
7726833194939107688,预测：直接跳过该节点，并将此样本划入所有子节点，划分后乘以系数计算，系数为不缺失部分的样本分布,answer
-6890026604813196591,采用预枝剪,answer
3597696087415931281,CART的连续特征改进点,question
-1892845165075148572,分类情况下的变量特征选择,answer
-8296920939509393438,离散变量：二分划分,answer
-8122824118576931123,连续变量：和C4.5一致，如果当前节点为连续属性，则该属性后面依旧可以参与子节点的产生选择过程,answer
4272155436464728980,回归情况下，连续变量不再采取中间值划分，采用最小方差法,answer
1969994177334835538,CART分类树建立算法的具体流程,question
1701389836235326332,我们的算法从根节点开始，用训练集递归的建立CART树。,answer
-5349013786883087844,对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,answer
-7728347767114543571,计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归,answer
8567592328120194636,计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数,answer
-3324010915512127190,在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,answer
2982157998584776679,递归1～4,answer
-2935926838408628815,CART回归树建立算法的具体流程,question
1628259231731313271,其他部分都一样，在构建过程中遇到连续值的话，并不是利用C4.5中的中间值基尼系数的方式，而是采取了最小方差方法：,answer
-7012612119030292833,对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),answer
-3696690480389050117,CART输出结果的逻辑？,question
8260716453857209364,回归树：利用最终叶子的均值或者中位数来作为输出结果,answer
-7580089293581928812,分类树：利用最终叶子的大概率的分类类别来作为输出结果,answer
-8343394715628063527,CART树算法的剪枝过程是怎么样的？,question
-2142674655790964957,目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量,answer
170122082157504912,当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,answer
4224450851043308956,当然，这是两种极端情况。一般来说，𝛼越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的𝛼，一定存在使损失函数𝐶𝛼(𝑇)最小的唯一子树。,answer
-8675478079014050356,"由枝剪到根结点及不枝剪两种情况可得：𝛼=(𝐶(𝑇)−𝐶(𝑇𝑡))/(|𝑇𝑡|−1),C(T)为根结点误差",answer
-2374439002569999318,计算出每个子树是否剪枝的阈值𝛼,answer
3008027559787233456,选择阈值𝛼集合中的最小值,answer
6053943910793601466,分别针对不同的最小值𝛼所对应的剪枝后的最优子树做交叉验证,answer
1473676328671801220,树形结构为何不需要归一化？,question
-178201819987703090,无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,answer
-7366592240839794226,决策树的优缺点,question
-2003513078678557636,优点：,answer
7757946408689146847,缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散,answer
-4679603778032328779,可解释性强,answer
-501982487329195419,算法对数据没有强假设,answer
-793376102687610632,可以解决线性及非线性问题,answer
-850860440115007789,有特征选择等辅助功能,answer
-8756969101530223625,处理关联性数据比较薄弱,answer
1107524418771167900,正负量级有偏样本的样本效果较差,answer
4387706608067405057,单棵树的拟合效果欠佳，容易过拟合,answer
-6892209997750858762,简单介绍SVM?,question
-503913543032804326,从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2·||W||·||W||)subjectto：y(wx+b)>=1，其中||·||为2范数,answer
8838308419145003771,然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,answer
-6405287485969797110,最后再利用SMO（序列最小优化）来解决这个对偶问题,answer
3365334259452402129,什么叫最优超平面？,question
-1977508283746687284,两类样本分别分割在该超平面的两侧,answer
2997456706657383931,超平面两侧的点离超平面尽可能的远,answer
-528209187864422491,什么是支持向量？,question
-1221171815895134828,在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。换句话说，就是超平面附近决定超平面位置的那些参与计算锁定平面位置的点,answer
-7597631618279506675,SVM 和全部数据有关还是和局部数据有关?,question
-185020870694068125,局部,answer
6709230853434556007,加大训练数据量一定能提高SVM准确率吗？,question
2816104668005912791,支持向量的添加才会提高，否则无效,answer
7147962502960402293,如何解决多分类问题？,question
2316204966519813609,对训练器进行组合。其中比较典型的有一对一，和一对多,answer
-4522310751944247001,可以做回归吗，怎么做？,question
-5410193762923842729,可以，把loss函数变为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93jrdscfrj309l011dfo.jpg),answer
8856587565302393116,SVM 能解决哪些问题？,question
-4953783779364941095,线性问题,answer
-4240616014906898576,对于n为数据，找到n1维的超平面将数据分成2份。通过增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的,answer
-7125092331577579249,非线性问题,answer
1453194949014862609,SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,answer
-1782764784909445230,介绍一下你知道的不同的SVM分类器？,question
-2707361818249748185,硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器,answer
3073758479757631877,软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器,answer
3202232957510331265,kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,answer
-7077013011718311566,什么叫软间隔？,question
2966118926189366050,软间隔允许部分样本点不满足约束条件：1<y(wx+b),answer
1194541339153793871,SVM 软间隔与硬间隔表达式,question
651157089427433560,硬间隔：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93ha4y4glj3094011a9w.jpg),answer
-1138741085831283460,软间隔：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93hae5o8lj30d701f0sn.jpg),answer
8768888920756994506,SVM原问题和对偶问题的关系/解释原问题和对偶问题？,question
412237656868323419,svm原问题是：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),answer
-8015263448265112585,svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg),answer
1575443721160702081,"拉格朗日乘子法：求f的最小值时，有h=0的限制条件，那么就构造∑λh+f=Loss,作为新loss",answer
2829419979684876329,引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件,answer
-4203645338985067967,在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,answer
-3480339675499769015,为什么要把原问题转换为对偶问题？,question
-7758220282117234757,因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,answer
2187460928191890296,引入了核函数,answer
1563792230815803698,为什么求解对偶问题更加高效？,question
6607365710097697843,原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题,answer
-107980782634928542,因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0,answer
-771005139814626243,alpha系数有多少个？,question
-263974353207067379,样本点的个数,answer
3826289833614922997,KKT限制条件，KKT条件有哪些，完整描述,question
2139440831837737359,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,answer
5285909522367683995,KKT乘子λ>=0,answer
-2718670706634798544,引入拉格朗日的优化方法后的损失函数解释,question
2349705450555492122,原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),answer
4808608743555971871,优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),answer
4040269147226307734,要求KKT乘子λ>=0,answer
1924320668232142824,核函数的作用是啥,question
30966422952211519,核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,answer
-597402229902133676,核函数的种类和应用场景,question
-8638452741672808046,线性核函数：主要用于线性可分的情形。参数少，速度快。,answer
-6537799521153632397,多项式核函数：,answer
-2919879723823114324,高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,answer
-4341213865485189892,sigmoid核函数：,answer
-5690541262062388634,拉普拉斯核函数：,answer
-6957352854995049476,如何选择核函数,question
-7533371610012756660,我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,answer
-85087986214498761,常用核函数的定义？,question
-8569826871527454787,在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,answer
8217632099313991677,"1)线性：K(v1,v2)=<v1,v2>",answer
-3605544354060490852,"2)多项式：K(v1,v2)=(r<v1,v2>+c)^n",answer
-4632357843753004779,"3)Radialbasisfunction：K(v1,v2)=exp(r||v1v2||^2)",answer
1596799535765319540,"4)Sigmoid：tanh(r<v1,v2>+c)",answer
-3826943885134718335,核函数需要满足什么条件？,question
8227803919023894794,Mercer定理：核函数矩阵是对称半正定的,answer
191281974126540112,为什么在数据量大的情况下常常用lr代替核SVM？,question
-5497046391343082275,计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),answer
8738547075460278077,在使用核函数的时候参数假设全靠试，时间成本过高,answer
4562126214319942965,高斯核可以升到多少维？为什么,question
2487711593349200457,无穷维,answer
3506550017263715165,e的n次方的泰勒展开得到了一个无穷维度的映射,answer
1175454645775350238,SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,question
1746219042363926283,如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,answer
-2924458895271334959,"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",question
-5362390373700675072,线性问题：,answer
18750816511156071,线性：,answer
-6845613198903218353,逻辑回归，线性svm,answer
2856290812401617653,非线性：,answer
5267898523131746885,贝叶斯，决策树，核svm，DNN,answer
8800409819309901217,数据问题：,answer
482906123982235253,数据量大特征多：,answer
1883256835234462118,逻辑回归,answer
-6698070558437640973,决策树算法,answer
2785393976619335131,数据量少特征少：,answer
-3295651776428484082,核svm,answer
1623331394447991642,缺失值多：,answer
635392726415438033,树模型,answer
-8091475000159435417,Linear SVM 和 LR 有什么异同？,question
1551118602731765522,LR是参数模型，SVM为非参数模型。,answer
-6944996621242965092,LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,answer
-6274003717899352486,在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,answer
-5018162009755126073,LR的模型相对简单，在进行大规模线性分类时比较方便。,answer
7292522758964990174,损失函数是啥,question
-5804569688523494446,"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",answer
-4059825037259665998,最小二乘/梯度下降手推,question
-3751849076430229775,最小二乘,answer
2273359157530730369,损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93458dklvj306l011t8j.jpg),answer
-8904863828131778656,求导可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93489pnxxj3052014jr7.jpg),answer
-436669080945888301,使右侧为0可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934a6q83tj304300kdfm.jpg),answer
-5540655382902688272,如果X点乘X的转置可逆则有唯一解，否则无法如此求解,answer
-652920037202665989,梯度下降,answer
-1635371777662058443,损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg),answer
6983577592629148347,求导可得梯度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934mwtnodj307401fdfp.jpg),answer
3682875447926164231,介绍一下岭回归,question
1351675341009020455,加上l2的线性回归：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934px37g4j305b017glf.jpg),answer
1599314431588868097,在用最小二乘推导的过程和上面一样，最后在结果上进行了平滑，保证有解：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934t64beij304w00kmwy.jpg),answer
-1763514556478564295,什么时候使用岭回归？,question
1404353317828252819,样本数少，或者样本重复程度高,answer
943869738851118427,什么时候用Lasso回归？,question
4635440829892721296,特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,answer
-5458935959612185695,请问从EM角度理解kmeans?,question
7778888081797673152,kmeans是两个步骤交替进行，可以分别看成E步和M步,answer
3805596196954487513,M步中将每类的中心更新为分给该类各点的均值，可以认为是在「各类分布均为单位方差的高斯分布」的假设下，最大化似然值；,answer
-6794237936316676963,E步中将每个点分给中心距它最近的类（硬分配），可以看成是EM算法中E步（软分配）的近似,answer
-4025577193018487749,为什么kmeans一定会收敛?,question
3717594716684726098,M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,answer
5368057702790658355,kmeans初始点除了随机选取之外的方法？,question
-213991365771374,先层次聚类，再在不同层次上选取初始点进行kmeans聚类,answer
-8075053675585412150,解释一下朴素贝叶斯中考虑到的条件独立假设,question
-1578523231136735808,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92b15s6daj308x00rq2s.jpg),answer
8335465772639439017,讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,question
7349705992857594377,贝叶斯公式是完整的数学公式P(A/B)=P(A)P(B/A)/P(B),answer
-3718033369718182497,"朴素贝叶斯=贝叶斯公式+条件独立假设，在实际使用过程中，朴素贝叶斯完全只需要关注P(A,B)=P(A)P(B/A)即可",answer
5983536492917102592,朴素贝叶斯中出现的常见模型有哪些,question
-4420083417562722068,多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,answer
8025893123619724560,因为统计次数，所以会出现0次可能，所以实际中进行了平滑操作,answer
-3090593858188878680,先验平滑：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92bmactdlj303o0133yb.jpg),answer
-4442749475986120093,后验平滑：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92borlh0nj3043018q2r.jpg),answer
-4317252408770126307,两者形式非常像，区别就在先验平滑分母考虑的是平滑类别y个数，后验平滑分母考虑的是平滑特征对应特征x可选的个数,answer
8673217792886048597,高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),answer
-1676756337877911829,高斯模型假设在对应类别下的每一维特征都服从高斯分布（正态分布）,answer
-2426350383703782906,伯努利：伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次,answer
3653378141349637787,出现估计概率值为 0 怎么处理,question
7378621823551977389,拉普拉斯平滑,answer
1591291747183631056,朴素贝叶斯的优缺点？,question
-6011806804508701452,优点：对小规模数据表现很好，适合多分类任务，适合增量式训练,answer
-4423677391731956658,缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）,answer
-157709759143041438,朴素贝叶斯与 LR 区别？,question
-6379452956142858202,生成模型和判别模型,answer
5170798535152398045,条件独立要求,answer
-1909065379180483808,小数据集和大数据集,answer
-1621320528225303543,logistic分布函数和密度函数，手绘大概的图像,question
-7030987285401208705,分布函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93b9whhwuj306z01amwz.jpg),answer
7361144394821845491,密度函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93bdnikzbj306e01pjr8.jpg),answer
926237437130584589,其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,answer
8678731964829318895,LR推导，基础5连问,question
7276692563367303268,基础公式,answer
1325559008525893103,f(x)=wx+b,answer
-3582185394713195067,y=sigmoid(f(x)),answer
-8062233916340847577,可以看作是一次线性拟合+一次sigmoid的非线性变化,answer
-405077666066454411,伯努利过程,answer
5858583751550987654,对于lr来说事情只有发生和不发生两种可能，对于已知样本来说，满足伯努利的概率假设：,answer
8396290608503874308,"p(y=1/x,θ)=h(θ,x)",answer
-7057355721494834234,"p(y=0/x,θ)=1h(θ,x)",answer
9114889379650777438,"p(y/x,θ)=h(θ,x)^y·(1h(θ,x))^(1y)",answer
-8863294056844722784,第i个样本正确预测的概率如上可得,answer
3863706594744177084,几率odds,answer
6239661298779511959,数据特征下属于正例及反例的比值,answer
-6213886009376478243,ln(y/(1y)),answer
-9153736748568350885,极大似然,answer
2467550476147727783,第i个样本正确预测的概率如上可得每条样本的情况下,answer
-4369845952701003844,综合全部样本发生的概率都要最大的话，采取极大似然连乘可得：,answer
-2006192781177417227,"∏(h(θ,x)^y·(1h(θ,x))^(1y))",answer
8168576614076424505,损失函数,answer
474753740005992046,通常会对极大似然取对数，得到损失函数，方便计算,answer
2450200688016023674,"∑ylogh(θ,x)+(1y)log(1h(θ,x))最大",answer
-5416461382219012415,"及1/m·∑ylogh(θ,x)+(1y)log(1h(θ,x))最小",answer
-2054435469266421911,损失函数求偏导，更新θ,answer
1311574404523125323,θj+1=θj∆·∂Loss/∂θ=θj∆·1/m·∑x·(hy),answer
-7688866679414747244,∆为学习率,answer
6847217144928123261,梯度下降如何并行化？,question
-115968947003967554,首先需要理解梯度下降的更新公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cn8ok1fj307a01ft8k.jpg),answer
-3743859622218753181,∑处的并行，不同样本在不同机器上进行计算，计算完再进行合并,answer
5641871205178144002,同一条样本不同特征维度进行拆分，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)处并行，把![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93csl2l0pj301400ia9t.jpg)内的xi和Wi拆分成块分别计算后合并，再把外层![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)同样拆分成若干块进行计算,answer
-7080916004902884769,LR明明是分类模型为什么叫回归？,question
-9201882076943484744,观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,answer
-4870158568762298328,为什么LR可以用来做CTR预估？,question
-1709651008546647527,1.点击行为为正向，未点击行为为负向，ctr需要得到点击行为的概率，lr可以产出正向行为的概率，完美match,answer
-4873453134115888601,2.实现简单，方便并行，计算迭代速度很快,answer
-4347415889151769117,3.可解释性强，可结合正则化等优化方法,answer
2471674174583766545,满足什么样条件的数据用LR最好？,question
-5108255550561150037,特征之间尽可能独立,answer
-4733617550333035075,不独立所以我们把不独立的特征交叉了,answer
-2165501828222059592,还记得FM的思路？,answer
-2997391335675596022,离散特征,answer
-2909599796293511078,连续特征通常没有特别含义，31岁和32岁差在哪？,answer
8487161534382839976,离散特征方便交叉考虑,answer
621952182145900592,在异常值处理上也更加方便,answer
-8069809808386381547,使的lr满足分布假设,answer
1332781610267244004,什么分布假设？,answer
3294235897192417452,在某种确定分类上的特征分布满足高斯分布,answer
964024326811964075,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wh7dd6bkj310w034gmb.jpg),answer
8456638067460116104,C1和C2为正负类，观测样本中该特征在正负类中出现概率的比值满足线性条件的前提就是P服从正太分布,answer
3765832646164276279,实际中不满足的很多，不满足我们通常就离散化，oneHotEncode,answer
2472523737296390791,此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中,answer
-7757003781759996912,LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,question
2114386650507272154,思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,answer
-5232326034888711108,思路二：Exponentialmodel的形式是这样的：假设第i个特征对第k类的贡献是![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmq11x6zj300s00f3y9.jpg)，则数据点![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmqpotqfj302700imwx.jpg)属于第k类的概率正比于![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmrp4qv4j305b00i0sj.jpg)。,answer
-3017015058596164696,二分类上：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmsc1vfkj30jm036wet.jpg),answer
-4810148662699792359,化简即为sigmoid,answer
7018984752838643523,以上思路源自：PRML（PatternRecognitionandMachineLearning）,answer
-1542357230646068832,思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,answer
-3897786572137936006,利用几率odds的意义在哪？,question
-8169343840741555183,直接对分类模型进行建模，前提假设为非常弱的指定类别上自变量的条件分布满足高斯,answer
-6531524633511876279,由预测0/1的类别扩展到了预测01的概率值,answer
5235604312570399036,任意阶可导的优秀性质,answer
-1409808733128220522,Sigmoid函数到底起了什么作用？,question
-3590544315338531470,"数据规约：\[0,1]",answer
-3645582317671315758,线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,answer
4725614905534657035,sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,answer
8906248842345342278,LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,question
-7218945113434391245,更新速度只与真实的x和y相关，与激活函数无关，更新平稳,answer
-6775129870604493655,比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,answer
1060510014233164916,mse下的lr损失函数非凸，难以得到解析解,answer
4424369601647127480,LR中若标签为+1和-1，损失函数如何推导？,question
-6694571378896923824,way1:把01的sigmoid的lr结果Y映射为2y1，推导不变,answer
-7449878583960338063,"way2:把激活函数换成tanh，因为tanh的值域范围为\[1,1],满足结果，推导不变",answer
-3502717813995246996,way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,answer
8957199932218140551,如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,question
8669194847895662273,如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,answer
-7698536536272986437,每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了,answer
1985268534246928966,增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,answer
-2200375372357859020,泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,answer
323088645138332408,LR可以用核么？可以怎么用？,question
1151767015625995942,结论：可以，加l2正则项后可用,answer
-7538342241328339941,原因：,answer
4752672603833807097,核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,answer
-5289158363030631776,w拆解的z的线性组合中的系数α来源,answer
-6792017353225656491,representertheorem的证明,answer
-319610365427536370,凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,answer
-8113480932239223227,如何将将W*表示成β的形式带到我们最佳化的问题,answer
-2450995730262603836,LR中的L1/L2正则项是啥？,question
5343688664288433683,"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",answer
-93153794356436221,"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",answer
-7192062876788297093,lr加l1还是l2好？,question
-2698103042747193442,这个问题还可以换一个说法，l1和l2的各自作用。,answer
-1007207025243104926,刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,answer
2076294403995820530,正则化是依据什么理论实现模型优化？,question
-1413534557661867457,结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,answer
4449983820317555624,LR可以用来处理非线性问题么？,question
1123593457287011801,特征交叉，类似fm,answer
1548935001250149170,核逻辑回归，类似svm,answer
-1388558212753926816,线性变换+非线性激活，类似neuralnetwork,answer
1574572949193186209,为什么LR需要归一化或者取对数?,question
-4627129319075068289,**模型中对数据对处理一般都有一个标答是提升数据表达能力，也就是使数据含有的可分信息量更大**,answer
-2059510836624948951,工程角度：,answer
1552606875523802607,加速收敛,answer
-5492493214729514996,提高计算效率,answer
1690810455793980550,理论角度:,answer
2910228059689154145,梯度下降过程稳定,answer
-2366731666075921355,使得数据在某类上更服从高斯分布，满足前提假设，这个是必须要答出来的,answer
4467726170750645382,[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),answer
-3572629075688114882,为什么LR把特征离散化后效果更好？离散化的好处有哪些？,question
-9163352906828744347,原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,answer
4492557645464060473,离散后结合正则化可以进行特征筛选，更好防止过拟合,answer
-1434473929109072927,数据的鲁棒性更好，不会因为无意义的连续值变动导致异常因素的影响，（31岁和32岁的差异在哪呢？）,answer
-4315066270943787763,离散变量的计算相对于连续变量更快,answer
3399690599919985182,逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,question
7400294031174713277,lr的output是彼此之间相对谁的可能性更高，而不是概率，概率是事情发生的可能，lr的output不代表可能,answer
5814873099509455941,LR对比万物？,question
-8261904098845317603,lr和线性回归,answer
-7491394798121864278,lr解用的极大似然，线性回归用的最小二乘,answer
7413906960767381227,lr用于分类，线性回归用于回归,answer
-2576606073155692561,但两者都是广义线性回归GLM问题,answer
329952601291063302,两者对非线性问题的处理能力都是欠佳的,answer
-1510946430418513866,lr和最大熵,answer
1546491892119530707,在解决二分类问题是等同的,answer
-1575923485906616988,lr和svm,answer
5790979953085366779,都可分类，都是判别式模型思路,answer
-6067501674729427292,通常都是用正则化进行规约,answer
2911208649142820611,模型上,answer
3609684028846725206,lr是交叉熵，svm是HingeLoss,answer
5940771867795833594,lr是全量数据拟合，svm是支持向量拟合,answer
4951038874842762548,lr是参数估计有参数的前提假设，svm没有,answer
-2479842690712438745,lr依赖的是极大似然，svm依赖的是距离,answer
-463854441417084802,lr和朴素贝叶斯,answer
6659219338195227557,如果朴素贝叶斯也有在某一类上的数据x满足高斯分布的假设前提，lr和朴素贝叶斯一致,answer
5864617280919302908,lr是判别模型，朴素贝叶斯是生成模型,answer
-7186570903542231646,lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,answer
-3353066765195317949,lr和最大熵模型,answer
-5709436343599756086,本质没有区别,answer
-300011383432758190,最大熵模型在解决二分类问题就是逻辑回归,answer
-343326703928627891,最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,answer
4152923664901975954,LR梯度下降方法？,question
6786592848109475116,随机梯度下降,answer
5706939349852271308,局部最优解，可跳出鞍点,answer
6603844909086664730,计算快,answer
5952963119080418352,批梯度下降,answer
-5433141029114252904,全局最优解,answer
6126578657633570134,计算量大,answer
8962888048320760995,mini批梯度下降,answer
-537787591387042765,综合以上两种方法,answer
1690401985080485017,除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,answer
5767313111709880034,LR的优缺点？,question
-8247241443253765941,优点,answer
3559887948273443082,简单，易部署，训练速度快,answer
8175452868820403317,模型下限较高,answer
-7446586616494098415,缺点,answer
-6760045362746772268,只能线性可分,answer
9163906665405143534,数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),answer
-4841668292042563042,模型上限较低,answer
-1472632791752415108,除了做分类，你还会用LR做什么？,question
7994504569905156727,特征筛选，特征的系数决定该特征的重要性,answer
90483282436493484,你有用过sklearn中的lr么？你用的是哪个包？,question
7646410595010654448,sklearn.linear_model.LogisticRegression,answer
7444036304760007087,看过源码么？为什么去看？,question
8792400497739962517,看部分参数的解释,answer
1198087271582343524,比如dual、weight_class中的1:0还是0:1比,answer
-7067520576935515368,比如输出值的形式，输出的格式,answer
-293728696968888279,谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,question
2770020687323549829,penalty是正则化，solver是函数优化方法,answer
8640388638689685488,penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,answer
-7049697845695046611,牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,answer
-8063664110572521944,l1和l2选择参考上面讲的正则化部分,answer
-449081222727680978,随机梯度下降在数据较少的时候最好别用，但是速度比较快。默认的是坐标轴下降法,answer
-2184369481875314401,谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,question
-3342038775522003731,首先，决定是否为多分类的参数是multi_class,answer
2648847896856304993,在二分类的时候，multi和ovr和auto都是一样的,answer
-1289980229328525193,在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,answer
8838718631676202714,我的总结,question
-7634397203284293917,逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,answer
-1617696153851765881,逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,answer
-4816294200236279411,逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,answer
8984899259968758069,逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,answer
3457472234156786560,解释下随机森林?,question
5604037146911418184,随机森林=bagging+决策树,answer
6310942231241405097,随机：特征选择随机+数据采样随机,answer
-8701521948354854625,特征随机是在决策树**每个结点上选择的时候随机**，并不是在每棵树创建的时候随机,answer
-1428868071210895985,每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,answer
1251759023929602669,数据采样，是有放回的采样,answer
6965812184916476980,1个样本**未被选到**的概率为p=(11/N)^N=1/e，即为OOB,answer
6363309005031557400,森林：多决策树组合,answer
814689532637995658,可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,answer
-720561751833233780,随机森林用的是什么树？,question
-8819770360431899466,CART树,answer
7364294417629832664,随机森林的生成过程？,question
-969389405252300747,生成单棵决策树,answer
8232473844503551563,随机选取样本,answer
4446401594273568551,从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,answer
-2263676877697891595,不需要剪枝，直到该节点的所有训练样例都属于同一类,answer
2228952839091103086,生成若干个决策树,answer
959650187573633286,解释下随机森林节点的分裂策略？,question
2325112970546746515,Gini系数,answer
7300019686604542547,在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),answer
-4314487982422408384,随机森林的损失函数是什么？,question
6343447036040247414,"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",answer
-5636596653276447657,回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,answer
777010694337186859,参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),answer
2907524675914282712,为了防止随机森林过拟合可以怎么做?,question
-1244332928910674160,增加树的数量,answer
7223969639467587455,增加叶子结点的数据数量,answer
5748430255615931624,bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),answer
-3027818699604692305,随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,answer
-8012867296720321300,随机森林特征选择的过程？,question
-3407807201211090823,特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,answer
7512038410190762160,通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）,answer
1088971722462478172,是使用uniform或者gaussian抽取随机值替换原特征,answer
4577854582636688420,是否用过随机森林，有什么技巧?,question
-7173373136759018180,除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,answer
4869456835100578771,RF的参数有哪些，如何调参？,question
994848512447897673,要调整的参数主要是n_estimators和max_features,answer
-1561123216824695610,n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,answer
-5949101718411635508,max_features是分割节点时考虑的特征的随机子集的大小。这个值越低，方差减小得越多，但是偏差的增大也越多,answer
-5392066380383064375,回归：max_features=n_features,answer
-6033350236335486962,分类：max_features=sqrt(n_features),answer
1326101445497124119,其他参数中,answer
-5269126157152819960,class_weight也可以调整正负样本的权重,answer
7531518014627074567,max_depth=None和min_samples_split=2结合，为不限制生成一个不修剪的完全树,answer
-1305641494973048667,RF的优缺点 ？,question
8172233986741259571,优点:,answer
3434409125881614335,不同决策树可以由不同主机并行训练生成，效率很高,answer
2830089450041154997,随机森林算法继承了CART的优点,answer
-11789479582262826,将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题,answer
217297141493856565,没有严格数学理论支持,answer
-6713026365223149001,介绍一下Boosting的思想？,question
2151760194341391847,初始化训练一个弱学习器，初始化下的各条样本的权重一致,answer
-3058491991003419440,根据上一个弱学习器的结果，调整权重，使得错分的样本的权重变得更高,answer
-3644033395094241052,基于调整后的样本及样本权重训练下一个弱学习器,answer
193934836125493599,预测时直接串联综合各学习器的加权结果,answer
3217112944445015489,最小二乘回归树的切分过程是怎么样的？,question
4352083290165059198,回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值,answer
3644688330232414445,分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性,answer
885848427163569291,属性上有多个值，则需要遍历所有可能的属性值，挑选使平方误差最小的划分属性值作为本属性的划分值,answer
-7798554608216387850,递归重复以上步骤，直到满足叶子结点上值的要求,answer
-8186455828990489437,有哪些直接利用了Boosting思想的树模型？,question
-8451986298671435940,adaboost，gbdt等等,answer
-4541507103429731168,gbdt和boostingtree的boosting分别体现在哪里？,question
7011887335279787184,boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,answer
996232814405966697,gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,answer
8271663902735727397,gbdt的中的tree是什么tree？有什么特征？,question
2123589007399146093,Carttree，但是都是回归树,answer
2169014031608736073,常用回归问题的损失函数？,question
-3758029269504595787,mse:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bmopzfjj303s011t8i.jpg),answer
9098025136733590306,负梯度：yh(x),answer
-1747000544213282037,初始模型F0由目标变量的平均值给出,answer
-5874169616649414378,绝对损失:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bne7cipj303400q742.jpg),answer
345727015848886435,负梯度：sign(yh(x)),answer
-7424860473405534472,初始模型F0由目标变量的中值给出,answer
2995004458395664230,Huber损失：mse和绝对损失的结合,answer
-3602648821203030772,负梯度：yh(x)和sign(yh(x))分段函数,answer
4791713194295433604,它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,answer
7647224743603935436,常用分类问题的损失函数？,question
5721741066360513135,对数似然损失函数,answer
1848269756604329825,"二元且标签y属于{1,+1}：𝐿(𝑦,𝑓(𝑥))=𝑙𝑜𝑔(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥)))",answer
-8435480527625909375,负梯度：y/(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥))),answer
2806635905760552619,多元：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94cj1yce6j306x01iglg.jpg),answer
-8139788539607909567,"指数损失函数:𝐿(𝑦,𝑓(𝑥))=𝑒𝑥𝑝(−𝑦𝑓(𝑥))",answer
3266758441465791196,负梯度：y·𝑒𝑥𝑝(−𝑦𝑓(𝑥)),answer
-183578546482086541,除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,answer
4547317486145313552,什么是gbdt中的损失函数的负梯度？,question
7380149387739833139,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg),answer
2096406320323132008,当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,answer
7407973754872079622,如何用损失函数的负梯度实现gbdt？,question
4022503365066089419,"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",answer
-8977241300540467062,构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),answer
-1396287389485857371,"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值𝑐𝑡𝑗,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",answer
-7211259391871506066,首先，根据feature切分后的损失均方差大小，选取最优的特征切分,answer
6207510642185919779,其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,answer
5467241012480279101,这样就完整的构造出一棵树：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bfr5cn5j303d01kjr6.jpg),answer
4981408192932956137,本轮最终得到的强学习器的表达式如下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94binx5prj307o01k0sl.jpg),answer
-6216507185336252533,拟合损失函数的负梯度为什么是可行的？,question
1512754218066675304,泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg),answer
6527210027454507179,m轮树模型可以写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h8zovulj305v00iglf.jpg),answer
1483647528200266543,"对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)",answer
6992038026503320031,"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",answer
-6690634318561252684,即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,question
5902893086070323784,前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（𝛽）,answer
-6569326120811633434,依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,answer
-6735274451600721928,Shrinkage收缩的作用？,question
2240441931893175141,每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,answer
7599957388800506462,原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94i9bokjzj306g00iglf.jpg),answer
2325404445086364265,Shrinkage：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94iawlfq3j307600it8j.jpg),answer
-888844062452958892,feature属性会被重复多次使用么？,question
-1533533631019131700,会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,answer
-4295190464207441703,gbdt如何进行正则化的？,question
-3992286405431389909,子采样,answer
5418983909337669564,每一棵树基于原始原本的一个子集进行训练,answer
-1848603848266545084,rf是有放回采样，gbdt是无放回采样,answer
-6210422837831950567,特征子采样可以来控制模型整体的方差,answer
6339497663858927206,利用Shrinkage收缩，控制每一棵子树的贡献度,answer
-4583594204732682940,每棵Cart树的枝剪,answer
3603032335969691310,为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,question
-3328414256154939902,对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受,answer
-311034561494343035,bagging，关注于提升分类器的泛化能力,answer
-6386456051154004152,boosting，关注于提升分类器的精度,answer
-92770415802950975,gbdt的优缺点？,question
-4612554639262738915,数据要求比较低，不需要前提假设，能处理缺失值，连续值，离散值,answer
-9161273175127650317,使用一些健壮的损失函数，对异常值的鲁棒性非常强,answer
9171618381948489570,调参相对较简单,answer
1296141068627284800,gbdt和randomforest区别？,question
-9131345546594215539,相同：,answer
-6636272055497492874,都是多棵树的组合,answer
-7705468336263290108,不同：,answer
2994403247605914660,RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,answer
-6000712551215690461,gbdt对异常值比rf更加敏感,answer
-7921767406478803920,gbdt是串行，rf是并行,answer
4797846228720983676,gbdt是cart回归树，rf是cart分类回归树都可以,answer
3458508960340265649,gbdt是提高降低偏差提高性能，rf是通过降低方差提高性能,answer
-8194943225902631600,gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,answer
5736885911562435075,GBDT和LR的差异？,question
2959556599942636190,从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,answer
-909417745965305875,当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,answer
-8696487158375855524,XGboost缺点,question
-1340027156211865586,每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,answer
524235127143864497,预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,answer
-1081349253820875682,levelwise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数,answer
-7638600315835258656,LightGBM对Xgboost的优化,question
-1887941612980716085,将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,answer
1553017484804118141,优点：时间开销由O(features)降低到O(bins),answer
8824388306725101184,缺点：很多数据精度被丢失，相当于用了正则,answer
4651212871111491789,利用leafwise代替levelwise,answer
-7560601190921410104,每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环,answer
508085270897924517,直方图做差加速,answer
2517979825143079965,LightGBM亮点,question
-1847382946067444426,单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,answer
-8232927916916998770,"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",answer
-1413180564419976746,xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,question
3946960066134847235,显示的把树模型复杂度作为正则项加到优化目标中,answer
-54276782002095053,优化目标计算中用到二阶泰勒展开代替一阶，更加准确,answer
-5088659805646802963,实现了分裂点寻找近似算法,answer
777009821392775892,暴力枚举,answer
5523441101274119809,近似算法（分桶）,answer
-7250010112500430915,更加高效和快速,answer
-7823060384166663786,数据事先排序并且以block形式存储，有利于并行计算,answer
487039368410267957,基于分布式通信框架rabit，可以运行在MPI和yarn上,answer
-3425230066149114602,实现做了面向体系结构的优化，针对cache和内存做了性能优化,answer
5257212214704243014,xgboost和gbdt的区别？,question
-1405254358955577375,模型优化上：,answer
-606536575597935962,基模型的优化：,answer
2011836704093514484,gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),answer
4721142044458892385,损失函数上的优化：,answer
-3927316354014290380,gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,answer
-7432794558083878403,gbdt没有在loss中带入结点个数和预测值的正则项,answer
2317119480947119179,特征选择上的优化：,answer
-2954062784950269133,实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,answer
-5645920210791303411,节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权,answer
6984744805053947133,正则化的优化：,answer
-5627885833458664903,特征采样,answer
-4745839965830856963,样本采样,answer
-8239726427440994894,工程优化上：,answer
-575170222970198863,xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,answer
1304545841207685916,"cacheaware,outofcorecomputation",answer
2339465197781076630,支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit,answer
-2345375797526860574,xgboost优化目标/损失函数改变成什么样？,question
885140658460465477,原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mjezeisj307401fmx0.jpg),answer
-2521510908529200572,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒一阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mrtqxv0j30480173yc.jpg),answer
6535278750903404180,改变：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mldvhz5j30ay01k3yf.jpg),answer
4207362395076194184,J为叶子结点的个数，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mo56g1vj300o00e0s1.jpg)为第j个叶子结点中的最优值,answer
2808563246090494692,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒二阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mtjds7qj309r0193yg.jpg),answer
7427530430615294308,xgboost如何使用MAE或MAPE作为目标函数？,question
-5944542552993782657,MAE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mxhhvg8j303l011q2q.jpg),answer
-5304935451334407007,MAPE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mx7uyuej303f0170sj.jpg),answer
-1572786253760426062,利用可导的函数逼近MAE或MAPE,answer
-6995460333962411879,mse,answer
2882974504505822139,Huberloss,answer
-6352040305960513390,PseudoHuberloss,answer
7743286908852770299,xgboost如何寻找分裂节点的候选集？,question
-6594416822584336960,法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大,answer
-6989442486126235845,近似算法（approx）,answer
7001900072917951752,近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,answer
4560640911157169741,离散值直接分桶,answer
2309668524135869164,连续值分位数分桶,answer
6721072063010527766,xgboost如何处理缺失值？,question
-7258625789083947702,训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个,answer
-2134143667324994025,预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,answer
6122944399466608148,xgboost在计算速度上有了哪些点上提升？,question
-9161733885692297063,特征预排序,answer
5646475598183038015,按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,answer
638810667735789104,block可以仅存放样本的索引，而不是样本本身，这样节省了大量的存储空间,answer
6709901722341711181,xgboost特征重要性是如何得到的？,question
8076370721004852422,’weight‘：代表着某个特征被选作分裂结点的次数；,answer
-2807242395824365567,’gain‘：使用该特征作为分类结点的信息增益；,answer
-8347674708279385496,’cover‘：某特征作为划分结点，覆盖样本总数的平均值；,answer
-4154296355484597751,XGBoost中如何对树进行剪枝？,question
-7793256744604968366,在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,answer
5072625942970159467,在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,answer
-6563960512849072924,当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂,answer
4916816380629223601,XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,answer
-8323298060262963658,XGBoost模型如果过拟合了怎么解决？,question
-7395491088050709948,直接修改模型：,answer
2107147197463183846,降低树的深度,answer
1086337134417075447,增大叶子结点的权重,answer
7398230438493936725,增大惩罚系数,answer
-5063622658760967715,subsample的力度变大，降低异常点的影响,answer
-8625461139232724681,减小learningrate，提高estimator,answer
-137493320226270400,xgboost如何调参数？,question
-5895415019161651800,先确定learningrate和estimator,answer
-1516726132685030822,再确定每棵树的基本信息，max_depth和min_child_weight,answer
-7090112566873340155,再确定全局信息：比如最小分裂增益，子采样参数，正则参数,answer
3683116332631528844,重新降低learningrate，得到最优解,answer
685542938721678497,Attention对比RNN和CNN，分别有哪点你觉得的优势？,question
-6029492968115128508,对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,answer
6334783359150329425,对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序,answer
-3772212624821300031,写出Attention的公式？,question
6399422161015311893,![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986pbyaj0j308t019t8l.jpg),answer
6394746321134661083,解释你怎么理解Attention的公式的？,question
-3787109930511441394,"Q:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986rjgs7qj301400g741.jpg),K:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986rz9d1ej301a00g741.jpg),V:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986shjz4tj301900g741.jpg)",answer
-4868623375564414024,首先，我们可以理解为Attention把input重新进行了一轮编码，获得一个新的序列,answer
2126815945357957294,除以![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98734eqn7j300y00na9t.jpg)的目的是为了平衡qk的值，避免softmax之后过小,answer
-5737504734095522111,qk除了点击还可以直接拼接再内接一个参数变量等等,answer
-8114529202553546710,MultiAttention只是重复了h次的Attention，最后把结果进行拼接,answer
5884343234876956525,Attention模型怎么避免词袋模型的顺序问题的困境的？,question
4788000955712193334,增加了positionEmbedding,answer
8810203207211857794,可以直接随机初始化,answer
-3623613899444043897,也可以参考Google的sin/cos位置初始化方法,answer
-76613722507758889,如此选取的原因之一是sin(a+b)=sin(a)cos(b)+cos(a)sin(b)。这很好的保证了位置p+k可以表示成p的线性变换，相对位置可解释,answer
-8460855244483945202,"Attention机制，里面的q,k,v分别代表什么？",question
8012274854562057053,Q：指的是query，相当于decoder的内容,answer
7148382531585817701,K：指的是key，相当于encoder的内容,answer
3608028186593309448,V：指的是value，相当于encoder的内容,answer
-2321199947733891401,q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,answer
-2717954443654824358,为什么self-attention可以替代seq2seq？,question
-8350899345908770163,seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,answer
-6564921958495320259,selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,answer
2793417592778741965,维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,question
157415903110820279,假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,answer
-698812980206689001,"针对Q和K中的每一维i都有qi和ki相互独立且均值0方差1，不妨记![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envjoy8oj301h00gdfl.jpg),![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envuw3p0j301g00ga9t.jpg)",answer
6976943543925430942,E(XY)=E(X)E(Y)=0,answer
6372274332278365214,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9enzh4vzvj30gh017t8t.jpg),answer
3439660181026318386,所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,answer
5850315929257486522,你觉得bn过程是什么样的？,question
334067165438034227,按batch进行期望和标准差计算,answer
4715075719265174944,对整体数据进行标准化,answer
-2442603935654868930,对标准化的数据进行线性变换,answer
2874080617677372275,变换系数需要学习,answer
-6106456137021952792,手写一下bn过程？,question
-2129660946345083463,"mu=1.0*np.sum(X,axis=0)/X.shape\[0]",answer
-6301510856842985327,Xmu=Xmu,answer
5740741869266648229,sq=Xmu**2,answer
2870246544927608271,"var=1.0*np.sum(sq,axis=0)/X.shape\[0]",answer
-1046023628431119050,out=alhpa*(XXmu)/np.sqrt(var+eps)+beta,answer
3318883804551152952,知道LN么？讲讲原理,question
1595186332018930721,和bn过程近似，只是作用的方向是在维度上，而不是batch上,answer
-5806437631399157994,这样做的好处就是不会受到batch大小不一致的影响,answer
5352771766437092433,介绍残差网络,question
-5354034898347912313,常见结构，CV里面用的比较多,answer
5048418155818830990,y=F(x)+x,answer
8843383368431755747,y=F(x)+indentity`*`x,answer
8347484653458290491,残差网络为什么能解决梯度消失的问题,question
-5989993916109267122,![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is4x344xj304j01jglf.jpg),answer
-3457893637152398973,![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8cigikj305s0180sl.jpg),answer
7043594907874554138,虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,answer
3140408265116683860,残差网络残差作用,question
-6692906471247694242,防止梯度消失,answer
-4155941423787878735,恒等映射使得网络突破层数限制，避免网络退化,answer
751140406791631443,对输出的变化更敏感,answer
2157306269519753320,X=5;F(X)=5.1;F(X)=H(X)+X=>H(X)=0.1,answer
-268210878380503061,X=5;F(X)=5.2;F(X)=H(X)+X=>H(X)=0.2,answer
-3869783192432796682,H(X)变换了100%，去掉相同的主体部分，从而突出微小的变化,answer
1146261786643479240,你平时有用过么？或者你在哪些地方遇到了,question
-8073502929281068765,我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络,answer
8705540711787058803,Bert和Transform中attention部分残差网络用的比较频繁,answer
7749703738286082738,Bert的双向体现在什么地方？,question
8503737107461566862,mask+attention，mask的word结合全部其他encoderword的信息,answer
8092706369014363827,Bert的是怎样实现mask构造的？,question
-8345472148677044102,MLM：将完整句子中的部分字mask，预测该mask词,answer
-4039939807678053816,NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,answer
-4172882578176193188,在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,question
-8222795955897132517,mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,answer
-5048538901759046063,随机替换也帮助训练修正了\[unused]和\[UNK],answer
-3873599747934072555,强迫文本记忆上下文信息,answer
1105967358483539657,为什么BERT有3个嵌入层，它们都是如何实现的？,question
6707940889025341227,input_id是语义表达，和传统的w2v一样，方法也一样的lookup,answer
-8047342118322388,"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",answer
-2185921686538033862,"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",answer
-6193320793430139870,bert的损失函数？,question
3687505302580345979,"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",answer
5377719400470667146,"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",answer
-338240186398437735,MLM+NSP即为最后的损失,answer
-8038428051305772111,手写一个multi-head attention？,question
2430637862811535933,"tf.multal(tf.nn.softmax(tf.multiply(tf.multal(q,k,transpose_b=True),1/math.sqrt(float(size_per_head)))),v)",answer
7793405042212844466,长文本预测如何构造Tokens？,question
-6836070110415022975,headonly：保存前510个token（留两个位置给\[CLS]和\[SEP]）,answer
-395808199285592428,tailonly：保存最后510个token,answer
-8864331585340776277,head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,answer
3901623841389693960,你用过什么模块？bert流程是怎么样的？,question
-484330753847700319,modeling.py,answer
3018139377696090681,"首先定义处理好输入的tokens的对应的id作为input_id,因为不是训练所以input_mask和segment_id都是采取默认的1即可",answer
1805333164428978716,在通过embedding_lookup把input_id向量化，如果存在句子之间的位置差异则需要对segment_id进行处理，否则无操作；再进行position_embedding操作,answer
-5759217147238184947,进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,answer
-7334815938223813692,输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],answer
7289441108158517653,知道分词模块：FullTokenizer做了哪些事情么？,question
5956505400099933660,BasicTokenizer：根据空格等进行普通的分词,answer
-6851552413157481886,包括了一些预处理的方法：去除无意义词，跳过'\t'这些词，unicode变换，中文字符筛选等等,answer
-9046335907753305768,WordpieceTokenizer：前者的结果再细粒度的切分为WordPiece,answer
-1315003113360649251,中文不处理，因为有词缀一说：解决OOV,answer
6074490763632422684,Bert中如何获得词意和句意？,question
5590563389899860264,get_pooled_out代表了涵盖了整条语句的信息,answer
-3512104561025622209,get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,answer
-4761335500544651764,源码中Attention后实际的流程是如何的？,question
-4740746469470564716,Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,answer
4333778079063500836,所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output,answer
-5862589344574037256,为什么要在Attention后使用残差结构？,question
1988425679170701732,残差结构能够很好的消除层数加深所带来的信息损失问题,answer
-29866706903589696,平时用官方Bert包么？耗时怎么样？,question
-4339265670218990417,第三方：bert_serving,answer
-3198917157216310065,官方：bert_base,answer
6203821531402352439,耗时：64GTesla，64max_seq_length，8090doc/s,answer
-6749342127688717625,在线预测只能一条一条的入参，实际上在可承受的计算量内batch越大整体的计算性能性价比越高,answer
-2618111408138607410,你觉得BERT比普通LM的新颖点？,question
-3352135105119248905,mask机制,answer
-8434222964044213468,next_sentence_predict机制,answer
904425011346374439,elmo、GPT、bert三者之间有什么区别？,question
-6570665020737695308,特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,answer
-2306789684836389556,单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,answer
-6087644301307919549,GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,answer
6919300599551906583,阐述CRF原理？,question
2315679569313347055,"首先X,Y是随机变量，P(Y/X)是给定X条件下Y的条件概率分布",answer
6404094529888424016,如果Y满足马尔可夫满足马尔科夫性，及不相邻则条件独立,answer
-7052827991654242336,则条件概率分布P(Y|X)为条件随机场CRF,answer
4697574029053143104,线性链条件随机场的公式是？,question
-595614978562736712,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9ah66y1nxj30cr015747.jpg),answer
-2247227996089055319,CRF与HMM区别?,question
-384374479730449606,"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",answer
8138085548285400933,CRF是无向图，HMM是有向图,answer
2451721076093545878,CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,answer
-4442266505504728490,Bert+crf中的各部分作用详解？,question
-7389561683487327888,Bert把中文文本进行了embedding，得到每个字的表征向量,answer
-1849702053140386254,dense操作得到了每个文本文本对应的未归一化的tag概率,answer
-5196994842471248899,CRF在选择每个词的tag的过程其实就是一个最优Tag路径的选择过程,answer
5183068251367710983,CRF层能从训练数据中获得约束性的规则,answer
-1517215760983709548,比如开始都是以xxxB，中间都是以xxxI，结尾都是以xxxE,answer
7164267134841341082,比如在只有label1I，label2I..的情况下，不会出现label1B,answer
6679046579282266511,GolVe的损失函数？,question
738862680619219740,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9achsr4agj3094016a9x.jpg),answer
-2514089970826932668,解释GolVe的损失函数？,question
2915533539599444963,"其实，一句话解释就是想构造一个向量表征方式，使得向量的点击和共现矩阵中的对应关系一致。因为共现矩阵中的对应关系证明了，存在i，k，j三个不同的文本，如果i和k相关，j和k相关，那么p(i,j)=p(j,k)近似于1，其他情况都过大和过小。",answer
-1521841095746814138,为什么GolVe会用的相对比W2V少？,question
-4787524336470064214,GloVe算法本身使用了全局信息，自然内存费的也就多一些,answer
-3524456276349466125,公现矩阵，NXN的，N为词袋量,answer
248474900968588189,W2V的工程实现结果相对来说支持的更多，比如most_similarty等功能,answer
7628830005540678905,如何处理未出现词？,question
7273599574384128132,"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",answer
-6865177864270698996,详述LDA原理？,question
-3917697791679234430,从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg),answer
3503584312409299017,多项式分布的共轭分布是狄利克雷分布,answer
-868243676561387094,二项式分布的共轭分布是Beta分布,answer
1416485219622284508,从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),answer
-6778341897232089032,从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg),answer
-8292368996033050681,从词语的多项式分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)中采样最终生成词语![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8uydisuj300n00e0s0.jpg),answer
-9024046853406506244,文档里某个单词出现的概率可以用公式表示：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b9y6avdtj306e01jdfo.jpg),answer
2482760535166423228,采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,answer
3633882422402913072,LDA中的主题矩阵如何计算?词分布矩阵如何计算？,question
-1768727814469844219,这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点,answer
-8475877828842937841,吉布斯采样,answer
7242816679967777561,	先随机给每个词附上主题,answer
-8641481474330237802,	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布,answer
6903676844458848439,	有了联合概率分布，去除词wi后，就可以得到其他词主题条件概率分布,answer
2235857676719071726,	根据条件概率分布使用坐标轮换的吉布斯采样方法，得到词对应的平稳矩阵及词对应的主题,answer
5340712624610836841,	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布,answer
7428466319683647830,通常会引申出如下几个问题：,answer
-8452991003512242440,	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）,answer
73351904701824623,	MCMC中什么叫做蒙特卡洛方法？,answer
-2536924451187707389,		通常用于求概率密度的积分,answer
-1401041447631733948,		用已知分布去评估未知分布,answer
-2674749875774174016,		rejectacpect过程,answer
-1975387834810933614,	马尔科夫链收敛性质？,answer
552341306390329855,		非周期性，不能出现死循环,answer
6178231140918184899,		连通性，不能有断点,answer
-6181881140596645756,	MCMC中什么叫做马尔科夫链采样过程？,answer
914669980480754671,		先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵,answer
8004204684151584305,"		再根据平稳矩阵后的条件概率p(x/xt)得到平稳分布的样本集(xn+1,xn+2...)",answer
-7509979437810971733,	给定平稳矩阵如何得到概率分布样本集？,answer
7687185136924715058,		MC采样,answer
208914430559888077,"			给定任意的转移矩阵Q，已知π(i)p(i,j)=π(j)p(j,i)，近似拟合π(i)Q(i,j)a(i,j)=π(j)Q(j,i)a(j,i)",answer
8439118934826696207,			根据Q的条件概率Q(x/xt)得到xt+1,answer
-2429310374194182430,			u~uniform,answer
1887930141083581746,"			u<π(xt+1)Q(xt+1,xt)则accept，就和蒙特模拟一样否则xt+1=xt",answer
-2189141914038695374,"			(xt,xt+1...)代表着我们的分布样本集",answer
2363834490234630801,		MH采样,answer
361194174938752259,"			左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度",answer
4014499282380210370,		Gibbs采样,answer
237451261548435879,			同上，差别在固定n−1个特征在某一个特征采样及坐标轮换采样,answer
-834250896597720884,	什么叫做坐标转换采样？,answer
2266871893888906924,		平面上任意两点满足细致平稳条件π(A)P(A>B)=π(B)P(B>A),answer
-711167227532561743,		从条件概率分布P(x2|x(t)1)中采样得到样本x(t+1)2,answer
5984224771840930716,		从条件概率分布P(x1|x(t+1)2)中采样得到样本x(t+1)1,answer
7638571152447129133,		其为一对样本，有点像Lasso回归中的固定n1维特征求一维特征求极值的思路,answer
5717845996098317987,变分推断EM算法,answer
1467687898122208968,	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta,answer
1201981695810916999,	变分推断在于隐藏变量没法直接求，**用三个独立分布的变分分步去拟合三个隐藏变量的条件分布**,answer
252138464075484309,		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵,answer
1233218144866018243,	EM过程,answer
2233033313202542037,		E：最小化相对熵，偏导为0得到变分参数,answer
2676560845528253876,		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值,answer
3421405992953458753,LDA的共轭分布解释下?,question
841348660512245391,以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,answer
3792484455315323022,PLSA和LDA的区别?,question
3785953755055964505,LDA是加了狄利克雷先验的PLSA,answer
-5209677385032900436,PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,answer
-357817260641774542,LDA是贝叶斯思想，PLSA是MLE,answer
-1426709353512395549,怎么确定LDA的topic个数,question
8822478400933950979,对文档d属于哪个topic有多不确定，这个不确定程度就是Perplexity,answer
-3630006480246478812,多次尝试，调优perplexitytopicnumber曲线,answer
1941237476008651401,困惑度越小，越容易过拟合,answer
2030559341605983294,某个词属于某个主题的困惑度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b7zjns8uj305i012jr7.jpg)，某个文章的困惑度即为词的连乘：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b83z3d22j304q01dweb.jpg),answer
-1940472869650301421,LDA和Word2Vec区别？LDA和Doc2Vec区别？,question
8758193324339754024,LDA比较是doc，word2vec是词,answer
-68045427621472817,LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,answer
-2313439937967298624,LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,answer
3494030212527975645,LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,answer
-5520952636719254884,LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,question
-8448581131573069236,通常alpha为1/k，k为类别数，beta一般为0.01,answer
-7963708007231023419,alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,answer
7813774129379113940,beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,answer
8914201417838163592,chucksize大一些更新的过程比较平稳，收敛更加平稳,answer
192700772596039753,迭代次数一般不超过2000次，200万doc大约在2300次收敛,answer
-6747640770528047698,从隐藏层到输出的Softmax层的计算有哪些方法？,question
3681732212945117996,层次softmax,answer
-8897806460551214430,层次softmax流程？,question
-3131360937295356931,构造HuffmanTree,answer
435099822219687934,最大化对数似然函数,answer
7028745665111891855,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9adl5ex45j305g00qmwz.jpg),answer
-7509832989397941278,输入层：是上下文的词语的词向量,answer
4480887838482486203,投影层：对其求和，所谓求和，就是简单的向量加法,answer
-86140118095292551,输出层：输出最可能的word,answer
8187225765851324733,沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,answer
-3575447656655762957,对每层每个变量求偏导，参考sgd,answer
-622106343094208141,负采样流程？,question
6547621929582050041,统计每个词出现对概率，丢弃词频过低对词,answer
-97833868467656714,每次选择softmax的负样本的时候，从丢弃之后的词库里选择（选择是需要参考出现概率的）,answer
3731133849460140988,负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,answer
-4308638752163780406,word2vec两种方法各自的优势?,question
-7300423080816892550,**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,answer
1971760253436814510,"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",answer
-9017716685579033867,怎么衡量学到的embedding的好坏?,question
-640659627783100294,从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,answer
5590647825971812999,对item2vec得到的词向量进行聚类或者可视化,answer
-4641362782713787618,word2vec和glove区别？,question
7991620161706272222,word2vec是基于邻近词共现，glove是基于全文共现,answer
-732139742762985983,word2vec利用了负采样或者层次softmax加速，相对更快,answer
-7959420856504164546,glove用了全局共现矩阵，更占内存资源,answer
3449236447056795378,word2vec是“predictive”的模型，而GloVe是“countbased”的模型,answer
4689828508958017411,你觉得word2vec有哪些问题？,question
5144953882857534683,没考虑词序,answer
7705636177378752214,对于中文依赖分词结果的好坏,answer
-6436299131626079121,新生词无法友好处理,answer
8218386351988267310,无正则化处理,answer
-8050061525498490123,AutoML,feature
-8324501489036188249,特征,feature
-5544936238224460059,优化,feature
-497142214195286277,参数,feature
5053257191497025668,搜索,feature
-4761497587328816659,贝叶斯公式,feature
-6701626840926550108,训练,feature
1530371015378611453,机器学习,feature
3062047372954099770,Boosting,feature
-2825097708828868156,深度,feature
8576460167107986689,复杂度,feature
-1433498185418334419,判别,feature
-917562806630305415,CRF,feature
-1032172441593368807,EM,feature
-6199710015744940582,熵,feature
3313861085377373548,混合,feature
7137913124976279461,估计,feature
2022440307965688228,MLE,feature
-2617957453102320029,MAP,feature
-4659686796258077472,DNN,feature
6731005953396241523,DeepFM,feature
-6258994298214380066,deepFM,feature
-1812071868249962512,欠拟合,feature
5254495007856854500,过拟合,feature
-6741395181858223552,长文本,feature
-2570022855037340071,deepfm,feature
-7129859232605710422,初始化,feature
-1907053954427609598,activation,feature
5741896736266000154,unit,feature
-8790052853973851314,DICE,feature
2118193316049258487,XDeepFm,feature
-6516728328131657484,DCN,feature
2986207403495252354,向量,feature
4798936937250633058,softmax,feature
-2285426121254952702,RNN,feature
1653626481180219336,YouTube,feature
4261145284398935743,预测,feature
5442415327445366170,feature,feature
-4353976871388024009,留一法,feature
6914463341579554079,random,feature
7956899294149627304,导数,feature
-3677777004610155524,函数,feature
1384077669967274667,切线,feature
-3481888188119001142,法线,feature
5132435207599912693,拉格朗日中值,feature
7681837270315442428,拉普拉斯,feature
-4511918021189912011,泊松,feature
6333458813985191308,迭代,feature
-4364678691881594051,特征值,feature
5677100055665439291,分解,feature
-3680560783822261713,统计,feature
7361970251802995346,矩阵分解,feature
7663063773896674555,聚类,feature
3611585546884456393,平衡,feature
-3969688204381287574,原始数据,feature
243725613885631344,标准化,feature
-1226713519944303700,离散,feature
-862098348692910997,增益,feature
2508180782988180740,ID3,feature
-301531013363200196,CART,feature
-5009090089485465546,分类,feature
-6059094752038252756,树,feature
3910767649695352686,回归,feature
7299030006037181139,输出,feature
8183967944759427583,逻辑,feature
-8295054476685346187,剪枝,feature
-1838045316067901708,SVM,feature
5826048140129028313,表达式,feature
-4838743961339623550,对偶,feature
-4773400861813631005,KKT,feature
6224935930285237859,拉格朗日,feature
712666552894038468,优化方法,feature
4678168901473516483,损失,feature
2497878763069635578,核,feature
6668455041696984329,lr,feature
-1818172721326216091,决策,feature
-4731161995092137476,svm,feature
6647119140654491444,Linear,feature
6884966336263713188,LR,feature
-6200105028595368443,二乘,feature
3748418560464412761,梯度,feature
2106683458743627295,kmeans,feature
-2120192704528933734,收敛,feature
3591927256332949008,差别,feature
8348658974796981169,logistic,feature
-2946800045132878343,CTR,feature
-849413615791936415,sigmoid,feature
-7074587935383148420,激活函数,feature
193193253955835977,odds,feature
8938418188432334682,Sigmoid,feature
-6447905859969242234,L1,feature
1904821935332018130,L2,feature
6835307520058493388,l1,feature
-7401136679017130521,l2,feature
3778447077477818220,非线性,feature
6423305617882343868,目标,feature
8772322784707055883,sklearn,feature
4483091434181509661,LogisticRegression,feature
-5953207597555479678,随机森林,feature
5018639423130115743,RF,feature
407564444900572623,gbdt,feature
115462930534324835,boostingtree,feature
2316248438038425915,boosting,feature
5681638002218361803,拟合,feature
5651203551650109751,集成学习,feature
-2985625939043554436,randomforest,feature
1062211309128604780,GBDT,feature
-9056745939082543143,XGboost,feature
5285528058471020291,LightGBM,feature
-7297777757324070413,Xgboost,feature
3369135019687756641,xgboost,feature
-8853357191368208077,MAE,feature
-639180342627033576,MAPE,feature
-436373067038728195,重要性,feature
2359629890283275005,XGBoost,feature
-8106345859375772499,Attention,feature
-5000877333444891256,CNN,feature
-9117871592010824466,词袋模型,feature
1419489745988310805,seq2seq,feature
-3484728516386417466,维度,feature
-6584860414828647506,点积,feature
2449717231708465643,bn,feature
482974006087068892,LN,feature
-1032671936945161913,Bert,feature
3076035170031564811,mask,feature
2638864736780239137,BERT,feature
1126497330649747515,attention,feature
8674920998456384671,FullTokenizer,feature
801970237450521446,LM,feature
8600150602578813792,elmo,feature
2829239101907691051,GPT,feature
-7179438744940296394,HMM,feature
-3009256281444402016,crf,feature
8935211448703885115,GolVe,feature
-6372501736814344005,W2V,feature
4468429247654445918,主题,feature
-4306639346035908680,Word2Vec,feature
-4043255877617511924,Doc2Vec,feature
-1191884031665702320,Dirichlet,feature
36215592098551730,beta,feature
5243811218593020439,trick,feature
-1359302114842040116,隐藏层,feature
-7374523495869878325,Softmax,feature
3009651152638339425,KL,feature
-254058729133064704,Meta,feature
-1893347194182908156,稳定性,feature
8234040648343508901,抽取,feature
-4224263626321137556,准确性,feature
8969620289228509050,监督学习,feature
-5412940278855852934,误差,feature
8699254298376427633,高次方程,feature
-6843241586096726081,多项式,feature
5776759067582331996,稀疏,feature
2628259868502014525,bagging,feature
1464784306854391442,Var,feature
-5311307502488197457,dense,feature
5474797007320311567,normalization,feature
8782792632823141188,自变量,feature
4197403195542773471,因变量,feature
-4158551377692790693,不确定性,feature
3934308126120431469,FM,feature
9200735145656430774,FFM,feature
-3842135816173274783,Dense,feature
7007418920297063901,knn,feature
7725440148002585850,判别式,feature
-5489849789162011823,Wide,feature
3612551639044066027,epoch,feature
613622361048011664,learningrate,feature
-2182451587576247289,Deep,feature
-6844055609139332994,Xavier,feature
5884394170786892475,正态分布,feature
-1610852548664631416,relu,feature
1566036982162809091,神经元,feature
414047898710134243,加权,feature
-5878999875211421984,batch,feature
-1185802877471440097,FNN,feature
-5790301504920334579,dnn,feature
4074744578161410231,推荐,feature
-7526283654234208555,DeepFm,feature
489913560275946907,高频,feature
1049458817285175310,隐层,feature
781270746379499151,压缩,feature
-7425116710758571907,pooling,feature
-9118584596764936505,全量,feature
-1063601929900105498,nlp,feature
174921144565164049,doc2vec,feature
-1857466428930692974,加快,feature
-4523547401630415241,预处理,feature
-3431645282541265684,反映,feature
-3090779288482198432,连续性,feature
5892664850503182840,逼近,feature
3714743259931672180,平方和,feature
-5467322479217155418,平方根,feature
3928692121330930602,统计分析,feature
6625381043690348794,信息量,feature
-3841172894103473401,有放回,feature
894470342599712559,无放回,feature
5501322895145989001,放回,feature
-2604766793566023585,最大值,feature
1169915252050922009,偏导恒,feature
8978158595067779977,多元,feature
6032460995592815067,排序,feature
-11742290462895986,空间,feature
7466740712759401179,DBSCAN,feature
-4936536969399246017,ceiling,feature
2550371802845688746,log2,feature
9175633321269152180,阈值,feature
3080359166709157507,平均值,feature
4317357023252026785,数据挖掘,feature
8581626546050289390,下采样,feature
2591704253625834700,高维,feature
-9100010341369400145,上采样,feature
590482025305826676,泛化能力,feature
-8926434020482007313,抽样,feature
-1404848956858117563,互斥,feature
-29238730190876047,EasyEnsemble,feature
-3115944203079416038,融合,feature
8024897855583027047,Ensemble,feature
-7799275280499388082,BalanceCascade,feature
855081888565711486,Boost,feature
-7020249294477912197,adaboost,feature
8582790982698682272,较差,feature
4793295664235087616,分段,feature
1496898592927823193,筛选,feature
1378952903002441678,pca,feature
2754822979905165397,svd,feature
307348040790015276,lda,feature
-6034111282174361247,计数,feature
5581331591483854025,词干,feature
7249268895974389717,相似性,feature
4110369332783025149,词性,feature
8577300440725378891,词形,feature
7442636971733892224,注意力,feature
-8129372112026537680,狄利克雷,feature
-5037482685321112681,xlnet,feature
-2998774376424788472,卡方,feature
2991580670529828026,散度,feature
-6605920324754486382,有效性,feature
-6131044900173181516,存疑,feature
-8293243981382202237,递归,feature
-6265756467120252041,线性回归,feature
-7683381414519810301,限制性,feature
4641950703662016971,求偏,feature
8971456980984846051,规划,feature
-3059199997702624432,联立方程,feature
-587117326239221734,不等式,feature
4924918300692188234,LibSVM,feature
-8099849128880966218,exp,feature
-1637522320119931888,泰勒展开,feature
628854860820336837,margin,feature
-293792275752260661,logisticalloss,feature
744232680607896795,hingeloss,feature
-5857924849769292941,均方,feature
7197766158003283588,右侧,feature
7888761925403499685,可逆,feature
-8683192206714457721,线性关系,feature
-2889460437133901513,MSE,feature
1985501134350756311,数学公式,feature
2920889205759735137,领域,feature
8668143497408728926,高斯公式,feature
7327362682576159857,blob,feature
-998298542361683041,比率,feature
8199367781934561362,glm,feature
9063331158809068243,敏感度,feature
-3349545555079390006,最小化,feature
3405732713149139523,加速,feature
3486278910183467573,效率,feature
-3771109492746329937,GLM,feature
4016656102240197192,HingeLoss,feature
8772800633595499935,坐标轴,feature
1402365289792805968,一阶,feature
-2891587633957177041,LabelEncoder,feature
6765398923725420320,目标值,feature
-3681022501652464227,鲁棒,feature
166364841281574479,标准,feature
-2311394746120356917,临界值,feature
1448225511998717028,修剪,feature
1524392738601874571,串联,feature
-3715127758614189358,分枝,feature
-6221424118900504686,步长,feature
-6203981494731365386,直方图,feature
8948098933802831868,block,feature
-3624526490794373817,分布式,feature
4207035709632093114,estimator,feature
3243120711818725859,decoder,feature
2537687317806027097,encoder,feature
-4729647902777250755,解码,feature
8629975194991470863,Encoder,feature
6664758843244265334,Decoder,feature
4572081849660442909,token,feature
-7082257432045248162,FFN,feature
-5223339975925900870,Transformer,feature
6720965617221462354,CV,feature
5420071627749060628,xdeepfm,feature
-8570021804371229170,Transform,feature
-4250569423340147367,word,feature
-8272129993289757586,语料库,feature
2420093902680425300,w2v,feature
-4061913833442632588,lookup,feature
1344227422591684369,有序,feature
5681242675106411764,tokens,feature
3172854968588347870,transformer,feature
-5169039900388968641,BasicTokenizer,feature
6965803194106265529,WordpieceTokenizer,feature
2730600353212448137,WordPiece,feature
-6200147316408027675,LSTM,feature
6891361161366387520,静态,feature
6930951164570914162,条件随机场,feature
-5873841633026772756,词袋,feature
2480953920248477848,二项式,feature
-8964448840636860885,蒙特卡洛,feature
-8107570341574627641,连通性,feature
-4543461367820092019,断点,feature
-1865886088124705429,Gibbs,feature
4799302562084683217,kl,feature
6193783082802751141,偏导,feature
-6260410699546908031,logistics,feature
-4120764935098506935,sgd,feature
1156192346749589976,词频,feature
5185464233119973893,Mikolov,feature
2387229466313404017,CBOW,feature
-4124091269316208503,item2vec,feature
-2645977627709783892,基础概念,root
-6554637646629173298,先验概率和后验概率,first
8509083764919077879,方差与偏差,first
6190243665508550201,生成与判别模型,first
140873452964316584,频率概率,first
7572685019909437710,DIN,first
4355267420512087754,YouTubeNet,first
9188055597959925007,数学,root
3636533050383356852,gcd,first
1072553011488027508,平面曲线的切线和法线,first
7827888111372982186,微分中值定理,first
2518422997827984469,期望、方差、标准差和协方差,first
-2866004950794000147,概率密度分布,first
-9058308954546691978,概率论,first
2163773414205606591,欧拉公式,first
739356176062184599,牛顿法,first
-1587004343677428557,矩阵,first
3355814868070473980,数据预处理,root
4030250658844292292,异常点识别,first
799005817608839767,数据平衡,first
4433983848439462430,特征提取,first
-6638521969822211896,缺失值处理,first
-6285962837755487719,决策树,first
6265233779160925663,支持向量机,first
5031321689844388474,深度学习,root
2402697932242882010,batch_normalization,first
913766234967248011,残差网络,first
4325580639240905303,自然语言处理,root
-8789888309477226506,GloVe,first
